[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Screencasts",
    "section": "",
    "text": "If you’re looking for real-world examples of live data analyses, you’ve come to the right place.\nDavid Robinson, a highly experienced Data Scientist, has recorded many screencasts where he analyses data that he’s never seen before. These are fantastic examples of how to think about approaching an analysis.\nThe recordings were done as part of a weekly R programming challenge called TidyTuesday. All code is shared and all datasets are publicly available.\nIn each video you’ll learn:\nThis is a wealth of knowledge for new and experienced analysts alike.\nUse the search bar to look for specific functions, packages or other keywords.\nBelow you’ll find a list of 82 time-stamped screencasts."
  },
  {
    "objectID": "index.html#header-2",
    "href": "index.html#header-2",
    "title": "R Screencasts",
    "section": "Header 2",
    "text": "Header 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "David Robinson for recording the screencasts.\nAlex Cookson and Eric Fletcher for painstakingly time-stamping and describing all the content.\nOscar Baruffa for creating this website collating it all.\nThomas Mock for maintaining the TidyTuesday challenge.\nCountless others for R, RStudio, Quarto and a gazillion packages that make this all possible.\n\nIf you’d like to keep up to date with this website and other resources from Oscar Baruffa, sign up to the newsletter for posts about R, data and careers."
  },
  {
    "objectID": "content_pages/African-American Achievements.html",
    "href": "content_pages/African-American Achievements.html",
    "title": "African-American Achievements",
    "section": "",
    "text": "Notable topics: plotly interactive timeline, Wikipedia web scraping\nRecorded on: 2020-06-09\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/African-American History.html",
    "href": "content_pages/African-American History.html",
    "title": "African-American History",
    "section": "",
    "text": "Notable topics: Network diagram, Wordcloud\nRecorded on: 2020-06-16\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Animal Crossing.html",
    "href": "content_pages/Animal Crossing.html",
    "title": "Animal Crossing",
    "section": "",
    "text": "Notable topics: Topic modelling (stm package)\nRecorded on: 2020-05-05\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Art Collections.html",
    "href": "content_pages/Art Collections.html",
    "title": "Art Collections",
    "section": "",
    "text": "Notable topics: geom_area plot, distributions, calculating area (square meters) and ratio (width / height)\nRecorded on: 2021-01-12\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Australian Animal Outcomes.html",
    "href": "content_pages/Australian Animal Outcomes.html",
    "title": "Australian Animal Outcomes",
    "section": "",
    "text": "Notable topics: Data manipulation, Web Scraping (rvest package) and SelectorGadget, Animated Choropleth Map\nRecorded on: 2020-07-21\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Beach Volleyball.html",
    "href": "content_pages/Beach Volleyball.html",
    "title": "Beach Volleyball",
    "section": "",
    "text": "Notable topics: Data cleaning, Logistic regression\nRecorded on: 2020-05-19\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "index.html#all-recordings",
    "href": "index.html#all-recordings",
    "title": "R Screencasts",
    "section": "All recordings",
    "text": "All recordings"
  },
  {
    "objectID": "content_pages/African-American Achievements.html#full-screencast",
    "href": "content_pages/African-American Achievements.html#full-screencast",
    "title": "African-American Achievements",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/African-American Achievements.html#timestamp",
    "href": "content_pages/African-American Achievements.html#timestamp",
    "title": "African-American Achievements",
    "section": "Timestamp",
    "text": "Timestamp"
  },
  {
    "objectID": "content_pages/African-American History.html#full-screencast",
    "href": "content_pages/African-American History.html#full-screencast",
    "title": "African-American History",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/African-American History.html#timestamp",
    "href": "content_pages/African-American History.html#timestamp",
    "title": "African-American History",
    "section": "Timestamp",
    "text": "Timestamp"
  },
  {
    "objectID": "content_pages/Animal Crossing.html#full-screencast",
    "href": "content_pages/Animal Crossing.html#full-screencast",
    "title": "Animal Crossing",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Animal Crossing.html#timestamp",
    "href": "content_pages/Animal Crossing.html#timestamp",
    "title": "Animal Crossing",
    "section": "Timestamp",
    "text": "Timestamp"
  },
  {
    "objectID": "content_pages/Art Collections.html#full-screencast",
    "href": "content_pages/Art Collections.html#full-screencast",
    "title": "Art Collections",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Art Collections.html#timestamp",
    "href": "content_pages/Art Collections.html#timestamp",
    "title": "Art Collections",
    "section": "Timestamp",
    "text": "Timestamp"
  },
  {
    "objectID": "content_pages/Australian Animal Outcomes.html#full-screencast",
    "href": "content_pages/Australian Animal Outcomes.html#full-screencast",
    "title": "Australian Animal Outcomes",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Australian Animal Outcomes.html#timestamp",
    "href": "content_pages/Australian Animal Outcomes.html#timestamp",
    "title": "Australian Animal Outcomes",
    "section": "Timestamp",
    "text": "Timestamp"
  },
  {
    "objectID": "content_pages/Beach Volleyball.html#full-screencast",
    "href": "content_pages/Beach Volleyball.html#full-screencast",
    "title": "Beach Volleyball",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Beach Volleyball.html#timestamp",
    "href": "content_pages/Beach Volleyball.html#timestamp",
    "title": "Beach Volleyball",
    "section": "Timestamp",
    "text": "Timestamp"
  },
  {
    "objectID": "content_pages/African-American Achievements.html#timestamphi",
    "href": "content_pages/African-American Achievements.html#timestamphi",
    "title": "African-American Achievements",
    "section": "Timestamphi",
    "text": "Timestamphi"
  },
  {
    "objectID": "content_pages/African-American Achievements.html#timestamps",
    "href": "content_pages/African-American Achievements.html#timestamps",
    "title": "African-American Achievements",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:8:20\nUse fct_reorder from the forcats package to reorder the category factor levels by sorting along n.\nfct_reorder\nforcats\n\n\n\n\n0:11:35\nUse str_remove from the stringr package to remove anything after a bracket or parenthesis from the person variable with the regular expression \"[\\\\[\\\\(].*\" David then discusses how web scraping may be a better option than parsing the strings.\nstr_remove\nstringr\n\n\n\n\n0:12:25\nUse str_trim from the stringr package to remove the whitespace from the person variable. David then discusses how web scraping may be a better option than parsing the strings.\nstr_trim\nstringr\n\n\n\n\n0:15:50\nCreate an interactive plotly timeline.\nggplotly\nplotly\n\n\n\n\n0:18:20\nUse ylim(c(-.1, 1)) to set scale limits moving the geom_point to the bottom of the graph.\nylim\nggplot2\n\n\n\n\n0:19:30\nUse paste0 from base R to concatenate the accomplishment and person with \": \" in between the two displayed in the timeline hover label.\npaste0\nbase\n\n\n\n\n0:20:30\nSet y to category in ggplot aesthetics to get 8 separate timelines on one plot, one for each category. Doing this allows David to remove the ylim mentioned above.\naes\nggplot2\n\n\n\n\n0:22:25\nUse the plotly tooltip = text parameter to get just a single line of text in the plotly hover labels.\ntooltip\nplotly\n\n\n\n\n0:26:05\nUse glue from the glue package to reformat text with \\n included so that the single line of text can now be broken up into 2 separate lines in the hover labels.\nglue\nglue\n\n\n\n\n0:33:55\nUse separate_rows from the tidyr package to separate the occupation_s variable from the science dataset into multiple columns delimited by a semicolon with sep = \"; \"\nseparate_rows\ntidyr\n\n\n\n\n0:34:25\nUse str_to_title from the stringr package to conver the case to title case in the occupation_s variable.\nstr_to_title\nstringr\n\n\n\n\n0:35:15\nUse str_detect from the stringr package to detect the presence of statistician from within the occupation_s variable with regex(\"statistician\", ignore_case = TRUE) to perform a case-insensitive search.\nstr_detect\nstringr\n\n\n\n\n0:41:55\nUse the rvest package with Selector Gadget to scrape additional information about the individual from their Wikipedia infobox.\nread_html | html_nodes | html_table | setNames\nrvest\n\n\n\n\n0:49:15\nUse map and possibly from the purrr package to separate out the downloading of data from parsing the useful information. David then turns the infobox extraction step into an anonymous function using .%>% dot-pipe.\nmap | possibly | read_html\npurrr\n\n\n\n\n0:58:40\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/African-American History.html#timestamps",
    "href": "content_pages/African-American History.html#timestamps",
    "title": "African-American History",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:6:55\nUse fct_lump from the forcats package to lump together all the factor levels in ship_name except the n most frequent. Used within filter with ! = \"Other\" to remove other.\nfct_lump\nforcats\n\n\n\n\n0:8:00\nuse fct_reorder from the forcats package to reorder the ship_name factor levels y sorting along the n_slaves_arrived variable.\nfct_reorder\nforcats\n\n\n\n\n0:10:20\nAdd geom_vline to geom_histogram to annotate the plot with a vertical line indicating the Revolutionary War and the Civil War.\ngeom_vline\nggplot2\n\n\n\n\n0:13:00\nUse truncated division within count to create a new decade variable equal to 10 * (year_arrival %/% 10))\ncount\ndplyr\n\n\n\n\n0:17:20\nUse str_trunc from the stringr package to truncate the titles in each facet panel accounting for the slave ports with really long names.\nstr_trunc\nstringr\n\n\n\n\n0:18:05\nAnother option for accounting for long titles in the facet panels is to use strip.text within theme with element_text(size = 6)\ntheme\nggplot2\n\n\n\n\n0:26:55\nUse the ggraph package to create a network diagram using port_origin and port_arrival.\nggraph | geom_edge_link | geom_node_point | geom_node_text\nggraph\n\n\n\n\n0:29:05\nUse arrow from the grid package to add directional arrows to the points in the network diagram.\narrow\ngrid\n\n\n\n\n0:29:40\nUse scale_width_size_continuous from the ggraph packge to adjust the size of the points in the network diagram.\nscale_edge_size_continuous\nggraph\n\n\n\n\n0:35:25\nWithin summarize use mean(n_slaves_arrived, na.rm = TRUE) * n()) to come up with an estimated total numer of slaves since 49% of the data is missing.\nsummarize | mean\ndplyr\n\n\n\n\n0:48:20\nCreate a faceted stacked percent barplot (spinogram) showing the percentage of black_free, black_slaves, white, and other for each region.\ngeom_col | facet_wrap\nggplot2\n\n\n\n\n0:51:00\nUse the wordcloud package to create a wordcloud with the african_names dataset. David hsa issues with the wordcloud package and opts to use ggwordcloud instead. Also, mentions the worldcloud2 package.\nwordcloud | geom_text_wordcloud\nwordcloud | ggwordcloud\n\n\n\n\n0:55:20\nUse fct_recode from the forcats package to change the factor levels for the gender variable while renaming Man = \"Boy\" and Woman = \"Girl\"\nfct_recode\nforcats\n\n\n\n\n0:57:20\nUse reorder_within from the tidytext package to reorder the geom_col by n within gender variable for each facet panel.\nreorder_within\ntidytext\n\n\n\n\n0:59:00\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Animal Crossing.html#timestamps",
    "href": "content_pages/Animal Crossing.html#timestamps",
    "title": "Animal Crossing",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:05\nStarting text analysis of critic reviews of Animal Crossing\nNA\nNA\n\n\n\n\n0:7:50\nUsing floor_date function from lubridate package to round dates down to nearest month (then week)\nfloor_date\nlubridate\n\n\n\n\n0:9:00\nUsing unnest_tokens function and anti_join functions from tidytext package to break reviews into individual words and remove stop words\nunnest_tokens | anti_join\ntidytext\n\n\n\n\n0:10:35\nTaking the average rating associated with individual words (simple approach to gauge sentiment)\nNA\nNA\n\n\n\n\n0:12:30\nUsing geom_line and geom_point to graph ratings over time\ngeom_line\nNA\n\n\n\n\n0:14:40\nUsing mean function and logical statement to calculate percentages that meet a certain condition\nmean\nNA\n\n\n\n\n0:22:30\nUsing geom_text to visualize what words are associated with positive/negative reviews\ngeom_text\nNA\n\n\n\n\n0:27:00\nDisclaimer that this exploration is not text regression – wine ratings screencast is a good resource for that\nNA\nNA\n\n\n\n\n0:28:30\nStarting to do topic modelling\nNA\nNA\n\n\n\n\n0:30:45\nExplanation of stm function from stm package\nstm\nstm\n\n\n\n\n0:34:30\nExplanation of stm function’s output (topic modelling output)\nstm\nstm\n\n\n\n\n0:36:55\nChanging the number of topics from 4 to 6\nNA\nNA\n\n\n\n\n0:37:40\nExplanation of how topic modelling works conceptually\nNA\nNA\n\n\n\n\n0:40:55\nUsing tidy function from broom package to find which “documents” (reviews) were the “strongest” representation of each topic\ntidy\nbroom\n\n\n\n\n0:44:50\nNoting that there might be a scraping issue resulting in review text being repeated\nNA\nNA\n\n\n\n\n0:46:05\n(Unsuccessfully) Using str_sub function to help fix repeated review text by locating where in the review text starts being repeated\nstr_sub\nNA\n\n\n\n\n0:48:20\n(Unsuccessfully) Using str_replace and map2_chr functions, as well as regex cpaturing groups to fix repeated text\nstr_replace | map2\nNA\n\n\n\n\n0:52:00\nLooking at the association between review grade and gamma of the topic model (how “strong” a review represents a topic)\nNA\nNA\n\n\n\n\n0:53:55\nUsing cor function with method = “spearman” to calculate correlation based on rank instead of actual values\ncor\nNA\n\n\n\n\n0:57:35\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Art Collections.html#timestamps",
    "href": "content_pages/Art Collections.html#timestamps",
    "title": "Art Collections",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:55\nUsing clean_names to convert variable names from camelcase to snakecase.\nclean_names\njanitor\n\n\n\n\n0:4:05\nUse fct_reorder to reorder geom_col columns in ascending order.\nfct_reorder | geom_col\nforcats | ggplot2\n\n\n\n\n0:4:50\n“Use extract to extract a character column into multiple columns using the regular expression \"\"(.*) on (.*)\"\" at 6:05 David decides to change this to: Use separate with sep = \"\" on \"\" and fill = \"\"left\"\" and extra = \"\"merge\"\" to control what happens when there are not enoughor too many pieces. at 7:10 David decides to change to fill = \"\"right\"\".”\nextract | separate\ntidyr\n\n\n\n\n0:7:50\nUse replace_na to replace NAs with specified values. In this case replace them with Missing.\nreplace_na\ntidyr\n\n\n\n\n0:10:25\n“Use fct_lump to lump artist and medium levels except for the n most frequent. at 11:30 David decides to use filter(fct_lump(artist, 16) != \"\"Other\"\") to get rid of the artist Other category.”\nfct_lump | filter\nforcats | dplyr\n\n\n\n\n0:13:55\n“Create a geom_area plot to show the distribution of paintings by medium over time. At 15:35 David decides to change from count to percentage to make it easier to show the difference in composition using mutate(pct = n / sum).”\ngeom_area\nggplot2\n\n\n\n\n0:14:20\nBucket year variable into decades using round(year -1) to round the year to the nearest 10.\ncount | round\nbase | dplyr\n\n\n\n\n0:16:35\nUse scale_y_continuous(labels = scales::percent) to change y-axis labels to percent format.\nscale_y_continuous\nscales\n\n\n\n\n0:18:35\nTurn the geom_area plot into a faceted geom_col.\nfacet_wrap | geom_col\nggplot2\n\n\n\n\n0:21:35\n“Calculate the percentage of artists for each medium per decade.”\nmutate | group_by | summarize | complete\ndplyr | tidyr\n\n\n\n\n0:29:20\nCalculate the distribution of the area (square meters) and ratio (width / height) of the art pieces.\nfilter | mutate | ggplot | geom_histogram | scale_x_log10 | geom_vline\ndplyr | ggplot2\n\n\n\n\n0:38:25\nCategorize the pieces by shape(landscape, portait, scquare) based on their ratio then plot using geom_area to look at the composition over time.\nmutate | case_when | geom_area | complete\ndplyr | ggplot2\n\n\n\n\n0:41:35\nCraete a line plot showing the median ratio by decade over time.\ngroup_by | summarize | filter | ggplot | geom_line | geom_point\ndplyr | ggplot2\n\n\n\n\n0:44:15\nCraete a line plot showing the median area by decade over time.\ngroup_by | summarize | filter | ggplot | geom_line | geom_point\ndplyr | ggplot2\n\n\n\n\n0:46:05\nCreate a boxplot showing the distribution of area over time.\nmutate | filter | ggplot | geom_boxplot | scale_y_log10\ndplyr | ggplot2\n\n\n\n\n0:48:25\nCreate various summary statistics for the artists such as avg_year, first_year,last_year,n_pieces,median_area,median_ratio`.\ngroup_by | summarize | arrange\ndplyr\n\n\n\n\n0:51:00\nCreate a boxplot showing the distribution of ratio over time for n amount of artists. Use glue to concatonate number of pieces for each artist ont he y axis.\nfilter | add_count | mutate | ggplot | geom_boxplot | scale_x_log10 | geom_vline | glue\ndplyr | ggplot2 | glue\n\n\n\n\n0:56:20\nCreate a boxplot showing the distribution of ratio over time for each medium. Use glue to concatonate number of pieces for each medium on the y axis.\nfilter | add_count | mutate | ggplot | geom_boxplot | scale_x_log10 | geom_vline | glue\ndplyr | ggplot2 | glue\n\n\n\n\n0:59:10\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Australian Animal Outcomes.html#timestamps",
    "href": "content_pages/Australian Animal Outcomes.html#timestamps",
    "title": "Australian Animal Outcomes",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:20\nUsing use_tidytemplate to open the project dataset with the package’s tidytemplate Rmd\nuse_tidytemplate\ntidytuesdayR\n\n\n\n\n0:4:30\nUsing rename to rename Total column to total\nrename\ndplyr\n\n\n\n\n0:6:20\nUsing fct_reorder to reorder stacked barplot with weight = sum\nfct_reorder\nforcats\n\n\n\n\n0:7:00\nUsing fct_lump with w = n to lump together outcome factor levels displaying the most frequenct with rest lumped into other\nfct_lump\nforcats\n\n\n\n\n0:9:15\nUsing fct_recode to combine the factor level In Stock with Currently In Care\nfct_recode\nforcats\n\n\n\n\n0:12:10\nUsing fct_reorder to reorder facet_wrap panels\nfct_reorder\nforcats\n\n\n\n\n0:13:03\nUsing scale_y_continuous with labels = comma to separate digits with comma\nscale_y_continuous\nggplot2 | scales\n\n\n\n\n0:14:10\nUsing complete to complete account for missing combinations of data where the value is 0 in the released column\ncomplete\ntidyr\n\n\n\n\n0:16:10\nUsing max (year) within filter to subset the data displaying only the most recent year\nmax\nbase\n\n\n\n\n0:19:30\nUsing pivot_longer to pivot location variables from wide to long\npivot_longer\ntidyr\n\n\n\n\n0:21:45\nWeb Scaraping table from Wikipedia with SelectorGadget and Rvest\nread_html | html_nodes | map |\nrvest | janitor\n\n\n\n\n0:25:45\nUsing str_to_upper to upper case the values in the shorthand column\nstr_to_upper\nstringr\n\n\n\n\n0:27:13\nUsing parse_number to remove commas from population and area columns\nparse_number\nreadr\n\n\n\n\n0:28:55\nUsing bind_rows to bind the two web scraped tables from Wikipedia together by row and column\nbind_rows\ndplyr\n\n\n\n\n0:29:35\nUsing inner_join to combine the Wikipedia table with the original data set\ninner_join\ndplyr\n\n\n\n\n0:29:47\nUsing mutate to create new per_capita_million column to show outcome on a per million people basis\nmutate\ndplyr\n\n\n\n\n0:37:25\nUsing summarize to create new column pct_euthanized showing percent of cats and dogs euthanized over time. Formula accounts for 0 values thus avoiding a resulting empty vector.\nsummarize\ndplyr\n\n\n\n\n0:39:10\nUsing scale_y_continuous with labels = percent to add percentage sign to y-axis values\nscale_y_continuous\nggplot2 | scales\n\n\n\n\n0:42:45\nCreate a choropleth map of Australia using an Australian States Shapefile using the sf and ggplot2 packages | Troubleshooting begins at 44:25 (downsizing / downsampling with sf_simplify)\nread_sf | geom_sf | sf_simplify\nsf | ggplot2\n\n\n\n\n0:55:45\nAdd animation to the map of Australia showing the percent of cats euthanized by region using gganimate\ntransition_manual\ngganimate\n\n\n\n\n1:01:35\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Beach Volleyball.html#timestamps",
    "href": "content_pages/Beach Volleyball.html#timestamps",
    "title": "Beach Volleyball",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:30\nUse pivot_longer from the dplyr package to pivot the data set from wide to long.\npivot_longer\ndplyr\n\n\n\n\n0:7:20\nUse mutate_at from the dplyr package with starts_with to change the class to character for all columns that start with w_ and l_.\nmutate_at\ndplyr\n\n\n\n\n0:8:00\nUse separate from the tidyr package to separate the name variable into three columns with extra = merge and fill = right.\nseparate\ntidyr\n\n\n\n\n0:10:35\nUse rename from the dplyr package to rename w_player1, w_player2, l_player1, and l_player2.\nrename\ndplyr\n\n\n\n\n0:12:50\nUse pivot_wider from the dplyr package to pivot the name variable from long to wide.\npivot_wider\ndplyr\n\n\n\n\n0:15:15\nUse str_to_upper to convert the winner_loser w and l values to uppercase.\nstr_to_upper\nstringr\n\n\n\n\n0:20:25\nAdd unique row numbers for each match using mutate with row_number from the dplyr package.\nrow_number\ndplyr\n\n\n\n\n0:21:20\nSeparate the score values into multiple rows using separate_rows from the tidyr package.\nseparate_rows\ntidyr\n\n\n\n\n0:22:45\nUse separate from the tidyr package to actual scores into two columns, one for the winners score w_score and another for the losers score l_score.\nseparate\ntidyr\n\n\n\n\n0:23:45\nUse na_if from the dplyr package to change the Forfeit or other value from the score variable to NA.\nna_if\ndplyr\n\n\n\n\n0:24:35\nUse str_remove from the stringr package to remove scores that include retired.\nstr_remove\nstringr\n\n\n\n\n0:25:25\nDetermine how many times the winners score w_score is greter than the losers score l_score at least 1/3 of the time.\nmutate | group_by | summarize\ndplyr\n\n\n\n\n0:28:30\nUse summarize from the dplyr package to create the summary statistics including the number of matches, winning percentage, date of first match, date of most recent match.\nsummarize\ndplyr\n\n\n\n\n0:34:15\nUse type_convert from the readr package to convert character class variables to numeric.\ntype_convert\nreadr\n\n\n\n\n0:35:00\nUse summarize_all from the dplyr package to calculate the calculate which fraction of the data is not NA.\nsummarize_all\ndplyr\n\n\n\n\n0:42:00\nUse summarize from the dplyr package to determine players number of matches, winning percentage, average attacks, average errors, average kills, average aces, average serve errors, and total rows with data for years prior to 2019.\nThe summary statistics are then used to answer how would we could predict if a player will win in 2019 using geom_point and logistic regression. Initially, David wanted to predict performance based on players first year performance. (NOTE - David mistakingly grouped by year and age. He cathces this around 1:02:00.)\nsummarize | inner_join | geom_point | glm |cbind\ndplyr | ggplot2\n\n\n\n\n0:49:25\nUse year from the lubridate package within a group_by to determine the age for each play given their birthdate.\nsummarize | year\nlubridate\n\n\n\n\n0:54:30\nTurn the summary statistics at timestamp 42:00 into a . DOT %>% PIPE function.\nNA\nNA\n\n\n\n\n1:04:30\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Not on Twitter but keen to try it out? The free book Twitter for R programmers will get you started.\nWant to learn R? Here’s some free resources to get you started - one book and one video course.\nWant even more R? Check out the collection of over 250 free books covering a wide variety of fields and topics at Big Book of R."
  },
  {
    "objectID": "content_pages/European Energy.html",
    "href": "content_pages/European Energy.html",
    "title": "European Energy",
    "section": "",
    "text": "Notable topics: Data manipulation, Country flags, Slope graph, Function creation\nRecorded on: 2020-08-04\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/European Energy.html#full-screencast",
    "href": "content_pages/European Energy.html#full-screencast",
    "title": "European Energy",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/European Energy.html#timestamps",
    "href": "content_pages/European Energy.html#timestamps",
    "title": "European Energy",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:01:50\nUsing count to get an overview of scategorical data\ncount\ndplyr\n\n\n\n\n0:07:25\nUsing pivot_longer and gather to pivot date variables from wide to long\npivot_longer | gather\ntidyr\n\n\n\n\n0:09:00\nUsing as.integer to change year variable from character to integer class\nas.integer\nbase\n\n\n\n\n0:10:10\nUsing fct_reorder to reorder stacked barplot\nfct_reorder\nforcats\n\n\n\n\n0:10:30\nUsing scale_y_continuous with labels = comma from scales package to insert a comma every three digits on the y-axis\nscale_y_continuous | comma\nggplot2 | scales\n\n\n\n\n0:16:35\nUsing replace_na and list to replace NA values in country_name column with United Kingdom\nreplace_na\ntidyr\n\n\n\n\n0:18:05\nUsing fct_lump to lump factor levels together except for the 10 most frequent for each facet panel\nfct_lump\nforcats\n\n\n\n\n0:20:10\nUsing reorder_within with fun = sum and scale_y_reordered to reorder the categories within each facet panel\nreorder_within | scale_y_reordered\ntidytext\n\n\n\n\n0:24:30\nUsing ggflags package to add country flags | Debugging strategies include 1) minimal reproducible example and 2) binary search\ngeom_flag\nggfalgs\n\n\n\n\n0:29:20\n(Unsuccessfully) Using fct_recode to rename the ISO two-digit identifier for the United Kingdom from the UK to GB\nfct_recode\nforcats\n\n\n\n\n0:33:20\nUsing ifelse to replace the ISO two-digit identifier for the United Kingdom from UK to GB & from EL to GR fro Greece | Debugging included\nifelse\nbase\n\n\n\n\n0:40:45\nUsing str_to_lower to convert observations in country column to lower case\nstr_to_lower\nstringr\n\n\n\n\n0:45:00\nCreating a slope graph to show differences in Nuclear production (2106 versus 2018) | Using scale_y_log10 to increase distance between points | Using ggflags for country flags\ngeom_point | geom_line | scale_y_log10 | geom_flag\nggplot2 | ggflags\n\n\n\n\n0:47:00\nUsing scale_x_continuous with breaks = c(2016, 2018) to show only 2016 and 2018 on x-axis\nscale_x_continuous\nggplot2\n\n\n\n\n0:48:20\nExtend x-axis limits using scale_x_continuous with limits = c(2015, 2019) and geom_text with an ifelse within hjust to alternate labels for the right and left side of slope graph\nscale_x_continuous | geom_text\nggplot2\n\n\n\n\n0:52:40\nCreating a slopegraph function\nfunction\nbase\n\n\n\n\n1:00:00\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Beer Production.html",
    "href": "content_pages/Beer Production.html",
    "title": "Beer Production",
    "section": "",
    "text": "Notable topics: tidymetrics package demonstration, Animated map (gganimate package)\nRecorded on: 2020-04-01\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Beer Production.html#full-screencast",
    "href": "content_pages/Beer Production.html#full-screencast",
    "title": "Beer Production",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Beer Production.html#timestamps",
    "href": "content_pages/Beer Production.html#timestamps",
    "title": "Beer Production",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:25\nAsking, “What ingredients are used in beer?”\nNA\nNA\n\n\n\n\n0:4:40\nUsing filter and max functions to look at the most recent period of time\nfilter | max\nNA\n\n\n\n\n0:7:25\nUsing paste and ymd functions (ymd is from lubridate package) to convert year-month field into an date-formatted field\npaste | ymd\nNA\n\n\n\n\n0:9:20\nSpotting potential missing or mis-parsed data\nNA\nNA\n\n\n\n\n0:13:50\nIntroducing the tidymetrics framework\nNA\nNA\n\n\n\n\n0:14:45\nUsing install_github function to install tidymetrics from GitHub\ninstall_github\nNA\n\n\n\n\n0:15:25\nUsing cross_by_dimensions function from tidymetrics package to get aggregations at different levels of multiple dimensions\ncross_by_dimensions\ntidymetrics\n\n\n\n\n0:18:10\nUsing cross_by_periods function from tidymetrics package to also get aggregations for different intervals (e.g, month, quarter, year)\ncross_by_periods\ntidymetrics\n\n\n\n\n0:22:00\nUsing use_metrics_scaffold function from tidymetrics package to create framework for documenting dimensions in RMarkdown YAML header\nuse_metrics_scaffold\ntidymetrics\n\n\n\n\n0:24:00\nUsing create_metrics function from tidymetrics package to save data as a tibble with useful metadata (good for visualizing interactively)\ncreate_metrics\ntidymetrics\n\n\n\n\n0:25:15\nUsing preview_metric function from shinymetrics package (still under development as of 2020-04-24) to demonstrate shinymetrics\npreview_metrics\nshinymetrics\n\n\n\n\n0:27:35\nSuccesfuly getting shinymetrics to work\nNA\nshinymetrics\n\n\n\n\n0:28:25\nExplanation of the shinymetrics bug David ran into\nNA\nshinymetrics\n\n\n\n\n0:34:10\nChanging order of ordinal variable (e.g., “1,000 to 10,000” and “10,000 to 20,000”) using the parse_number, fct_lump, and coalesce functions\nparse_number | fct_lump | coalesce\nNA\n\n\n\n\n0:41:25\nAsking, “Where is beer produced?”\nNA\nNA\n\n\n\n\n0:46:45\nLooking up sf package documentation to refresh memory on how to draw state borders for a map\nNA\nsf\n\n\n\n\n0:48:55\nUsing match function and state.abb vector (state abbreviations) from sf package to perform a lookup of state names\nmatch\nNA\n\n\n\n\n0:51:05\nUsing geom_sf function (and working through some hiccoughs) to create a choropleth map\ngeom_sf\nsf\n\n\n\n\n0:52:30\nUsing theme_map function from ggthemes package to get more appropriate styling for maps\ntheme_map\nggthemes\n\n\n\n\n0:55:40\nExperimenting with how to get the legend to display in the bottom right corner\nNA\nNA\n\n\n\n\n0:58:25\nStarting to build an animation of consumption patterns over time using gganimate package\nNA\ngganimate\n\n\n\n\n1:03:40\nGetting the year being animated to show up in the title of a gganimate map\nNA\ngganimate\n\n\n\n\n1:05:40\nSummary of screencast\nNA\nNA\n\n\n\n\n1:06:50\nSpotting a mistake in a group_by call causing the percentages not to add up properly\nNA\nNA\n\n\n\n\n1:09:10\nBrief extra overview of tidymetrics code\nNA\ntidymetrics"
  },
  {
    "objectID": "content_pages/Beyonce and Taylor Swift Lyrics.html",
    "href": "content_pages/Beyonce and Taylor Swift Lyrics.html",
    "title": "Beyonce and Taylor Swift Lyrics",
    "section": "",
    "text": "Notable topics: Text analysis, tf_idf, Log odds ratio, Diverging bar graph, Lollipop graph\nRecorded on: 2020-09-29\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Beyonce and Taylor Swift Lyrics.html#full-screencast",
    "href": "content_pages/Beyonce and Taylor Swift Lyrics.html#full-screencast",
    "title": "Beyonce and Taylor Swift Lyrics",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Beyonce and Taylor Swift Lyrics.html#timestamps",
    "href": "content_pages/Beyonce and Taylor Swift Lyrics.html#timestamps",
    "title": "Beyonce and Taylor Swift Lyrics",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:7:50\nUse fct_reorder from the forcats package to reorder title factor levels by sorting along the sales variable in geom_col plot.\nfct_reorder\nforcats\n\n\n\n\n0:8:10\nUse labels = dollar from the scales package to format the geom_col x-axis values as currency.\nlabels\nscales\n\n\n\n\n0:11:15\nUse rename_all(str_to_lower) to convert variable names to lowercase.\nrename_all | str_to_lower\ndplyr | stringr\n\n\n\n\n0:12:45\nUse unnest_tokens from the tidytext package to split the lyrics into one-lyric-per-row.\nunnest_tokens\ntidytext\n\n\n\n\n0:13:00\nUse anti_join from the tidytext package to find the most common words int he lyrics without stop_words.\nanti_join\ndplyr\n\n\n\n\n0:15:15\nUse bind_tf_idf from the tidytext package to determine tf - the proportion each word has in each album and idf - how specific each word is to each particular album.\nbind_tf_idf\ntidytext\n\n\n\n\n0:17:45\nUse reorder_within with scale_y_reordered in order to reorder the bars within each facet panel. David replaces top_n with slice_max from the dplyr package in order to show the top 10 words with ties = FALSE.\nreorder_within | scale_y_reordered | slice_max\ntidytext | dplyr\n\n\n\n\n0:20:45\nUse bind_log_odds from the tidylo package to calculate the log odds ratio of album and words, that is how much more common is the word in a specific album than across all the other albums.\nbind_log_odds\ntidylo\n\n\n\n\n0:23:10\nUse filter(str_length(word) <= 3) to come up with a list in order to remove common filler words like ah, uh, ha, ey, eeh, and huh.\nfilter | str_length\ndplyr | stringr\n\n\n\n\n0:27:00\nUse mdy from the lubridate package and str_remove(released, \" \\\\(.*)\")) from the stringr package to parse the dates in the released variable.\ndistinct | mdy | str_remove\ndplyr | lubridate | stringr\n\n\n\n\n0:28:15\nUse inner_join from the dplyr package to join taylor_swift_words with release_dates.\nDavid ends up having to use fct_recode since the albums reputation and folklore were nor lowercase in a previous table thus excluding them from the inner_join.\ninner_join | fct_recode\ndplyr | forcats\n\n\n\n\n0:28:30\nUse fct_reorder from the forcats package to reorder album factor levels by sorting along the released variable to be used in the faceted geom_col.\nfct_reorder | geom_col\nforcats | ggplot2\n\n\n\n\n0:34:40\nUse bind_rows from hte dplyr package to bind ts with beyonce with unnest_tokens from the tidytext package to get one lyric per row per artist.\nbind_rows | unnest_tokens\ndplyr | tidytext\n\n\n\n\n0:38:40\nUse bind_log_odds to figure out which words are more likely to come from a Taylor Swift or Beyonce song?\nbind_log_odds\ntidylo\n\n\n\n\n0:41:10\nUse slice_max from the dplyr package to select the top 100 words by num_words_total and then the top 25 by log_odds_weighted. Results are used to create a diverging bar chart showing which words are most common between Beyonce and Taylor Swift songs.\nslice_max | geom_col | ifelse | fct_reorder\ndplyr | ggplot2 | forcats\n\n\n\n\n0:44:40\nUse scale_x_continuous to make the log_odds_weighted scale more interpretable.\nscale_x_continuous\nggplot2\n\n\n\n\n0:50:45\nTake the previous plot and turn it into a lollipop graph with geom_point(aes(size = num_words_total, color = direction))\ngeom_col | geom_point | geom_vline\nggplot2\n\n\n\n\n0:53:05\nUse ifelse to change the 1x value on the x-axis to same.\nifelse\nbase\n\n\n\n\n0:54:15\nCreate a geom_point with geom_abline to show the most popular words they use in common.\npivot_wider | clean_names | geom_abline | geom_point | slice_max | scale_y_log_10 | scale_x_log_10 | geom_text\ntidyr | ggplot2 | dplyr\n\n\n\n\n1:01:55\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Big Mac Index.html",
    "href": "content_pages/Big Mac Index.html",
    "title": "Big Mac Index",
    "section": "",
    "text": "Notable topics: Data manipulation, Pairwise correlation\nRecorded on: 2020-12-22\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Big Mac Index.html#full-screencast",
    "href": "content_pages/Big Mac Index.html#full-screencast",
    "title": "Big Mac Index",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Big Mac Index.html#timestamps",
    "href": "content_pages/Big Mac Index.html#timestamps",
    "title": "Big Mac Index",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:45\nUse the add_count function from the dplyr package with name = \"country_total\" to count the number of observations by group in the name variable.\nadd_count\ndplyr\n\n\n\n\n0:5:55\nUse filter from the dplyr package with country_total == max(country_total) to filter the data for countries where every data point is provided.\nfilter | max\ndplyr | base\n\n\n\n\n0:6:30\nUse the rename function from the dplyr package to rename the name variable to country_name.\nrename\ndplyr\n\n\n\n\n0:6:45\nUse theme(legend.position = \"none\") to hide the legend generated by the geom_line plot.\ntheme\nggplot2\n\n\n\n\n0:7:00\nUse the expand_limits function from the ggplot2 package with y = 0 so that each facet panel has a y-axis that starts at the same point, in this case 0.\nexpand_limits\nggplot2\n\n\n\n\n0:8:30\nReorder facet panels using the fct_reorder from the forcats package with a function passed in to the .fun argument to calculate the ratio between max and min values in the local_price variable. At 12:00, David changes from using max and min to last and first to calculate the Big Mac inflation rate.\nfct_reorder\nforcats\n\n\n\n\n0:13:20\nUse scale_x_log10 from the ggplot2 package to change the breaks for the x-axis while also applying a log10 tranformation.\nscale_x_log10\nggplot2\n\n\n\n\n0:15:05\nUse geom_text from the from the ggplot2 with paste0 package to add labels to each bar in the plot indicating how many time X the price of a Big Mac increased from 2000 to 2020.\ngeom_text | paste0\nggplot2 | base\n\n\n\n\n0:28:10\nAdd two lines to a plot using 2 geom_line with color = argument and y= argument to distinguish between the two lines.\ngeom_line\nggplot2\n\n\n\n\n0:34:05\nUse geom_hline from the ggplot2 package to add horizontal reference line to each facet panel.\ngeom_hline\nggplot2\n\n\n\n\n0:35:40\nUse theme from the ggplot2 package with axis.text.x = element_text(angle = 90, hjust = 1) to rmake the x-axis labels horizontal in order to avoid overcrowding.\ntheme\nggplot2\n\n\n\n\n0:38:25\nUse geom_text to add country names to each point in geom_point plot. David then opts to use geom_text_repel from the ggrepel package instead to avoid overcrowding.\ngeom_text | geom_text_repel\nggplot2 | ggrepel\n\n\n\n\n0:38:40\nUse geom_smooth from the ggplot2 package with lm smoothing method to help show the linear trend when comparing gdp_dollar to usd_raw.\ngeom_smooth\nggplot2\n\n\n\n\n0:47:00\nUse the gganimate package to animate the GDP per capital versus adjusted big mac index relative to USD over time.\ntransition_manual | transition_time\ngganimate\n\n\n\n\n0:53:05\nUse str_to_upper and str_remove to remove _adjusted from base_currency while uppercasing the characters that remain.\nstr_to_upper | str_remove\nstringr\n\n\n\n\n0:58:05\nUse pairwise_cor from the widyr package to perform pairwise correlation to figure out which countries Big Mac prices tend to move together over time.\npairwise_cor\nwidyr\n\n\n\n\n1:00:50\nScreencast summary.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Bird Collisions.html",
    "href": "content_pages/Bird Collisions.html",
    "title": "Bird Collisions",
    "section": "",
    "text": "Notable topics: Bootstrapping\nRecorded on: 2019-05-03\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Bird Collisions.html#full-screencast",
    "href": "content_pages/Bird Collisions.html#full-screencast",
    "title": "Bird Collisions",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Bird Collisions.html#timestamps",
    "href": "content_pages/Bird Collisions.html#timestamps",
    "title": "Bird Collisions",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:45\nAnalyzing when NAs appear in a dimension\nNA\nNA\n\n\n\n\n0:7:30\nLooking at multiple categorical variable at the same time by gathering them into one column and eventually graphing each as a different facet\ngather\nNA\n\n\n\n\n0:9:30\nRe-order facet graphs according to which ones have the fewest categories in them to ones that have the most\nNA\nNA\n\n\n\n\n0:20:45\nGeometric mean for estimating counts when there are a lot of low values (1-3 bird collisions, in this case)\nNA\nNA\n\n\n\n\n0:23:15\nFilling in “blank” observations where there were no observations made\nNA\nNA\n\n\n\n\n0:27:00\nUsing log+1 to convert a dimension with values of 0 into a log scale\nNA\nNA\n\n\n\n\n0:29:00\nAdding confidence bounds for data using a geometric mean (where he first gets the idea of bootstrapping)\nNA\nNA\n\n\n\n\n0:32:00\nActual coding of bootstrap starts\nNA\nNA\n\n\n\n\n0:38:30\nAdding confidence bounds using bootstrap data\nNA\nNA\n\n\n\n\n0:42:00\nInvestigating potential confounding variables\nNA\nNA\n\n\n\n\n0:44:15\nDiscussing approaches to dealing with confounding variables\nNA\nNA\n\n\n\n\n0:46:45\nUsing complete function to get explicit NA values\ncomplete\nNA"
  },
  {
    "objectID": "content_pages/Board Game Reviews.html",
    "href": "content_pages/Board Game Reviews.html",
    "title": "Board Game Reviews",
    "section": "",
    "text": "Notable topics: LASSO regression (glmnet package)\nRecorded on: 2019-03-15\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Board Game Reviews.html#full-screencast",
    "href": "content_pages/Board Game Reviews.html#full-screencast",
    "title": "Board Game Reviews",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Board Game Reviews.html#timestamps",
    "href": "content_pages/Board Game Reviews.html#timestamps",
    "title": "Board Game Reviews",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:50\nStarting EDA (exploratory data analysis) with counts of categorical variables\nNA\nNA\n\n\n\n\n0:7:25\nSpecifying scale_x_log10 function’s breaks argument to get sensisble tick marks for time on histogram\nscale_x_log10\nNA\n\n\n\n\n0:8:45\nTweaking geom_histogram function’s binwidth argument to get something that makes sense for log scale\ngeom_histogram\nNA\n\n\n\n\n0:10:10\nUsing separate_rows to break down comma-separated values for three different categorical variables\nseparate_rows\nNA\n\n\n\n\n0:15:55\nUsing top_n to get top 20 observations from each of several categories (not quite right, fixed at 17:47)\ntop_n\nNA\n\n\n\n\n0:16:15\nTroubleshooting various issues with facetted graph (e.g., ordering, values appearing in multiple categories)\nfacet_wrap\nNA\n\n\n\n\n0:19:55\nStarting prediction of average rating with a linear model\nlm\nNA\n\n\n\n\n0:20:50\nSplitting data into train/test sets (training/holdout)\nNA\nNA\n\n\n\n\n0:22:55\nInvestigating relationship between max number of players and average rating (to determine if it should be in linear model)\nNA\nNA\n\n\n\n\n0:25:05\nExploring average rating over time (“Do newer games tend to be rated higher/lower?”)\nNA\nNA\n\n\n\n\n0:27:35\nDiscussing necessity of controlling for year a game was published in the linear model\nNA\nNA\n\n\n\n\n0:28:30\nNon-model approach to exploring relationship between game features (e.g., card game, made in Germany) on average rating\nNA\nNA\n\n\n\n\n0:30:50\nUsing geom_boxplot function to create boxplot of average ratings for most common game features\ngeom_boxplot\nNA\n\n\n\n\n0:34:05\nUsing unite function to combine multiple variables into one\nunite\nNA\n\n\n\n\n0:37:25\nIntroducing Lasso regression as good option when you have many features likely to be correlated with one another\nNA\nNA\n\n\n\n\n0:38:15\nWriting code to set up Lasso regression using glmnet and tidytext packages\nNA\nNA\n\n\n\n\n0:40:05\nAdding average rating to the feature matrix (warning: method is messy)\nNA\nNA\n\n\n\n\n0:41:40\nUsing setdiff function to find games that are in one set, but not in another (while setting up matrix for Lasso regression)\nsetdiff\nNA\n\n\n\n\n0:44:15\nSpotting the error stemming from the step above (calling row names from the wrong data)\nNA\nNA\n\n\n\n\n0:45:45\nExplaining what a Lasso regression does, including the penalty parameter lambda\nglmnet\nglmnet\n\n\n\n\n0:48:35\nUsing a cross-validated Lasso model to choose the level of the penalty parameter (lambda)\ncv.glmnet\nglmnet\n\n\n\n\n0:51:35\nAdding non-categorical variables to the Lasso model to control for them (e.g., max number of players)\nNA\nNA\n\n\n\n\n0:55:15\nUsing unite function to combine multiple variables into one, separated by a colon\nunite\nNA\n\n\n\n\n0:58:45\nGraphing the top 20 coefficients in the Lasso model that have the biggest effect on predicted average rating\nNA\nNA\n\n\n\n\n1:00:55\nMentioning the yardstick package as a way to evaluate the model’s performance\nNA\nyardstick\n\n\n\n\n1:01:15\nDiscussing drawbacks of linear models like Lasso (can’t do non-linear relationships or interaction effects)\nNA\nNA"
  },
  {
    "objectID": "content_pages/Bob Ross Paintings.html",
    "href": "content_pages/Bob Ross Paintings.html",
    "title": "Bob Ross Paintings",
    "section": "",
    "text": "Notable topics: Network graphs, Principal Component Analysis (PCA)\nRecorded on: 2019-08-12\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Bob Ross Paintings.html#full-screencast",
    "href": "content_pages/Bob Ross Paintings.html#full-screencast",
    "title": "Bob Ross Paintings",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Bob Ross Paintings.html#timestamps",
    "href": "content_pages/Bob Ross Paintings.html#timestamps",
    "title": "Bob Ross Paintings",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:40\nUsing clean_names function in janitor package to get field names to snake_case\nclean_names\njanitor\n\n\n\n\n0:1:50\nUsing gather function to get wide elements into tall (tidy) format\ngather\nNA\n\n\n\n\n0:2:35\nCleaning text (str_to_title, str_replace) to get into nicer-to-read format\nstr_to_title | str_replace\nNA\n\n\n\n\n0:3:30\nUsing str_remove_all function to trim trimming quotation marks and backslashes\nstr_remove_all\nNA\n\n\n\n\n0:4:40\nUsing extract function to extract the season number and episode number from episode field; uses regex capturing groups\nextract\nNA\n\n\n\n\n0:14:00\nUsing add_count function’s name argument to specify field’s name\nadd_count\nNA\n\n\n\n\n0:15:35\nGetting into whether the elements of Ross’s paintings changed over time (e.g., are mountains more/less common over time?)\nNA\nNA\n\n\n\n\n0:20:00\nQuick point: could have used logistic regression to see change over time of elements\nNA\nbroom\n\n\n\n\n0:21:10\nAsking, “What elements tends to appear together?” prompting clustering analysis\nNA\nwidyr\n\n\n\n\n0:22:15\nUsing pairwise_cor to see which elements tend to appear together\npairwise_cor\nwidyr\n\n\n\n\n0:22:50\nDiscussion of a blind spot of pairwise correlation (high or perfect correlation on elements that only appear once or twice)\nNA\nNA\n\n\n\n\n0:28:05\nAsking, “What are clusters of elements that belong together?”\nNA\nNA\n\n\n\n\n0:28:30\nCreating network plot using ggraph and igraph packages\nNA\nggraph | igraph\n\n\n\n\n0:30:15\nReviewing network plot for interesting clusters (e.g., beach cluster, mountain cluster, structure cluster)\nNA\nNA\n\n\n\n\n0:31:55\nExplanation of Principal Component Analysis (PCA)\nNA\nNA\n\n\n\n\n0:34:35\nStart of actual PCA coding\nNA\nNA\n\n\n\n\n0:34:50\nUsing acast function to create matrix of painting titles x painting elements (initially wrong, corrected at 36:30)\nacast\nreshape2\n\n\n\n\n0:36:55\nCentering the matrix data using t function (transpose of matrix), colSums function, and colMeans function\nt | colSums | colMeans\nNA\n\n\n\n\n0:38:15\nUsing svd function to performn singular value decomposition, then tidying with broom package\nsvd\nNA\n\n\n\n\n0:39:55\nExploring one principal component to get a better feel for what PCA is doing\nNA\nNA\n\n\n\n\n0:43:20\nUsing reorder_within function to re-order factors within a grouping\nreorder_within\ntidytext\n\n\n\n\n0:48:00\nExploring different matrix names in PCA (u, v, d)\nNA\nNA\n\n\n\n\n0:56:50\nLooking at top 6 principal components of painting elements\nNA\nNA\n\n\n\n\n0:57:45\nShowing percentage of variation that each principal component is responsible for\nNA\nNA"
  },
  {
    "objectID": "content_pages/Broadway Musicals.html",
    "href": "content_pages/Broadway Musicals.html",
    "title": "Broadway Musicals",
    "section": "",
    "text": "Notable topics: Creating an interactive dashboard with shinymetrics and tidymetrics, moving windows, period aggregation\nRecorded on: 2020-04-28\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Broadway Musicals.html#full-screencast",
    "href": "content_pages/Broadway Musicals.html#full-screencast",
    "title": "Broadway Musicals",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Broadway Musicals.html#timestamps",
    "href": "content_pages/Broadway Musicals.html#timestamps",
    "title": "Broadway Musicals",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:8:15\nUse the cross_by_periods function from the tidymetrics package to aggregate data over time (month, quarter, and year) then visualize with geom_line.\nrename | cross_by_periods\ntidymetrics\n\n\n\n\n0:14:00\nUse the cross_by_periods function from the tidymetrics package with windows = c(28)) to create a 4-week rolling average across month, quarter, and year.\ncross_by_periods\ntidymetrics\n\n\n\n\n0:21:50\nCreate and interactive dashboard using the shinymetrics and tidymetrics packages.\nuse_metrics_scaffold | create_metrics\nshinymetrics | Tidaymetrics\n\n\n\n\n0:25:00\nUse the str_remove function from the stringr package to remove matched pattern in a string.\nstr_remove\nstringr\n\n\n\n\n0:25:20\nUse the cross_by_dimensions function from the tidymetrics package which acts as an extended group_by that allows complete summaries across each individual dimension and possible combinations.\ncross_by_dimensions\ntidymetrics\n\n\n\n\n0:41:25\nUse the shinybones package to create an interactive dashboard to visualize all 3 metrics at the same time.\nNA\nshinybones"
  },
  {
    "objectID": "content_pages/Car Fuel Efficiency.html",
    "href": "content_pages/Car Fuel Efficiency.html",
    "title": "Car Fuel Efficiency",
    "section": "",
    "text": "Notable topics: Natural splines for regression\nRecorded on: 2019-10-15\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Car Fuel Efficiency.html#full-screencast",
    "href": "content_pages/Car Fuel Efficiency.html#full-screencast",
    "title": "Car Fuel Efficiency",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Car Fuel Efficiency.html#timestamps",
    "href": "content_pages/Car Fuel Efficiency.html#timestamps",
    "title": "Car Fuel Efficiency",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:3:20\nUsing select and sort and colnames functions to sort variables in alphabetical order\nselect | sort | colnames\nNA\n\n\n\n\n0:10:00\nAdding geom_abline for y = x to a scatter plot for comparison\ngeom_abline\nNA\n\n\n\n\n0:18:00\nVisualising using geom_boxplot for mpg by vehicle class (size of car)\ngeom_boxplot\nNA\n\n\n\n\n0:24:45\nStart of explanation of prediction goals\nNA\nNA\n\n\n\n\n0:27:00\nCreating train and test sets, along with trick using sample_frac function to randomly re-arrange all rows in a dataset\nsample_frac\nNA\n\n\n\n\n0:28:35\nFirst step of developing linear model: visually adding geom_smooth\ngeom_smooth\nNA\n\n\n\n\n0:30:00\nUsing augment function to add extra variables from model to original dataset (fitted values and residuals, especially)\naugment\nNA\n\n\n\n\n0:30:45\nCreating residuals plot and explaining what you want and don’t want to see\nNA\nNA\n\n\n\n\n0:31:50\nExplanation of splines\nNA\nNA\n\n\n\n\n0:33:30\nVisualising effect of regressing using natural splines\nNA\nNA\n\n\n\n\n0:35:10\nCreating a tibble to test different degrees of freedom (1:10) for natural splines\nNA\nNA\n\n\n\n\n0:36:30\nUsing unnest function to get tidy versions of different models\nunnest\nNA\n\n\n\n\n0:37:55\nVisualising fitted values of all 6 different models at the same time\nNA\nNA\n\n\n\n\n0:42:10\nInvestigating whether the model got “better” as we added degrees of freedom to the natural splines, using the glance function\nglance\nNA\n\n\n\n\n0:47:45\nUsing ANOVA to perform a statistical test on whether natural splines as a group explain variation in MPG\nNA\nNA\n\n\n\n\n0:48:30\nExploring colinearity of dependant variables (displacement and cylinders)\nNA\nNA\n\n\n\n\n0:55:10\nBinning years into every two years using floor function\nfloor\nNA\n\n\n\n\n0:56:40\nUsing summarise_at function to do quick averaging of multiple variables\nsummarise_at\nNA"
  },
  {
    "objectID": "content_pages/Caribou Locations.html",
    "href": "content_pages/Caribou Locations.html",
    "title": "Caribou Locations",
    "section": "",
    "text": "Notable topics: Maps with ggplot2, Calculating distance and speed with geosphere\nRecorded on: 2020-06-23\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Caribou Locations.html#full-screencast",
    "href": "content_pages/Caribou Locations.html#full-screencast",
    "title": "Caribou Locations",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Caribou Locations.html#timestamps",
    "href": "content_pages/Caribou Locations.html#timestamps",
    "title": "Caribou Locations",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:00\nUse summarize and across to calculate the proportion of NA values in the individuals dataset. Note, you do not need to use list().\nsummarize | across\ndplyr\n\n\n\n\n0:9:00\nUse ggplot and borders from the ggplot2 package to create a map of Canada with deploy_on_longitude and deploy_on_latitude from the individuals dataset.\nggplot | borders\nggplot2\n\n\n\n\n0:13:50\nImport Canada province shapefile using the sf package. [Unsuccessful]\nread_sf\nsf\n\n\n\n\n0:25:00\nUse min and max from base r within summarize to find out the start and end dates for each caribou in the locations dataset.\nmin | max\nbase\n\n\n\n\n0:27:15\nUse sample from base r to pick one single caribou at a time then use the subset with geom_path from ggplot2 to track the path a that caribou takes over time. color = factor(floor_date(timestamp, \"quarter\") is used to color the path according to what quarter the observation occured in.\nsample | geom_path\nbase | ggplot2\n\n\n\n\n0:35:15\nUse as.Date from base r and floor_date from the lubridate package to convert timestamp variable into quarters then facet_wrap the previous plot by quarter.\nas.Date | floor_date\nbase | lubridate\n\n\n\n\n0:37:15\nWithin mutate, use as.numeric(difftime(timestamp, lag(timestamp), unit = \"hours\")) from base r to figure out the gap in time between observations.\ndifftime | lag\nbase\n\n\n\n\n0:43:05\nUse distHaversine from the geosphere package to calculate distance in km then convert it to speed in kph.\ndistHaversine | cbind\ngeosphere\n\n\n\n\n1:00:00\nSummary of dataset.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Chopped.html",
    "href": "content_pages/Chopped.html",
    "title": "Chopped",
    "section": "",
    "text": "Notable topics: Data manipulation, Modeling (Linear Regression, Random Forest, and Natural Spline)\nRecorded on: 2020-08-25\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Chopped.html#full-screencast",
    "href": "content_pages/Chopped.html#full-screencast",
    "title": "Chopped",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Chopped.html#timestamps",
    "href": "content_pages/Chopped.html#timestamps",
    "title": "Chopped",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:20\nUse geom_histogram to visualize the distribution of episode ratings.\ngeom_histogram\nggplot\n\n\n\n\n0:6:30\nUse geom_point and geom_line with color = factor(season) to visualize the episode rating for every episode.\ngeom_point | geom_line\nggplot |\n\n\n\n\n0:7:15\nUse group_by and summarize to show the average rating for each season and the number of episodes in each season.\ngroup_by | summarize\ndplyr\n\n\n\n\n0:7:15\nContinuing from previous row:\nUse geom_line and geom_point with size = n_episodes to visualize the average rating for each season with point size indicating the total number of episodes (larger = more episodes, smaller = fewer episodes).\ngeom_line | geom_point\nggplot\n\n\n\n\n0:10:55\nUse fct_reorder to reorder the episode_name factor levels by sorting along the episode_rating variable.\nfct_reorder\nforcats\n\n\n\n\n0:10:55\nUse geom_point to visualize the top episodes by rating.\nUse the ‘glue’ package to place season number and episode number before episode name on the y axis.\ngnemonol | arrange\ngplot | dplyr | glue\n\n\n\n\n0:15:20\nUse pivot_longer to combine ingredients into one single column.\nUse separate_rows with sep = \", \" to separate out the ingredients with each ingredient getting its own row.\npivot_longer | separate_rows\ntidyr\n\n\n\n\n0:18:10\nUse fct_lump to lump ingredients together except for the 10 most frequent.\nUse fct_reorder to reorder ingredient factor levels by sorting against n.\nfct_lump | fct_reorder\nforcats\n\n\n\n\n0:18:10\nUse geom_col to create a stacked bar plot to visualize the most common ingredients by course.\ngeom_col\nggplot\n\n\n\n\n0:19:45\nUse fct_relevel to reorder course factor levels to appetizer, entree, dessert.\nfct_relevel\nforcats\n\n\n\n\n0:21:00\nUse fct_rev and scale_fill_discrete with guide = guide_legend(reverse = TRUE) to reorder the segments within the stacked bar plot.\nfct_rev | scale_fill_discrete\nforcats | ggplot\n\n\n\n\n0:23:20\nUse the widyr package and pairwise_cor to find out what ingredients appear together.\nMentioned: David Robinson - The {widyr} Package YouTube Talk at 2020 R Conference\nadd_count | distinct | pairwise_cor\nwidyr | dplyr\n\n\n\n\n0:26:20\nUse ggraph , geom_edge_link, geom_node_point, geom_node_text to create an ingredient network diagram to show their makeup and how they interact.\nggraph | geom_edge_link | geom_node_point | geom_node_text\nwidyr | ggraph | tidygraph\n\n\n\n\n0:28:00\nUse pairwise_count from widyr to count the number of times each pair of items appear together within a group defined by feature.\npairwise_count\nwidyr\n\n\n\n\n0:30:15\nUse unite from the tidyr package in order to paste together the episode_course and series_episode columns into one column to figure out if any pairs of ingredients appear together in the same course across episodes.\nunite | pairwise_count\ntidyr | widyr\n\n\n\n\n0:31:55\nUse summarize with min, mean,max, andn()to create thefirst_season,avg_season,last_seasonandn_appearances` variables.\nsummarize | min | mean | max\ndplyr | base\n\n\n\n\n0:34:35\nUse slice with tail to get the n ingredients that appear in early and late seasons.\nslice | tail\ndplyr | base\n\n\n\n\n0:35:40\nUse geom_boxplot to visualize the distribution of each ingredient across all seasons.\nsemi_join | geom_boxplot | fct_reorder\ndplyr | ggplot | forcats\n\n\n\n\n0:36:50\nFit predictive models (linear regression , random forest, and natural spline) to determine if episode rating is explained by the ingredients or season.\nUse pivot_wider with values_fill = list(value = 0)) with 1 indicating ingredient was used and 0 indicating it wasn’t used.\npivot_wider | lm | linear_reg | set_engine | fit | initial_split | training | plot | base | vfold_cv | fit_resamples | tune_grid | collect_metrics | geom_line | tidy | rand_forest | clean_names | step_ns | tune_grid | collect_metrics | prep | juice\ntidymodels | stats | rsample | ggplot | broom | parsnip | janitor\n\n\n\n\n1:17:25\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Cocktails.html",
    "href": "content_pages/Cocktails.html",
    "title": "Cocktails",
    "section": "",
    "text": "Notable topics: Pairwise correlation, Network diagram, Principal component analysis (PCA)\nRecorded on: 2020-05-26\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Cocktails.html#full-screencast",
    "href": "content_pages/Cocktails.html#full-screencast",
    "title": "Cocktails",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Cocktails.html#timestamps",
    "href": "content_pages/Cocktails.html#timestamps",
    "title": "Cocktails",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:6:20\nUse fct_reorder from the forcats package to reorder the ingredient factor levels along n.\nfct_reorder\nforcats\n\n\n\n\n0:7:40\nUse fct_lump from the forcats package to lump together all the levels except the n most frequent in the category and ingredient variables.\nfct_lump\nforcats\n\n\n\n\n0:11:30\nUse pairwise_cor from the widyr package to find the correlation between the ingredients.\npairwise_cor\nwidyr\n\n\n\n\n0:16:00\nUse reorder_within from the tidytext package with scale_x_reordered to reorder the the columns in each facet.\nreorder_within | scale_x_reordered\ntidytext\n\n\n\n\n0:19:45\nUse the ggraph and igraph packages to create a network diagram\ngraph_from_data_frame | ggraph | geom_edge_link | geom_node_point | geom_node_label\nigraph | ggraph\n\n\n\n\n0:25:15\nUse extract from the tidyr package with regex = (.*) oz to create a new variable amount which doesn’t include the oz.\nextract\ntidyr\n\n\n\n\n0:26:40\nUse extract with regex to turn the strings in the new amount variable into separate columns for the ones, numerator, and denominator.\nextract\ntidyr\n\n\n\n\n0:28:53\nUse replace_na from the tidyr package to replace NA with zeros in the ones, numberator, and denominator columns. David ends up reaplcing the zero in the denominator column with ones in order for the calculation to work.\nreplace_na\ntidyr\n\n\n\n\n0:31:49\nUse geom_text_repel from the ggrepel package to add ingredient labels to the geom_point plot.\ngeom_text_repel\nggrepel\n\n\n\n\n0:32:30\nUse na_if from the dplyr package to replace zeros with NA\nna_if\ndplyr\n\n\n\n\n0:34:25\nUse scale_size_continuous with labels = percent_format() to convert size legend values to percent.\nscale_size_continuous\nggplot2\n\n\n\n\n0:36:35\nChange the size of the points in the network diagram proportional to n using vertices = ingredient_info within graph_from_data_frame and aes(size = n) within geom_node_point.\ngraph_from_data_frame | geom_node_point\nigraph | ggraph\n\n\n\n\n0:48:05\nUse widely_svd from the widyr package to perform principle component analysis on the ingredients.\nwidely_svd | top_n | abs | geom_col | reorder_within | scale_y_reordered | facet_wrap\nwidyr\n\n\n\n\n0:52:32\nUse paste0 to concatenate PC and dimension in the facet panel titles.\npaste0\nbase\n\n\n\n\n0:57:00\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Coffee Ratings.html",
    "href": "content_pages/Coffee Ratings.html",
    "title": "Coffee Ratings",
    "section": "",
    "text": "Notable topics: Ridgeline plot, Pairwise correlation, Network plot, Singular value decomposition, Linear model\nRecorded on: 2020-07-07\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Coffee Ratings.html#full-screencast",
    "href": "content_pages/Coffee Ratings.html#full-screencast",
    "title": "Coffee Ratings",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Coffee Ratings.html#timestamps",
    "href": "content_pages/Coffee Ratings.html#timestamps",
    "title": "Coffee Ratings",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:08:15\nUsing fct_lump within count and then mutate to lump the variety of coffee together except for the most frequent\ncount | mutate | fct_lump\nforcats\n\n\n\n\n0:08:50\nCreate a geom_boxplot to visualize the variety and the distribution of total_cup_points\ngeom_boxplot\nggplot2\n\n\n\n\n0:09:55\nCreate a geom_histogram to visualize the variety and the distribution of total_cup_points\ngeom_histogram\nggplot2\n\n\n\n\n0:11:40\nUsing fct_reorder to reorder variety by sorting it along total_cup_points in ascending order\nfct_reorder\nfct_reorder\n\n\n\n\n0:12:35\nUsing summarize with across to calculate the percent of missing data (NA) for each rating variable\nsummarize | across\ndplyr\n\n\n\n\n0:15:20\nCreate a bar chart using geom_col with fct_lump to visualize the frequency of top countries\ngeom_col | fct_lump\nggplot2 | forcats\n\n\n\n\n0:20:35\nUsing pivot_longer to pivot the rating metrics for wide format to long format\npivot_longer\ntidyr\n\n\n\n\n0:21:30\nCreate a geom_line chart to see if the sum of the rating categories equal to the total_cup_points column\ngeom_line\nggplot2\n\n\n\n\n0:23:10\nCreate a geom_density_ridges chart to show the distribution of ratings across each rating metric\ngeom_density_ridges\nggridges\n\n\n\n\n0:24:35\nUsing summarize with mean and sd to show the average rating per metric with its standard deviation\nsummarize\ndplyr\n\n\n\n\n0:26:15\nUsing pairwise_cor to find correlations amongst the rating metrics\npairwise_cor\nwidyr\n\n\n\n\n0:27:20\nCreate a network plot to show the clustering of the rating metrics\ngraph_from_data_frame | ggraph | geom_edge_link | geom_node_point | geom_node_text\nggraph | igraph\n\n\n\n\n0:29:35\nUsing widely_svd to visualize the biggest source of variation with the rating metrics (Singular value decomposition)\nwidely_svd | geom_col | reorder_within | scale_y_reordered\nwidyr | tidytext\n\n\n\n\n0:37:40\nCreate a geom_histogram to visualize the distribution of altitude\ngeom_histogram\nggplot2\n\n\n\n\n0:40:20\nUsing pmin to set a maximum numeric altitude value of 3000\npmin\nbase\n\n\n\n\n0:41:05\nCreate a geom-point chart to visualize the correlation between altitude and quality (total_cup_points)\ngeom_point | geom_smooth\nggplot2\n\n\n\n\n0:42:00\nUsing summarize with cor to show the correlation between altitude and each rating metric\nsummarize | cor\ndplyr | stats\n\n\n\n\n0:44:25\nCreate a linear model lm for each rating metric then visualize the results using a geom_line chart to show how each kilometer of altitude contributes to the score\nlm | geom_point | tidy | map | geom_errorbarh\nstats | broom | purrr | ggplot2\n\n\n\n\n0:50:35\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/College Majors and Income.html",
    "href": "content_pages/College Majors and Income.html",
    "title": "College Majors and Income",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2018-10-15\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/College Majors and Income.html#full-screencast",
    "href": "content_pages/College Majors and Income.html#full-screencast",
    "title": "College Majors and Income",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/College Majors and Income.html#timestamps",
    "href": "content_pages/College Majors and Income.html#timestamps",
    "title": "College Majors and Income",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:45\nUsing read_csv function to import data directly from Github to R (without cloning the repository)\nread_csv\nNA\n\n\n\n\n0:7:20\nCreating a histogram (geom_histogram), then a boxplot (geom_boxplot), to explore the distribution of salaries\ngeom_histogram | geom_boxplot\nNA\n\n\n\n\n0:8:55\nUsing fct_reorder function to sort boxplot of college majors by salary\nfct_reorder\nNA\n\n\n\n\n0:9:35\nUsing dollar_format function from scales package to convert scientific notation to dollar format (e.g., “4e+04” becomes “$40,000”)\ndollar_format\nscales\n\n\n\n\n0:14:10\nCreating a dotplot (geom_point) of 20 top-earning majors (includes adjusting axis, using the colour aesthetic, and adding error bars)\ngeom_point\nNA\n\n\n\n\n0:17:45\nUsing str_to_title function to convert string from ALL CAPS to Title Case\nstr_to_title\nNA\n\n\n\n\n0:20:45\nCreating a Bland-Altman graph to explore relationship between sample size and median salary\nNA\nNA\n\n\n\n\n0:21:45\nUsing geom_text_repel function from ggrepel package to get text labels on scatter plot points\ngeom_text_repel\nggrepel\n\n\n\n\n0:28:30\nUsing count function’s wt argument to specify what should be counted (default is number of rows)\ncount\nNA\n\n\n\n\n0:30:00\nSpicing up a dull bar graph by adding a redundant colour aesthetic (trick from Julia Silge)\nNA\nNA\n\n\n\n\n0:36:20\nStarting to explore relationship between gender and salary\nNA\nNA\n\n\n\n\n0:37:10\nCreating a stacked bar graph (geom_col) of gender breakdown within majors\ngeom_col\nNA\n\n\n\n\n0:40:15\nUsing summarise_at to aggregate men and women from majors into categories of majors\nsummarise_at\nNA\n\n\n\n\n0:45:30\nGraphing scatterplot (geom_point) of share of women and median salary\ngeom_point\nNA\n\n\n\n\n0:47:10\nUsing geom_smooth function to add a line of best fit to scatterplot above\ngeom_smooth\nNA\n\n\n\n\n0:48:40\nExplanation of why not to aggregate first when performing a statistical test (including explanation of Simpson’s Paradox)\nNA\nNA\n\n\n\n\n0:49:55\nFixing geom_smooth so that we get one overall line while still being able to map to the colour aesthetic\ngeom_smooth\nNA\n\n\n\n\n0:51:10\nPredicting median salary from share of women with weighted linear regression (to take sample sizes into account)\nlm\nNA\n\n\n\n\n0:56:05\nUsing nest function and tidy function from the broom package to apply a linear model to many categories at once\nnest | tidy\nbroom\n\n\n\n\n0:58:05\nUsing p.adjust function to adjust p-values to correct for multiple testing (using FDR, False Discovery Rate)\np.adjust\nNA\n\n\n\n\n1:04:50\nShowing how to add an appendix to an RMarkdown file with code that doesn’t run when compiled\nNA\nNA\n\n\n\n\n1:09:00\nUsing fct_lump function to aggregate major categories into the top four and an “Other” category\nfct_lump\nNA\n\n\n\n\n1:10:05\nAdding sample size to the size aesthetic within the aes function\nNA\nNA\n\n\n\n\n1:10:50\nUsing ggplotly function from plotly package to create an interactive scatterplot (tooltips appear when moused over)\nggplotly\nplotly\n\n\n\n\n1:15:55\nExploring IQR (Inter-Quartile Range) of salaries by major\nNA\nNA"
  },
  {
    "objectID": "content_pages/CORD-19 Data Package.html",
    "href": "content_pages/CORD-19 Data Package.html",
    "title": "CORD-19 Data Package",
    "section": "",
    "text": "Notable topics: R package development and documentation-writing\nRecorded on: 2020-03-19\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/CORD-19 Data Package.html#full-screencast",
    "href": "content_pages/CORD-19 Data Package.html#full-screencast",
    "title": "CORD-19 Data Package",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/CORD-19 Data Package.html#timestamps",
    "href": "content_pages/CORD-19 Data Package.html#timestamps",
    "title": "CORD-19 Data Package",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:10\nOverview of JSON files with the data David will make a package of\nNA\nNA\n\n\n\n\n0:3:05\nStarting to create a new package with “New Project” in RStudio\nNA\nNA\n\n\n\n\n0:5:40\nCreating a file to reference the license for the dataset\nNA\nNA\n\n\n\n\n0:7:25\nUsing use_data_raw function from usethis package to set up a folder structure and preliminary function for raw data\nuse_data_raw\nusethis\n\n\n\n\n0:8:30\nExplanation that we want to limit the number of packages we load when building a package (e.g., no library(tidyverse) )\nNA\nNA\n\n\n\n\n0:9:00\nUsing use_package function from usethis package to add “Suggested packages”\nuse_package\nusethis\n\n\n\n\n0:10:15\nReviewing import and cleaning code already completed\nNA\nNA\n\n\n\n\n0:14:55\nUsing roxygen2 package to write documentation\nNA\nNA\n\n\n\n\n0:19:35\nMore documentation writing\nNA\nNA\n\n\n\n\n0:24:50\nUsing use_data function from usethis package to create a folder structure and datafile for (finished/cleaned) data\nuse_data\nusethis\n\n\n\n\n0:26:10\nMaking a mistake clicking “Install and Restart” button on the “Build” tab (because of huge objects in the environment) (see 26:50 for alternative)\nNA\nNA\n\n\n\n\n0:26:50\nUsing load_all function from devtrools package as an alternative to “Install and Restart” from above step\nload_all\ndevtools\n\n\n\n\n0:27:35\nUsing document function from devtools package to process written documentation\ndocument\ndevtools\n\n\n\n\n0:32:20\nDe-duplicating paper data in a way the keeps records that have fewer missing values than other records for the same paper\nNA\nNA\n\n\n\n\n0:39:50\nUsing use_data function with its overwrite argument to overwrite existing data\nuse_data\nusethis\n\n\n\n\n0:47:30\nWriting documentation for paragraphs data\nNA\nNA\n\n\n\n\n0:57:55\nTesting an install of the package\nNA\nNA\n\n\n\n\n0:59:30\nAdding link to code in documentation\nNA\nNA\n\n\n\n\n1:03:00\nWriting examples of how to use the package (in documentation)\nNA\nNA\n\n\n\n\n1:08:45\nDiscussion of outstanding items that David hasn’t done yet (e.g., readme, vignettes, tests)\nNA\nNA\n\n\n\n\n1:09:20\nCreating a simple readme, including examples, with use_readme_rmd function from usethis package\nuse_readme_rmd\nusethis\n\n\n\n\n1:16:10\nUsing knit function from the knitr package to knit the readme into a markdown file\nknit\nknitr\n\n\n\n\n1:17:10\nCreating a GitHub repository to host the package (includes how to commit to a GitHub repo using RStudio’s GUI)\nNA\nNA\n\n\n\n\n1:18:15\nExplanation that version 0.0.0.9000 means that the package is in early development\nNA\nNA\n\n\n\n\n1:20:30\nActually creating the GitHub repository\nNA\nNA\n\n\n\n\n1:22:25\nOverview of remaining tasks\nNA\nNA"
  },
  {
    "objectID": "content_pages/COVID-19 Open Research Dataset (CORD-19).html",
    "href": "content_pages/COVID-19 Open Research Dataset (CORD-19).html",
    "title": "COVID-19 Open Research Dataset (CORD-19)",
    "section": "",
    "text": "Notable topics: JSON formatted data\nRecorded on: 2020-03-18\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/COVID-19 Open Research Dataset (CORD-19).html#full-screencast",
    "href": "content_pages/COVID-19 Open Research Dataset (CORD-19).html#full-screencast",
    "title": "COVID-19 Open Research Dataset (CORD-19)",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/COVID-19 Open Research Dataset (CORD-19).html#timestamps",
    "href": "content_pages/COVID-19 Open Research Dataset (CORD-19).html#timestamps",
    "title": "COVID-19 Open Research Dataset (CORD-19)",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:0:55\nDisclaimer that David’s not an epidemiologist\nNA\nNA\n\n\n\n\n0:2:55\nOverview of dataset\nNA\nNA\n\n\n\n\n0:7:50\nUsing dir function with its full.names argument to get file paths for all files in a folder\ndir\nNA\n\n\n\n\n0:9:45\nInspecting JSON-formatted data\nNA\nNA\n\n\n\n\n0:10:40\nIntroducing hoist function as a way to deal with nested lists (typical for JSON data)\nhoist\nNA\n\n\n\n\n0:11:40\nContinuing to use the hoist function\nhoist\nNA\n\n\n\n\n0:13:10\nBrief explanation of pluck specification\npluck\nNA\n\n\n\n\n0:16:35\nUsing object.size function to check size of json data\nobject.size\nNA\n\n\n\n\n0:17:40\nUsing map_chr and str_c functions together to combine paragraphs of text in a list into a single character string\nmap_chr | str_c\nNA\n\n\n\n\n0:20:00\nUsing unnest_tokens function from tidytext package to split full paragraphs into individual words\nunnest_tokens\ntidytext\n\n\n\n\n0:22:50\nOverview of scispaCy package for Python, which has named entity recognition features\nNA\nNA\n\n\n\n\n0:24:40\nIntroducting spacyr package, which is a R wrapper around the Python scispaCy package\nNA\nspacyr\n\n\n\n\n0:28:50\nShowing how tidytext can use a custom tokenization function (David uses spacyr package’s named entity recognition)\nNA\ntidytext\n\n\n\n\n0:32:20\nDemonstrating the tokenize_words function from the tokenizers package\ntokenize_words\ntokenizers\n\n\n\n\n0:37:00\nActually using a custom tokenizer in unnest_tokens function\nunnest_tokens\ntidytext\n\n\n\n\n0:39:45\nUsing sample_n function to get a random sample of n rows\nsample_n\nNA\n\n\n\n\n0:43:25\nAsking, “What are groups of words that tend to occur together?”\nNA\nNA\n\n\n\n\n0:44:30\nUsing pairwise_cor from widyr package to find correlation between named entities\npairwise_cor\nwidyr\n\n\n\n\n0:45:40\nUsing ggraph and igraph packages to create a network plot\nNA\nggraph | igraph\n\n\n\n\n0:52:05\nStarting to look at papers’ references\nNA\nNA\n\n\n\n\n0:53:30\nUsing unnest_longer then unnest_wider function to convert lists into a tibble\nunnest_wider\nNA\n\n\n\n\n0:59:30\nUsing str_trunc function to truncate long character strings to a certain number of characters\nstr_trunc\nNA\n\n\n\n\n1:06:25\nUsing glue function for easy combination of strings and R code\nglue\nglue\n\n\n\n\n1:19:15\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/CRAN Package Code.html",
    "href": "content_pages/CRAN Package Code.html",
    "title": "CRAN Package Code",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2019-12-30\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/CRAN Package Code.html#full-screencast",
    "href": "content_pages/CRAN Package Code.html#full-screencast",
    "title": "CRAN Package Code",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/CRAN Package Code.html#timestamps",
    "href": "content_pages/CRAN Package Code.html#timestamps",
    "title": "CRAN Package Code",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:30\nSummarizing many things by language (e.g., lines of code, comment/code ratio)\nNA\nNA\n\n\n\n\n0:9:35\nUsing gather function to consolidate multiple metrics into one dimension, then visualizing by facetting by metric\ngather\nNA\n\n\n\n\n0:11:20\nSetting ncol = 1 within facet_wrap function to get facetted graphs to stack vertically\nfacet_wrap\nNA\n\n\n\n\n0:11:30\nUsing reorder_within function (tidytext package) to properly reorder factors within each facet\nreorder_within\ntidytext\n\n\n\n\n0:16:00\nUsing geom_text label to add language name as label to scatter points\ngeom_text\nNA\n\n\n\n\n0:20:00\nCompleting preliminary overview and looking at distribution of R code in packages\nNA\nNA\n\n\n\n\n0:26:15\nUsing str_extract to extract only letters and names from character vector (using regex)\nstr_extract\nNA\n\n\n\n\n0:34:00\nRe-ordering the order of categorical variables in the legend using guides function\nguides\nNA\n\n\n\n\n0:36:00\nInvestigating comment/code ratio\nNA\nNA\n\n\n\n\n0:43:05\nImporting additional package data (looking around for a bit, then starting to actually import ~46:00)\nNA\nNA\n\n\n\n\n0:54:40\nImporting even more additional data (available packages)\nNA\nNA\n\n\n\n\n0:57:50\nUsing separate_rows function to separate delimited values\nseparate_rows\nNA\n\n\n\n\n0:58:45\nUsing extract function and regex to pull out specific types of characters from a string\nextract\nNA\n\n\n\n\n1:05:35\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Dolphins.html",
    "href": "content_pages/Dolphins.html",
    "title": "Dolphins",
    "section": "",
    "text": "Notable topics: Survival analysis\nRecorded on: 2018-12-18\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Dolphins.html#full-screencast",
    "href": "content_pages/Dolphins.html#full-screencast",
    "title": "Dolphins",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Dolphins.html#timestamps",
    "href": "content_pages/Dolphins.html#timestamps",
    "title": "Dolphins",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:6:25\nUsing year function from lubridate package to simplify calculating age of dolphins\nNA\nNA\n\n\n\n\n0:8:30\nCombining count and fct_lump functions to get counts of top 5 species (with other species lumped in “Other”)\ncount | fct_lump\nNA\n\n\n\n\n0:9:55\nCreating boxplot of species and age\nNA\nNA\n\n\n\n\n0:11:50\nDealing with different types of NA (double, logical) (he doesn’t get it in this case, but it’s still useful)\nNA\nNA\n\n\n\n\n0:15:30\nAdding acquisition type as colour dimension to histogram\nNA\nNA\n\n\n\n\n0:16:00\nCreating a spinogram of acquisition type over time (alternative to histogram) using geom_area\ngeom_area\nNA\n\n\n\n\n0:17:25\nBinning year into decade using truncated division operator %/%\n%/%\nNA\n\n\n\n\n0:19:10\nFixing annoying triangular gaps in spinogram using complete function to fill in gaps in data\ncomplete\nNA\n\n\n\n\n0:21:15\nUsing fct_reorder function to reorder acquisition type (bigger categories are placed on the bottom of the spinogram)\nfct_reorder\nNA\n\n\n\n\n0:23:25\nAdding vertical dashed reference line using geom_vline function\ngeom_vline\nNA\n\n\n\n\n0:24:05\nStarting analysis of acquisition location\nNA\nNA\n\n\n\n\n0:27:05\nMatching messy text data with regex to aggregate into a few categories variables with fuzzyjoin package\nregex_left_join\nfuzzyjoin\n\n\n\n\n0:31:30\nUsing distinct function’s .keep_all argument to keep only one row per animal ID\ndistinct\nNA\n\n\n\n\n0:33:10\nUsing coalesce function to conditionally replace NAs (same functionality as SQL verb)\ncoalesce\nNA\n\n\n\n\n0:40:00\nStarting survival analysis\nNA\nNA\n\n\n\n\n0:46:25\nUsing survfit function from survival package to get a baseline survival curve (i.e., not regressed on any independent variables)\nsurvfit\nsurvival\n\n\n\n\n0:47:30\nFixing cases where death year is before birth year\nNA\nNA\n\n\n\n\n0:48:30\nFixing specification of survfit model to better fit the format of our data (right-censored data)\nNA\nNA\n\n\n\n\n0:50:10\nBuilt-in plot of baseline survival model (estimation of percentage survival at a given age)\nNA\nNA\n\n\n\n\n0:50:30\nUsing broom package to tidy the survival model data (which is better for ggplot2 plotting)\ntidy\nbroom\n\n\n\n\n0:52:20\nFitting survival curve based on sex\nNA\nNA\n\n\n\n\n0:54:25\nCox proportional hazards model (to investigate association of survival time and one or more predictors)\nNA\nNA\n\n\n\n\n0:55:50\nExplanation of why dolphins with unknown sex likely have a systematic bias with their data\nNA\nNA\n\n\n\n\n0:57:25\nInvestigating whether being born in captivity is associated with different survival rates\nNA\nNA\n\n\n\n\n1:00:10\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/French Train Delays.html",
    "href": "content_pages/French Train Delays.html",
    "title": "French Train Delays",
    "section": "",
    "text": "Notable topics: Heat map\nRecorded on: 2019-02-26\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/French Train Delays.html#full-screencast",
    "href": "content_pages/French Train Delays.html#full-screencast",
    "title": "French Train Delays",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/French Train Delays.html#timestamps",
    "href": "content_pages/French Train Delays.html#timestamps",
    "title": "French Train Delays",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:10:20\nBoxplots of departure stations using fct_lump function\nfct_lump\nNA\n\n\n\n\n0:14:25\nCreating heat map of departure and arrival delays, then cleaning up a sparse heat map\nNA\nNA\n\n\n\n\n0:15:30\nUsing fct_reorder function and length function to reorder stations based on how frequently they appear\nfct_reorder | length\nNA\n\n\n\n\n0:16:30\nUsing fct_infreq to reorder based on infrequently-appearing stations (same as above, but without a trick needed)\nfct_infreq\nNA\n\n\n\n\n0:17:45\nUsing fct_lump function to lump based on proportion instead of number of top categories desired\nfct_lump\nNA\n\n\n\n\n0:18:45\nUsing scale_fill_gradient2 function to specify diverging colour scale\nscale_fill_gradient2\nNA\n\n\n\n\n0:26:00\nChecking another person’s take on the data, which is a heatmap over time\nNA\nNA\n\n\n\n\n0:28:40\nConverting year and month (as digits) into date-class variable using sprintf function and padding month number with extra zero when necessary\nsprintf\nNA\n\n\n\n\n0:34:50\nUsing summarise_at function to quickly sum multiple columns\nsummarise_at\nNA\n\n\n\n\n0:39:35\nCreating heatmap using geom_tile function for percentage of late trains by station over time\ngeom_tile\nNA\n\n\n\n\n0:45:05\nUsing fill function to fill in missing NA values with data from previous observations\nfill\nNA\n\n\n\n\n0:50:35\nGrouping multiple variables into a single category using paste0 function\npaste0\nNA\n\n\n\n\n0:51:40\nGrouping heatmap into International / National chunks with a weird hack\nNA\nNA\n\n\n\n\n0:52:20\nFurther separating International / National visually\nNA\nNA\n\n\n\n\n0:53:30\nLess hacky way of separating International / National (compared to previous two rows)\nNA\nNA"
  },
  {
    "objectID": "content_pages/Friends.html",
    "href": "content_pages/Friends.html",
    "title": "Friends",
    "section": "",
    "text": "Notable topics: Data Manipulation, Linear Modeling, Pairwise Correlation, Text Mining\nRecorded on: 2020-09-08\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Friends.html#full-screencast",
    "href": "content_pages/Friends.html#full-screencast",
    "title": "Friends",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Friends.html#timestamps",
    "href": "content_pages/Friends.html#timestamps",
    "title": "Friends",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:7:30\nUse dplyr package’s count function to count the unique values of multiple variables.\ncount\ndplyr\n\n\n\n\n0:9:35\nUse geom_col to show how many lines of dialogue there is for each character. Use fct_reorder to reorder the speaker factor levels by sorting along n.\ngeom_col | fct_reorder\nggplot | forcats\n\n\n\n\n0:12:07\nUse semi_join to join friends dataset with main_cast with by = \"speaker returning all rows from friends with a match in main_cast.\nsemi_join\ndplyr\n\n\n\n\n0:12:30\nUse unite to create the episode_number variable which pastes together season and episode with sep = \".\".\nThen, use inner_join to combine above dataset with friends_info with by = c(\"season\", \"episode\").\nThen, use mutate and the glue package instead to combine { season }.{ episode } { title }.\nThen use fct_reorder(episode_title, season + .001 * episode) to order it by season first then episode.\nunite | inner_join | glue | fct_reorder\ntidyr | glue |forcats\n\n\n\n\n0:15:45\nUse geom_point to visualize episode_title and us_views_millions.\nUse as.integer to change episode_title to integer class.\nAdd labels to geom_point using geom_text with check_overlap = TRUE so text that overlaps previous text in the same layer will not be plotted.\ngeom_point | as.integer | geom_text | geom_line\nggplot | base\n\n\n\n\n0:19:95\nRun the above plot again using imdb_rating instead of us_views_millions\ngeom_point | as.integer | geom_text | geom_line\nggplot | base\n\n\n\n\n0:21:35\nAhead of modeling:\nUse geom_boxplot to visualize the distribution of speaking for main characters.\nUse the complete function with fill = list(n = 0) to replace existing explicit missing values in the data set.\nDemonstration of how to account for missing imdb_rating values using the fill function with .direction = \"downup\" to keep the imdb rating across the same title.\nsemi_join | geom_boxplot | coord_flip | fct_reorder | complete | fill | scale_x_log10\ndplyr | ggplot | forcats | tidyr | tidyr\n\n\n\n\n0:26:45\nAhead of modeling:\nUse summarize with cor(log2(n), imdb_rating) to find the correlation between speaker and imdb rating – the fact that the correlation is positive for all speakers gives David a suspicion that some episodes are longer than others because they’re in 2 parts with higher ratings due to important moments. David addresses this confounding factor by including percentage of lines instead of number of lines.\nVisualize results with geom_boxplot, geom_point with geom_smooth.\nsemi_join | summarize | add_count | geom_boxplot | geom_smooth | geom_point\ndplyr | ggplot\n\n\n\n\n0:34:05\nUse a linear model to predict imdb rating based on various variables.\nspread | across | semi_join | lm | aov\ntidyr | dplyr | stats\n\n\n\n\n0:42:00\nUse the tidytext and tidylo packages to see what words are most common amongst characters, and whether they are said more times than would be expected by chance.\nUse geom_col to visualize the most overrepresented words per character according to log_odds_weighted.\nunnest_tokens | anti_join | bind_log_odds | semi_join | geom_col | scale_y_reordered\ntidytext | tidylo | ggplot\n\n\n\n\n0:54:15\nUse the widyr package and pairwise correlation to determine which characters tend to appear in the same scences together?\nUse geom_col to visualize the correlation between characters.\nunite | semi_join | pairwise_corr\nwidyr | tidyr\n\n\n\n\n1:00:25\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/GDPR Violations.html",
    "href": "content_pages/GDPR Violations.html",
    "title": "GDPR Violations",
    "section": "",
    "text": "Notable topics: Data manipulation, Interactive dashboard with shinymetrics and tidymetrics\nRecorded on: 2020-04-21\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/GDPR Violations.html#full-screencast",
    "href": "content_pages/GDPR Violations.html#full-screencast",
    "title": "GDPR Violations",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/GDPR Violations.html#timestamps",
    "href": "content_pages/GDPR Violations.html#timestamps",
    "title": "GDPR Violations",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:05\nUse the mdy function from the lubridate package to change the date variable from character class to date class.\nmdy\nlubridate\n\n\n\n\n0:5:35\nUse the rename function from the dplyr package to rename variable in the dataset.\nrename\ndplyr\n\n\n\n\n0:6:15\nUse the fct_reorder function from the forcats package to sort the geom_col in descending order.\nfct_reorder\nforcats\n\n\n\n\n0:6:30\nUse the fct_lump function from the forcats package within count to lump together country names except for the 6 most frequent.\nfct_lump | count\nforcats | dplyr\n\n\n\n\n0:7:05\nUse the scale_x_continuous function from ggplot2 with the scales package to change the x-axis values to dollar format.\nscale_x_continuous\nggplot2 | scales\n\n\n\n\n0:8:15\nUse the month and floor_date function from the lubridate package to get the month component from the date variable to count the total fines per month.\nmonth\nlubridate\n\n\n\n\n0:8:55\nUse the na_if function from the dplyr package to convert specific date value to NA.\nna_if\ndplyr\n\n\n\n\n0:11:05\nUse the fct_reorder function from the forcats package to sort the stacked geom_col and legend labels in descending order.\nfct_reorder\nforcats | dplyr\n\n\n\n\n0:15:15\nUse the dollar function from the scales package to convert the price variable into dollar format.\ndollar\nscales\n\n\n\n\n0:15:40\nUse the str_trunc to shorten the summary string values to 140 characters.\nstr_trunc\nstringr\n\n\n\n\n0:17:35\nUse the separate_rows function from the tidyr package with a regular expression to separate the values in the article_violated variable with each matching group placed in its own row.\nseparate_rows\ntidyr\n\n\n\n\n0:19:30\nUse the extract function from the tidyr package with a regular expression to turn each matching group into a new column.\nextract\ntidyr\n\n\n\n\n0:27:30\nUse the geom_jitter function from the ggplot2 package to add points to the horizontal box plot.\ngeom_jitter\nggplot2\n\n\n\n\n0:31:55\nUse the inner_join function from the dplyr package to join together article_titles and separated_articles tables.\ninner_join\ndplyr\n\n\n\n\n0:32:55\nUse the paste0 function from base R to concatenate article and article_title.\npaste0\nbase R\n\n\n\n\n0:38:48\nUse the str_detect function from the stringr package to detect the presence of a pattern in a string.\nstr_detect\nstringr\n\n\n\n\n0:40:25\nUse the group_by and summarize functions from the dplyr package to aggregate fines that were issued to the same country on the same day allowing for size to be used in geom_point plot.\ngroup_by | summarize | geom_point\ndplyr | ggplot2\n\n\n\n\n0:41:14\nUse the scale_size_continuous function from the ggplot2 package to remove the size legend.\nggplot2\nggplot2\n\n\n\n\n0:42:55\nCreate an interactive dashboard using the shinymetrics and tidymetrics which is a tidy approach to business intelligence.\npreview_metric\ntidymetrics | shinymetrics\n\n\n\n\n0:47:25\nUse the cross_by_dimensions and cross_by_periods functions from the tidyr package which stacks an extra copy of the table for each dimension specified as an argument (country, article_title, type), replaces the value of the column with the word All and periods, and groups by all the columns. It acts as an extended group_by that allows complete summaries across each individual dimension and possible combinations.\ncross_by_dimensions | cross_by_periods | use_metrics_scaffold | create_metrics\ntidyr"
  },
  {
    "objectID": "content_pages/Global Crop Yields.html",
    "href": "content_pages/Global Crop Yields.html",
    "title": "Global Crop Yields",
    "section": "",
    "text": "Notable topics: Interactive Shiny Dashboard\nRecorded on: 2020-09-01\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Global Crop Yields.html#full-screencast",
    "href": "content_pages/Global Crop Yields.html#full-screencast",
    "title": "Global Crop Yields",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Global Crop Yields.html#timestamps",
    "href": "content_pages/Global Crop Yields.html#timestamps",
    "title": "Global Crop Yields",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:03:35\nUsing rename to shorten column name\nrename\ndplyr\n\n\n\n\n0:06:40\nUsing rename_all with str_remove and regex to remove characters in column name\nrename_all | str_remove\ndplyr | string\n\n\n\n\n0:07:40\nUsing pivot_longer to change data from wide to long\npivot_longer\ntidyr\n\n\n\n\n0:08:25\nCreate a faceted geom_line chart\ngeom_line | facet_wrap\nggplot2\n\n\n\n\n0:09:40\nUsing fct_reorder to reorder facet panels in ascending order\nfct_reorder\nforcats\n\n\n\n\n0:11:50\nCreate an interactive Shiny dashboard\nwrite_rds | inputPanel | renderPlot\nshiny | dplyr | ggplot2 | forcats | stringr | plotly\n\n\n\n\n0:33:20\nCreate a faceted geom_line chart with add_count and filter(n = max(x)) to subset the data for crops that have observations in every year\ngeom_line\nggplot2\n\n\n\n\n0:36:50\nCreate a faceted geom_point chart showing the crop yields at start and end over a 50 year period (1968 start date and 2018 end date)\ngeom_point | geom_abline\nggplot2\n\n\n\n\n0:45:00\nCreate a geom_boxplot to visualize the distribution of yield ratios for the different crops to see how efficiency has increased across countries\ngeom_boxplot\nggplot2\n\n\n\n\n0:46:00\nCreate a geom_col chart to visualize the median yield ratio for each crop\ngeom_col\nggplot2\n\n\n\n\n0:47:50\nCreate a geom_point chart to visualize efficiency imporvement for each country for a specific crop (yield start / yield ratio)\ngeom_point | geom_text_repel\nggplot2 | ggrepel\n\n\n\n\n0:50:25\nUsing the countrycode package to color geom_point chart by continent names\ncountrycode | geom_hline | geom_text_repel\ncountrycode | ggplot2 | ggrepel\n\n\n\n\n0:56:50\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Government Spending on Kids.html",
    "href": "content_pages/Government Spending on Kids.html",
    "title": "Government Spending on Kids",
    "section": "",
    "text": "Notable topics: Data Manipulation, Functions, Embracing, Reading in Many .csv Files, Pairwise Correlation\nRecorded on: 2020-09-15\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Government Spending on Kids.html#full-screencast",
    "href": "content_pages/Government Spending on Kids.html#full-screencast",
    "title": "Government Spending on Kids",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Government Spending on Kids.html#timestamps",
    "href": "content_pages/Government Spending on Kids.html#timestamps",
    "title": "Government Spending on Kids",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:6:15\nUsing geom_line and summarize to visualize education spending over time. First for all states. Then individual states. Then small groups of states using %in%. Then in random groups of size n using %in% and sample with unique. fct_reorder is used to reorder state factor levels by sorting along the inf_adj variable.\ngeom_vline used to add reference to the 2009 financial crisis.\ngeom_line | summarize | unique | sample | facet_wrap | fct_reorder | theme_tufte | geom_vline\nggplot | dplyr | base | ggthemes | forcats\n\n\n\n\n0:16:00\nTake the previous chart setting the inf_adj_perchild for the first year 1997 to 100% in order to show a measure of increase from 100% as opposed to absolute value for change over time for each state relative to 1997. geom_hline used to add reference for the 100% starting point. David ends up changing the starting point from 100% to 0%\nfct_reorder with max used to reorder the plots in descending order based on highest peak values.\nDavid briefly mentions the small multiples approach to analyzing data.\ngeom_line | summarize | unique | sample | facet_wrap | fct_reorder | theme_tufte | geom_vline | geom_hline\nggplot | dplyr | base | ggthemes | forcats\n\n\n\n\n0:23:35\nCreate a function named plot_changed_faceted to make it easier to visualize the many other variables included in the dataset.\nfunction\nNA\n\n\n\n\n0:27:25\nCreate a function named plot_faceted with a {{ y_axis }} embracing argument. Adding this function creates two stages: one for data transformation and another for plotting.\nfunction\nNA\n\n\n\n\n0:37:05\nUse the dir function with pattern and purrr package’s map_df function to read in many different .csv files with GDP values for each state.\nTroubleshooting Can't combine <character> and <double> columns error using function and mutate with across and as.numeric.\nExtract state name from filename using extract from tidyr and regular expression.\ndir | map_df | function | set_names | pivot_longer | separate | extract\nbase | purr | tidyr\n\n\n\n\n0:50:50\nUnsuccessful attempt at importing state population data via a not user friendly dataset from census.gov by skipping the first 3 rows of the Excel file.\nread_xlsx\nreadxl\n\n\n\n\n0:54:22\nUse geom_col to see which states spend the most for each child for a single variable and multiple variables using %in%.\nUse scale_fill_discrete with guide_legend(reverse = TRUE) to change the ordering of the legend.\ngeom_col | fct_reorder | scale_fill_discrete\nggplot | forcats\n\n\n\n\n0:57:40\nUse geom_col and ’pairwise_corrto visualize the correlation between variables across states in 2016 usingpairwise correlation`.\npairwise_corr | fct_reorder\nwidyr\n\n\n\n\n1:02:02\nUse geom_point to plot inf_adjust_perchild_PK12ed versus inf_adj_perchild_highered. geom_text used to apply state names to each point.\npivot_wider | geom_point |geom_text\nggplot | tidyr\n\n\n\n\n1:05:00\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Great American Beer Festival.html",
    "href": "content_pages/Great American Beer Festival.html",
    "title": "Great American Beer Festival",
    "section": "",
    "text": "Notable topics: Log odds ratio, Logistic regression, TIE Fighter plot\nRecorded on: 2020-10-20\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Great American Beer Festival.html#full-screencast",
    "href": "content_pages/Great American Beer Festival.html#full-screencast",
    "title": "Great American Beer Festival",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Great American Beer Festival.html#timestamps",
    "href": "content_pages/Great American Beer Festival.html#timestamps",
    "title": "Great American Beer Festival",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:8:20\nUse pivot_wider with values_fill = list(value =0)) from the tidyr package along with mutate(value = 1) to pivot the medal variable from long to wide adding a 1 for the medal type awarded and 0 for the remaining medal types in the row.\npivot_wider\ntidyr\n\n\n\n\n0:11:25\nUse fct_lump from the forcats package to lump together all the beers except for the N most frequent.\nfct_lump\nforcats\n\n\n\n\n0:12:25\nUse str_to_upper from the stringr package to convert the case of the state variable to uppercase.\nstr_to_upper\nstringr\n\n\n\n\n0:12:25\nUse fct_relevel from the the forcats package in order to reorder the medal factor levels.\nfct_relevel\nforcats\n\n\n\n\n0:13:25\nUse fct_reorder from the forcats package to sort beer_name factor levels by sorting along n.\nfct_reorder\nforcats\n\n\n\n\n0:14:30\nUse glue from the glue package to concatenate beer_name and brewery on the y-axis.\nglue\nglue\n\n\n\n\n0:15:00\nUse ties.mthod = \"first\" within fct_lump to show only the first brewery when a tie exists between them.\nfct_lump\nforcats\n\n\n\n\n0:19:25\nUse setdiff from the dplyr package and the state.abb built in vector from the datasets package to check which states are missing from the dataset.\nstate.abb | setdiff\ndatasets\n\n\n\n\n0:21:25\nUse summarize from the dplyr package to calculate the number of medals with n_medals = n(), number of beers with n_distinct, number of gold medals with sum(), and weighted medal totals using sum(as.integer() because medal is an ordered factor, so 1 for each bronze, 2 for each silver, and 3 for each gold.\nsummarize\ndplyr\n\n\n\n\n0:26:05\nImport Craft Beers Dataset from Kaggle using read_csv from the readr package.\nread_csv\nreadr\n\n\n\n\n0:28:00\nUse inner_join from the dplyr package to join together the 2 datasets from kaggle.\ninner_join\ndplyr\n\n\n\n\n0:29:40\nUse semi_join from the dplyr package to join together to see if the beer names match with the kaggle dataset. Ends up at a dead end with not enough matches between the datasets.\nsemi_join\ndplyr\n\n\n\n\n0:33:05\nUse bind_log_odds from the tidylo package to show the representation of each beer category for each state compared to the categories across the other states.\nbind_log_odds\ntidylo\n\n\n\n\n0:33:35\nUse complete from the tidyr package in order to turn missing values into explicit missing values.\ncomplete\ntidyr\n\n\n\n\n0:35:30\nUse reorder_within from the tidytext package and scale_y_reordered from the tidytext package in order to reorder the bars within each facet panel.\nreorder_within | scale_y_reordered | facet_wrap\ntidytext\n\n\n\n\n0:36:40\nUse fct_reorder from the forcats package to reorder the facet panels in descending order.\nfct_reorder\nforcats\n\n\n\n\n0:39:35\nFor the previous plot, use fill = log_odds_weighted > 0 in the ggplot aes argument to highlight the positive and negative values.\nfill\nggplot2\n\n\n\n\n0:41:45\nUse add_count from the dplyr package to add a year_total variable which shows the total awards for each year. Then use this to calculate the percent change in totals medals per state using mutate(pct_year = n / year)\nadd_count | mutate\ndplyr\n\n\n\n\n0:44:40\nUse glm from the stats package to create a logistic regression model to find out if their is a statistical trend in the probability of award success over time.\nglm | cbind\nstats\n\n\n\n\n0:47:15\nExapnd on the previous model by using the broom package to fit multiple logistic regressions across multiple states instead of doing it for an individual state at a time.\ngroup_by | summarize | list | glm|mutate | map\nbroom | purrr\n\n\n\n\n0:50:25\nUse conf.int = TRUE to add confidence bounds to the logistic regression output then use it to create a TIE Fighter plot to show which states become more or less frequent medal winners over time.\nconf.int\nNA\n\n\n\n\n0:53:00\nUse the state.name dataset with match from base r to change state abbreviation to the state name.\nstate.name | match\ndatasets\n\n\n\n\n0:55:00\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/HBCU Enrollment.html",
    "href": "content_pages/HBCU Enrollment.html",
    "title": "HBCU Enrollment",
    "section": "",
    "text": "Notable topics: Data Cleaning\nRecorded on: 2021-02-02\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/HBCU Enrollment.html#full-screencast",
    "href": "content_pages/HBCU Enrollment.html#full-screencast",
    "title": "HBCU Enrollment",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/HBCU Enrollment.html#timestamps",
    "href": "content_pages/HBCU Enrollment.html#timestamps",
    "title": "HBCU Enrollment",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:45\nDetect the presence or absence of a pattern in a string.\nstr_detect\nstringr\n\n\n\n\n0:3:30\nSeparate a character column into multiple columns with a regular expression or numeric locations\nseparate\ntidyr\n\n\n\n\n0:3:30\nRename column.\nrename\ndplyr\n\n\n\n\n0:4:20\nSelect only unique/distinct rows from a data frame.\ndistinct\ndplyr\n\n\n\n\n0:5:55\nExpand the y axis plot limits by starting at 0.\nexpand_limits\nggplot2\n\n\n\n\n0:6:20\nCombine two datasets while including all rows in x and y.\nfull_join\ndplyr\n\n\n\n\n0:11:00\nY axis labels as percentages (2.5%, 50%, etc).\npercent\nscales\n\n\n\n\n0:12:30\nBind multiple data frames by row and an explanation as to why it’s not the best approach for joining given the other options.\nbind_rows\ndplyr\n\n\n\n\n0:14:55\nBrief discussion on the differences between rbind and row_bind.\nrbind | row_bind\ndplyr | base\n\n\n\n\n0:16:10\nRemove matched patterns in a string.\nstr_remove\nstringr\n\n\n\n\n0:17:10\nTurn variable names into ‘snake case’ (e.g. Standard Error, standard_error).\nclean_names\njanitor\n\n\n\n\n0:18:10\nMutate multiple columns to change type from character to numeric while parsing out the numbers while getting rid of the other characters in the dataset.\nmutate_if | is.character | parse_number\ndplyr | base | readr\n\n\n\n\n0:18:50\nSubset rows using their positions.\nslice\ndplyr\n\n\n\n\n0:20:15\nReshape the data from wide to long such that there is one row for each year and race.\ngather | mutate | ifelse | str_remove | spread\ntidyr | dplyr | stringr | base\n\n\n\n\n0:21:25\nCompute the absolute value of x\nabs\nbase\n\n\n\n\n0:24:55\nRemove matched patterns in a string (e.g. black1, black & white1, white).\nstr_remove\nstringr\n\n\n\n\n0:25:35\nReorder factor levels in geom_line plot by sorting along another variable.\nfct_reorder\nforcats\n\n\n\n\n0:29:25\nBind multiple data frames by row.\nbind_rows\ndplyr\n\n\n\n\n0:36:05\nReorder factor levels by hand.\nfct_relevel\nforcats\n\n\n\n\n0:37:45\nDetect and remove the presence of a pattern in a string to remove duplication from geom_line plot legend.\nstr_remove\nstringr\n\n\n\n\n0:38:50\n“Reorder factor levels in geom_line plot by sorting along another variable with ordering based on the last value to make the data line up with how the values are displayed in the legend. ’fct_reorder(race_ethnicity, percent, last, .desc = TRUE)`”\nfct_reorder\nforcats\n\n\n\n\n0:40:35\nImport external Excel data set from Data.World.\nread_excel\nreadxl\n\n\n\n\n0:44:20\nSelect variables that match a pattern to remove.\nstarts_with\ntidyselect\n\n\n\n\n0:49:00\nUnpack data in one column (field_gender) into two separate columns (field, gender).\nstr_remove | group_by | first | ifelse | cumsum\nstringr | dplyr\n\n\n\n\n0:49:20\nSummary of screencast.\nNA\nNA\n\n\n\n\n0:58:00\nNA\nNA\nNA"
  },
  {
    "objectID": "content_pages/Himalayan Climbers.html",
    "href": "content_pages/Himalayan Climbers.html",
    "title": "Himalayan Climbers",
    "section": "",
    "text": "Notable topics: Data Manipulation, Empirical Bayes, Logistic Regression Model\nRecorded on: 2020-09-22\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Himalayan Climbers.html#full-screencast",
    "href": "content_pages/Himalayan Climbers.html#full-screencast",
    "title": "Himalayan Climbers",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Himalayan Climbers.html#timestamps",
    "href": "content_pages/Himalayan Climbers.html#timestamps",
    "title": "Himalayan Climbers",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:3:00\nCreate a geom_col chart to visualize the top 50 tallest mountains.\nUse fct_reorder to reorder the peak_name factor levels by sorting along the height_metres variable.\nggplot | fct_reorder\nggplot | forcats\n\n\n\n\n0:8:50\nUse summarize with across to get the total number of climbs, climbers, deaths, and first year climbed.\nUse mutate to calculate the percent death rate for members and hired staff.\nUse inner_join and select to join with peaks dataset by peak_id.\nsummarize | across | arrange | mutate | inner_join\ndplyr\n\n\n\n\n0:11:20\nTouching on statistical noise and how it impacts the death rate for mountains with fewer number of climbs, and how to account for it using various statistical methods including Beta Binomial Regression & Empirical Bayes.\nNA\nNA\n\n\n\n\n0:14:30\nFurther description of Empirical Bayes and how to account for not overestimating death rate for mountains with fewer climbers.\nRecommended reading: Introduction to Empirical Bayes: Examples from Baseball Statistics by David Robinson\nNA\nNA\n\n\n\n\n0:17:00\nUse the ebbr package (Empirical Bayes for Binomial in R) to create an Empirical Bayes Estimate for each mountain by fitting prior distribution across data and adjusting the death rates down or up based on the prior distributions.\nUse a geom_point chart to visualize the difference between the raw death rate and new ebbr fitted death rate.\nadd_ebb_estimate | geom_point | geom_abline\nebbr | ggplot\n\n\n\n\n0:21:20\nUse geom_point to visualize how deadly each mountain is with geom_errorbarh representing the 95% credible interval between minimum and maximum values.\nggplot | fct_reorder | geom_errorbarh\nggplot | forcats\n\n\n\n\n0:26:35\nUse geom_point to visualize the relationship between death rate and height of mountain.\nThere is not a clear relationship, but David does briefly mention how one could use Beta Binomial Regression to further inspect for possible relationships / trends.\ngeom_point\nggplot | forcats\n\n\n\n\n0:28:00\nUse geom_histogram and geom_boxplot to visualize the distribution of time it took climbers to go from basecamp to the mountain’s high point for successful climbs only.\nUse mutate to calculate the number of days it took climbers to get from basecamp to the highpoint.\nAdd column to data using case_when and str_detect to identify strings in termination_reason that contain the word Success and rename them to Success & how to use a vector and %in% to change multiple values in termination_reason to NA and rest to Failed.\nUse fct_lump to show the top 10 mountains while lumping the other factor levels (mountains) into other.\nmutate | case_when | str_detect |fct_lump | fct_reorder\ndplyr | stringr | forcats\n\n\n\n\n0:35:30\nFor just Mount Everest, use geom_histogram and geom_density with fill = success to visualize the days from basecamp to highpoint for climbs that ended in success, failure or other.\ngeom_histogram | geom_density\nggplot\n\n\n\n\n0:38:40\nFor just Mount Everest, use geom_histogram to see the distribution of climbs per year.\ngeom_histogram\nggplot\n\n\n\n\n0:39:55\nFor just Mount Everest, use ’geom_lineandgeom_pointto visualizepct_death` over time by decade.\nUse mutate with pmax and integer division to create a decade variable that lumps together the data for 1970 and before.\nmutate | pmax | geom_line | geom_point\nggplot | base | dplyr\n\n\n\n\n0:41:30\nWrite a function for summary statistics such as n_climbs, pct_success, first_climb, pct_death, ’pct_hired_staff_death`.\nfunction\nNA\n\n\n\n\n0:46:20\nFor just Mount Everest, use geom_line and geom_point to visualize pct_success over time by decade.\nmutate | pmax | geom_line | geom_point\nggplot | base | dplyr\n\n\n\n\n0:47:10\nFor just Mount Everest, use geom_line and geom_point to visualize pct_hired_staff_deaths over time by decade.\nDavid decides to visualize the pct_hired_staff_deaths and pct_death charts together on the same plot.\nmutate | pmax | geom_line | geom_point\nggplot | base | dplyr\n\n\n\n\n0:50:45\nFor just Mount Everest, fit a logistic regression model to predict the probability of death with format.pval to calculate the p.value.\nUse fct_lump to lump together all expedition_role factors except for the n most frequent.\nfct_lump | glm | format.pval\nforcats | stats | broom | base\n\n\n\n\n0:56:30\nUse group_by with integer division and summarize to calculate n_climbers and pct_death for age bucketed into decades.\ngroup_by | summarize\ndplyr\n\n\n\n\n0:59:45\nUse geom_point and geom_errorbarh to visualize the logistic regression model with confident intervals.\ngeom_point | geom_errorbarh | conf.int\nggplot | broom\n\n\n\n\n1:03:30\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Historical Phones.html",
    "href": "content_pages/Historical Phones.html",
    "title": "Historical Phones",
    "section": "",
    "text": "Notable topics: Joining tables, Animated world choropleth, Adding IQR to geom_line, World development indicators package\nRecorded on: 2020-11-10\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Historical Phones.html#full-screencast",
    "href": "content_pages/Historical Phones.html#full-screencast",
    "title": "Historical Phones",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Historical Phones.html#timestamps",
    "href": "content_pages/Historical Phones.html#timestamps",
    "title": "Historical Phones",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:15\nUse bind_rows from the dplyr package to combine the two data sets.\nbind_rows\ndplyr\n\n\n\n\n0:7:30\nUse group = interaction(type, country) within ggplot aes() to set the interaction type with every single country on one plot.\nggplot | group\nggplot2\n\n\n\n\n0:9:30\nUse semi_join from the dplyr package to join rows from phones with a match in country_sizes.\nsemi_join | top_n\ndplyr\n\n\n\n\n0:14:00\nUse quantile from the stats package within summarize to show the 25th, and 75th quantiles (interquartile range) on the plot.\nquantile | geom_ribbon\nstats | ggplot2\n\n\n\n\n0:17:50\nImport the wdi package (World Development Indicators from the World Bank) with extra = TRUE in order to get the iso3c code and income level for each country.\nWDI | select\nWDI | dplyr\n\n\n\n\n0:19:45\nUse inner_join from the dplyr package to join the WDI data with the phones data.\ninner_join\ndplyr\n\n\n\n\n0:20:35\nUse fct_relevel from the forcats package to reorder income factor levels in ascending order.\nfct_relevel\nforcats\n\n\n\n\n0:21:05\nCreate an anonymous function using . (dot).\n.\nNA\n\n\n\n\n0:29:30\nUse inner_join from the dplyr package to join the mobile data and landline data together with a geom_abline to see how different the total populations are between the two datasets.\ninner_join | geom_abline\ndplyr | ggplot2\n\n\n\n\n0:31:00\nUse geom_hline to add a refrence line to the plot shwoing when each country crossed the 50 per 100 subscription mark.\ngeom_hline\nggplot2\n\n\n\n\n0:35:20\nUse summarize from the dplyr package with min(year([Mobile >= 50])) to find the year in which each country crossed the 50 per 100 subscription mark.\nsummarize | min\ndplyr\n\n\n\n\n0:35:20\nUse summarize from the dplyr package with max(Mobile) to find the peak number of mobile subscriptions per country.\nsummarize | max\ndplyr\n\n\n\n\n0:35:20\nUse na_if from the dplyr package within summarize to change Inf to NA.\nna_if | summarize\ndplyr\n\n\n\n\n0:38:20\nUsing the WDIsearch function to search the WDI package for proper GDP per capita indicator. Ended up using the NY.GDP.PCAP.PP.KD indicator.\nWDIsearch\nWDI\n\n\n\n\n0:39:05\nAdding the GDP data from the WDI package to the country_incomes table.\nWDI | select\nWDI | dplyr\n\n\n\n\n0:39:52\nUsing the inner_join function from the dplyr package to join the phones table with the country_incomes table pulling in the gdp_per_capita variable.\ninner_join\ndplyr\n\n\n\n\n0:42:25\nUsing the WDIsearch function to search the WDI package for proper population indicator. Ended up using the SP.POP.TOTL indicator.\nWDIsearch\nWDI\n\n\n\n\n0:50:00\nCreate an animated choropleth world map with fill = subscriptions.\nmap_data | iso3166 | regex_left_join | left_join | transition_manual\nmaps | ggplot2 | fuzzyjoin | gganimate\n\n\n\n\n1:00:00\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Horror Movie Profits.html",
    "href": "content_pages/Horror Movie Profits.html",
    "title": "Horror Movie Profits",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2018-10-23\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Horror Movie Profits.html#full-screencast",
    "href": "content_pages/Horror Movie Profits.html#full-screencast",
    "title": "Horror Movie Profits",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Horror Movie Profits.html#timestamps",
    "href": "content_pages/Horror Movie Profits.html#timestamps",
    "title": "Horror Movie Profits",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:50\nUsing parse_date function from lubridate package to convert date formatted as character to date class (should have used mdy function though)\nparse_date\nlubridate\n\n\n\n\n0:7:45\nUsing fct_lump function to aggregate distributors into top 6 (by number of movies) and and “Other” category\nfct_lump\nNA\n\n\n\n\n0:8:50\nInvestigating strange numbers in the data and discovering duplication\nNA\nNA\n\n\n\n\n0:12:40\nUsing problems function to look at parsing errors when importing data\nproblems\nNA\n\n\n\n\n0:14:35\nUsing arrange and distinct function and its .keep_all argument to de-duplicate observations\narrange | distinct\nNA\n\n\n\n\n0:16:10\nUsing geom_boxplot function to create a boxplot of budget by distributor\ngoem_boxplot\nNA\n\n\n\n\n0:19:20\nUsing floor function to bin release years into decades (e.g., “1970” and “1973” both become “1970”)\nfloor\nNA\n\n\n\n\n0:21:30\nUsing summarise_at function to apply the same function to multiple variables at the same time\nsummarise_at\nNA\n\n\n\n\n0:24:10\nUsing geom_line to visualize multiple metrics at the same time\ngeom_line\nNA\n\n\n\n\n0:26:00\nUsing facet_wrap function to graph small multiples of genre-budget boxplots by distributor\nfacet_wrap\nNA\n\n\n\n\n0:28:35\nStarting analysis of profit ratio of movies\nNA\nNA\n\n\n\n\n0:32:50\nUsing paste0 function in a custom function to show labels of multiple (e.g., “4X” or “6X” to mean “4 times” or “6 times”)\npaste0\nNA\n\n\n\n\n0:41:20\nStarting analysis of the most common genres over time\nNA\nNA\n\n\n\n\n0:45:55\nStarting analysis of the most profitable individual horror movies\nNA\nNA\n\n\n\n\n0:51:45\nUsing paste0 function to add release date of movie to labels in a bar graph\npaste0\nNA\n\n\n\n\n0:53:25\nUsing geom_text function, along with its check_overlap argument, to add labels to some points on a scatterplot\ngeom_text\nNA\n\n\n\n\n0:58:10\nUsing ggplotly function from plotly package to create an interactive scatterplot\nggplotly\nplotly\n\n\n\n\n1:00:55\nReviewing unexplored areas of investigation\nNA\nNA"
  },
  {
    "objectID": "content_pages/Horror Movies.html",
    "href": "content_pages/Horror Movies.html",
    "title": "Horror Movies",
    "section": "",
    "text": "Notable topics: ANOVA, Text mining (tidytext package), LASSO regression (glmnet package)\nRecorded on: 2019-10-22\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Horror Movies.html#full-screencast",
    "href": "content_pages/Horror Movies.html#full-screencast",
    "title": "Horror Movies",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Horror Movies.html#timestamps",
    "href": "content_pages/Horror Movies.html#timestamps",
    "title": "Horror Movies",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:15\nExtracting digits (release year) from character string using regex, along with good explanation of extract function\nextract\nNA\n\n\n\n\n0:8:00\nQuick check on why parse_number is unable to parse some values – is it because they are NA or some other reason?\nparse_number\nNA\n\n\n\n\n0:9:45\nVisually investigating correlation between budget and rating\nNA\nNA\n\n\n\n\n0:11:50\nInvestigating correlation between MPAA rating (PG-13, R, etc.) and rating using boxplots\nNA\nNA\n\n\n\n\n0:12:50\nUsing pull function to quickly check levels of a factor\npull\nNA\n\n\n\n\n0:13:30\nUsing ANOVA to check difference of variation within groups (MPAA rating) than between groups\nNA\nNA\n\n\n\n\n0:15:40\nSeparating genre using separate_rows function (instead of str_split and unnest)\nseparate_rows\nNA\n\n\n\n\n0:18:00\nRemoving boilerplate “Directed by…” and “With…” part of plot variable and isolating plot, first using regex, then by using separate function with periods as separator\nseparate\nNA\n\n\n\n\n0:20:40\nUnnesting word tokens, removing stop words, and counting appearances\nNA\nNA\n\n\n\n\n0:21:20\nAggregating by word to find words that appear in high- or low-rated movies\nNA\nNA\n\n\n\n\n0:23:00\nDiscussing potential confounding factors for ratings associated with specific words\nNA\nNA\n\n\n\n\n0:24:50\nSearching for duplicated movie titles\nNA\nNA\n\n\n\n\n0:25:50\nDe-duping using distinct function\nNA\nNA\n\n\n\n\n0:26:55\nLoading in and explaining glmnet package\nNA\nglmnet\n\n\n\n\n0:28:00\nUsing movie titles to pull out ratings using rownmaes and match functions to create an index of which rating to pull out of the original dataset\nNA\nNA\n\n\n\n\n0:29:10\nActually using glmnet function to create lasso model\nNA\nNA\n\n\n\n\n0:34:05\nShowing built-in plot of lasso lambda against mean-squared error\nNA\nNA\n\n\n\n\n0:37:05\nExplaining when certain terms appeared in the lasso model as the lambda value dropped\nNA\nNA\n\n\n\n\n0:41:10\nGathering all variables except for title, so that the dataset is very tall\nNA\nNA\n\n\n\n\n0:42:35\nUsing unite function to combine two variables (better alternative to paste)\nunite\nNA\n\n\n\n\n0:45:45\nCreating a new lasso with tons of new variables other than plot words\nNA\nNA"
  },
  {
    "objectID": "content_pages/IKEA Furniture.html",
    "href": "content_pages/IKEA Furniture.html",
    "title": "IKEA Furniture",
    "section": "",
    "text": "Notable topics: Linear model, Coefficient/TIE fighter plot, Boxplots, Log scale discussion, Calculating volume\nRecorded on: 2020-11-03\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/IKEA Furniture.html#full-screencast",
    "href": "content_pages/IKEA Furniture.html#full-screencast",
    "title": "IKEA Furniture",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/IKEA Furniture.html#timestamps",
    "href": "content_pages/IKEA Furniture.html#timestamps",
    "title": "IKEA Furniture",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:30\nUse fct_reorder from the forcats package to reorder the factor levels for category sorted along n.\nfct_reorder\nforcats\n\n\n\n\n0:6:00\nBrief explanation of why scale_x_log10 is needed given the distribution of category and price with geom_boxplot.\nscale_x_log_10 | geom_boxplot\nggplot2\n\n\n\n\n0:7:00\nUsing geom_jitter with geom_boxplot to show how many items are within each category.\ngeom_jitter | geom_boxplot\nggplot2\n\n\n\n\n0:8:00\nUse add_count from the dplyr package and glue from the glue package to concatenate the category name with category_total on the geom_boxplot y-axis.\nglue | add_count\nglue | dplyr\n\n\n\n\n0:9:00\nConvert from Saudi Riyals to United States Dollars.\nmutate\ndplyr\n\n\n\n\n0:11:05\nCreate a ridgeplot - AKA joyplot - using ggridges package showing the distribution of price across category.\ngeom_density_ridges\nggridges\n\n\n\n\n0:12:50\nDiscussion on distributions and when to use a log scale.\nNA\nNA\n\n\n\n\n0:19:20\nUse fct_lump from the forcats package to lump together all the levels in category except for the n most frequent.\nfct_lump\nforcats\n\n\n\n\n0:21:00\nUse scale_fill_discrete from the ggplot2 package with guide = guide_legend(reverse = TRUE) to reverse the fill legend.\nscale_fill_discrete\nggplot2\n\n\n\n\n0:24:20\nUse str_trim from the stringr package to remove whitespace from the short_description variable. David then decides to use str_replace_all instead with the following regular expression \"\\\\s+\", \" \" to replace all whitespace with a single space instead.\nstr_trim | str_replace_all\nstringr\n\n\n\n\n0:25:30\nUse separate from the tidyr package with extra = \"merge\" and fill = \"right\" to separate item description from item dimension.\nseparate\ntidyr\n\n\n\n\n0:26:45\nUse extract from the tidyr package with the regular expression \"[\\\\d\\\\-xX]+) cm\" to extract the numbers before cm.\nextract\ntidyr\n\n\n\n\n0:29:50\nUse unite from the tidyr package to paste together the category and main_description columns into a new column named category_and_description.\nunite\ntidyr\n\n\n\n\n0:32:45\nCalculate the volume given the depth, height, and width of each item in dataset in liters using depth * height * width / 1000. At 36:15, David decides to change to cubic meters instead using depth * height * width / 1000000.\nmutate\ndplyr\n\n\n\n\n0:44:20\nUse str_squish from the stringr package to remove whitespace from the start to the end of the short_description variable.\nstr_squish\nstringr\n\n\n\n\n0:48:00\nUse lm from the stats package to create a linear model on a log, log scale to predict the price of an item based on volume + category. David then uses fct_relevel to reorder the factor levels for category such that tables & desks is first (starting point) since it’s the most frequent item in the category variable and it’s price distribution is in the middle.\nlm\nstats\n\n\n\n\n0:53:00\nUse the broom package to turn the model output into a coefficient / TIE fighter plot.\ntidy | geom_point | geom_errorbarh | geom_vline\nbroom\n\n\n\n\n0:56:20\nUse str_remove from the stringr package to remove category from the start of the strings on the y-axis using the regular expression \"^category\"\nstr_remove\nstringr\n\n\n\n\n0:57:50\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Kenya Census.html",
    "href": "content_pages/Kenya Census.html",
    "title": "Kenya Census",
    "section": "",
    "text": "Notable topics: Heatmap, Joining Datasets, ShapeFile, Choropleth Map, rKenyaCensus\nRecorded on: 2021-01-19\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Kenya Census.html#full-screencast",
    "href": "content_pages/Kenya Census.html#full-screencast",
    "title": "Kenya Census",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Kenya Census.html#timestamps",
    "href": "content_pages/Kenya Census.html#timestamps",
    "title": "Kenya Census",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:15\nTrim whitespace from a string.\nstr_trim\nstringr\n\n\n\n\n0:4:35\nReorder factor levels by sorting along another variable by sum.\nfct_reorder\nforcats\n\n\n\n\n0:5:00\nLabel x axis numbers in decimal format (e.g. 0.12, 1,234).\ncomma | scale_x_continuous\nscales | ggplot2\n\n\n\n\n0:5:15\nPivot data from wide to long.\ngather\ntidyr\n\n\n\n\n0:5:25\nConvert case of a string to title case.\nstr_to_title\nstringr\n\n\n\n\n0:6:50\nAdd text labels to the geom_point plot.\ngeom_text\nggplot2\n\n\n\n\n0:7:35\nAdd horizontal reference line to geom_point plot.\ngeom_hline\nggplot2\n\n\n\n\n0:7:35\nLabel y axis numbers in percent format.\ncomma | scale_y_continuous\nscales | ggplot2\n\n\n\n\n0:9:00\nExpand the plot axis limits by having the y axis begin at 0.\nexpand_limits\nggplot2\n\n\n\n\n0:9:10\nPosition x axis data on a log10 scale.\nscale_x_log10\nggplot2\n\n\n\n\n0:10:20\nPivot data from wide to long.\ngather\ntidyr\n\n\n\n\n0:11:15\nConvert case of a string to title case.\nstr_to_title\nstringr\n\n\n\n\n0:11:50\nReorder factor levels by sorting along another variable by sum.\nfct_reorder\nforcats\n\n\n\n\n0:14:45\nCreate a heatmap.\ngeom_tile\nggplot2\n\n\n\n\n0:15:13\nComplete a data frame with missing combinations of data.\ncomplete\ntidyr\n\n\n\n\n0:15:30\nRotate x axis labels 90 degrees.\ntheme\nggplot2\n\n\n\n\n0:16:55\nJoin two datasets while including all rows in x or y.\nfull_join\ndplyr\n\n\n\n\n0:18:00\nReplace matched patterns in a string using str_replace_all with the regular expression ([a-z])([A-Z]) and \"\\\\1 \\\\2\" to separate words that were joined together (e.g. TanaRiver, Tana River).\nstr_replace_all\nstringr\n\n\n\n\n0:19:40\nJoin two datasets while returning all rows from x without a match in y.\nanti_join\ndplyr\n\n\n\n\n0:19:40\nJoin two datasets while including all rows in y.\nright_join\ndplyr\n\n\n\n\n0:19:40\nJoin two datasets while including all rows in x and y.\ninner_join\ndplyr\n\n\n\n\n0:27:35\nImport and basic exploration of the rKenyaCensus package shapefiles.\nKenyaCounties_SHP\nrKenyaCensus\n\n\n\n\n0:28:15\nCreate a map using the rKenyaCensus shapefile data.\nst_as_sf | geom_sf\nggplot2 | sf\n\n\n\n\n0:35:00\nSimplify the shapefile data to make for faster processesing.\nst_simplify\nsf\n\n\n\n\n0:36:20\nJoin two datasets while including all rows in x.\nleft_join\ndplyr\n\n\n\n\n0:37:25\nCreate a choropleth map - TROUBLSHOOTING through 41:45.\nggplot | geom_sf | theme_map\nggplot2 | sf\n\n\n\n\n0:43:20\nCreate a flexible function that generates geom_col plots used for for exploring the many different datasets in the rKenyaCensus package.\nfilter | gather | mutate | group_by | summarize | spread | select\nggplot2 | dplyr | tidyr\n\n\n\n\n0:51:55\nLump together factor levels into “other”.\nfct_lump\nforcats\n\n\n\n\n0:59:20\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Malaria Incidence.html",
    "href": "content_pages/Malaria Incidence.html",
    "title": "Malaria Incidence",
    "section": "",
    "text": "Notable topics: Map visualization\nRecorded on: 2018-11-12\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Malaria Incidence.html#full-screencast",
    "href": "content_pages/Malaria Incidence.html#full-screencast",
    "title": "Malaria Incidence",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Malaria Incidence.html#timestamps",
    "href": "content_pages/Malaria Incidence.html#timestamps",
    "title": "Malaria Incidence",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:45\nImporting data using the malariaAtlas package\nNA\nmalariaAtlas\n\n\n\n\n0:14:10\nUsing geom_line function to visualize malaria prevalence over time\ngeom_line\nNA\n\n\n\n\n0:15:10\nQuick map visualization using longitude and latitude coordinates and the geom_point function\ngeom_point\nNA\n\n\n\n\n0:18:40\nUsing borders function to add Kenyan country borders to map\nborders\nNA\n\n\n\n\n0:19:50\nUsing scale_colour_gradient2 function to change the colour scale of points on the map\nscale_colour_gradient2\nNA\n\n\n\n\n0:20:40\nUsing arrange function to ensure that certain points on a map appear in front of/behind other points\narrange\nNA\n\n\n\n\n0:21:50\nAggregating data into decades using the truncated division operator %/%\n%/%\nNA\n\n\n\n\n0:24:45\nStarting to look at aggregated malaria data (instead of country-specific data)\nNA\nNA\n\n\n\n\n0:26:50\nUsing sample and unique functions to randomly select a few countries, which are then graphed\nsample | unique\nNA\n\n\n\n\n0:28:30\nUsing last function to select the most recent observation from a set of arranged data\nlast\nNA\n\n\n\n\n0:32:55\nCreating a Bland-Altman plot to explore relationship between current incidence and change in incidence in past 15 years\nNA\nNA\n\n\n\n\n0:35:45\nUsing anti_join function to find which countries are not in the malaria dataset\nanti_join\nNA\n\n\n\n\n0:36:40\nUsing the iso3166 dataset set in the maps package to match three-letter country code (i.e., the ISO 3166 code) with country names\nNA\nmaps\n\n\n\n\n0:38:30\nCreating a world map using geom_polygon function (and eventually theme_void and coord_map functions)\ngeom_polygon | theme_void | coord_map\nNA\n\n\n\n\n0:39:00\nGetting rid of Antarctica from world map\nNA\nNA\n\n\n\n\n0:42:35\nUsing facet_wrap function to create small multiples of world map for different time periods\nfacet_wrap\nNA\n\n\n\n\n0:47:30\nStarting to create an animated map of malaria deaths (actual code writing starts at 57:45)\nNA\nNA\n\n\n\n\n0:51:25\nStarting with a single year after working through some bugs\nNA\nNA\n\n\n\n\n0:52:10\nUsing regex_inner_join function from the fuzzyjoin package to join map datasets because one of them has values in regular expressions\nregex_inner_join\nfuzzyjoin\n\n\n\n\n0:55:15\nAs alternative to fuzzyjoin package in above step, using str_remove function to get rid of unwanted regex\nstr_remove\nNA\n\n\n\n\n0:57:45\nStarting to turn static map into an animation using gganimate package\nNA\ngganimate\n\n\n\n\n1:02:00\nThe actual animated map\nNA\nNA\n\n\n\n\n1:02:35\nUsing countrycode package to filter down to countries in a specific continent (Africa, in this case)\ncountrycode\ncountrycode\n\n\n\n\n1:03:55\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Maryland Bridges.html",
    "href": "content_pages/Maryland Bridges.html",
    "title": "Maryland Bridges",
    "section": "",
    "text": "Notable topics: Data manipulation, Map visualization\nRecorded on: 2018-11-27\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Maryland Bridges.html#full-screencast",
    "href": "content_pages/Maryland Bridges.html#full-screencast",
    "title": "Maryland Bridges",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Maryland Bridges.html#timestamps",
    "href": "content_pages/Maryland Bridges.html#timestamps",
    "title": "Maryland Bridges",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:9:15\nUsing geom_line to create an exploratory line graph\ngeom_line\nNA\n\n\n\n\n0:10:10\nUsing %/% operator (truncated division) to bin years into decades (e.g., 1980, 1984, and 1987 would all become “1980”)\n%/%\nNA\n\n\n\n\n0:12:30\nConverting two-digit year to four-digit year (e.g., “16” becomes “2016”) by adding 2000 to each one\nNA\nNA\n\n\n\n\n0:15:40\nUsing percent_format function from scales package to get nice-looking axis labels\npercent_format\nscales\n\n\n\n\n0:19:55\nUsing geom_col to create an ordered nice bar/column graph\ngeom_col\nNA\n\n\n\n\n0:21:35\nUsing replace_na to replace NA values with “Other”\nreplace_na\nNA\n\n\n\n\n0:27:15\nStarting exploration of average daily traffic\nNA\nNA\n\n\n\n\n0:29:05\nUsing comma_format function from scales package to get more readable axis labels (e.g., “1e+05” becomes “100,000”)\ncomma_format\nscales\n\n\n\n\n0:31:15\nUsing cut function to bin continuous variable into customized breaks (also does a mutate within a group_by!)\ncut\nNA\n\n\n\n\n0:34:30\nStarting to make a map\nNA\nNA\n\n\n\n\n0:37:00\nEncoding a continuous variable to colour, then using scale_colour_gradient2 function to specify colours and midpoint\nscale_colour_gradient2\nNA\n\n\n\n\n0:38:20\nSpecifying the trans argument (transformation) of the scale_colour_gradient2 function to get a logarithmic scale\nscale_colour_gradient2\nNA\n\n\n\n\n0:45:55\nUsing str_to_title function to get values to Title Case (first letter of each word capitalized)\nstr_to_title\nNA\n\n\n\n\n0:48:35\nPredicting whether bridges are in “Good” condition using logistic regression (remember to specify the family argument! Dave fixes this at 52:54)\nglm\nNA\n\n\n\n\n0:50:30\nExplanation of why we should NOT be using an OLS linear regression\nNA\nNA\n\n\n\n\n0:51:10\nUsing the augment function from the broom package to illustrate why a linear model is not a good fit\naugment\nbroom\n\n\n\n\n0:52:05\nSpecifying the type.predict argument in the augment function so that we get the actual predicted probability\naugment\nbroom\n\n\n\n\n0:54:40\nExplanation of why the sigmoidal shape of logistic regression can be a drawback\nNA\nNA\n\n\n\n\n0:55:05\nUsing a cubic spline model (a type of GAM, Generalized Additive Model) as an alternative to logistic regression\nNA\nNA\n\n\n\n\n0:56:00\nExplanation of the shape that a cubic spline model can take (which logistic regression cannot)\nNA\nNA\n\n\n\n\n1:02:15\nVisualizing the model in a different way, using a coefficient plot\nNA\nNA\n\n\n\n\n1:04:35\nUsing geom_vline function to add a red reference line to a graph\ngeom_vline\nNA\n\n\n\n\n1:04:50\nAdding confidence intervals to the coefficient plot by specifying conf.int argument of tidy function and graphing using the geom_errorbarh function\ntidy | geom_errorbarh\nNA\n\n\n\n\n1:05:35\nBrief explanation of log-odds coefficients\nNA\nNA\n\n\n\n\n1:09:10\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Media Franchise Revenue.html",
    "href": "content_pages/Media Franchise Revenue.html",
    "title": "Media Franchise Revenue",
    "section": "",
    "text": "Notable topics: Data manipulation (especially re-ordering factors)\nRecorded on: 2019-06-22\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Media Franchise Revenue.html#full-screencast",
    "href": "content_pages/Media Franchise Revenue.html#full-screencast",
    "title": "Media Franchise Revenue",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Media Franchise Revenue.html#timestamps",
    "href": "content_pages/Media Franchise Revenue.html#timestamps",
    "title": "Media Franchise Revenue",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:9:15\nExplaining use of semi_join function to aggregate and filter groups\nsemi_join\nNA\n\n\n\n\n0:11:00\nPutting the largest categories on the bottom of a stacked bar chart\nNA\nNA\n\n\n\n\n0:14:30\nUsing glue function as alternative to paste for combining text, plus good explanation of it\nglue\nglue\n\n\n\n\n0:19:30\nMultiple re-ordering using fct_reorder function of facetted graph (he works through several obstacles)\nfct_reorder\nNA\n\n\n\n\n0:20:40\nRe-ordering the position of facetted graphs so that highest total revenue is at top left\nNA\nNA\n\n\n\n\n0:26:00\nInvestigating relationship between year created and revenue\nNA\nNA\n\n\n\n\n0:26:40\nCreating scatter plot with points scaled by size and labelled points (geom_text function)\ngeom_text\nNA\n\n\n\n\n0:29:30\nSummary of screencast up to this point\nNA\nNA\n\n\n\n\n0:29:50\nStarting analysis original media of franchise (e.g., novel, video game, animated film) and revenue type (e.g., box office, merchandise)\nNA\nNA\n\n\n\n\n0:33:35\nGraphing original media and revenue category as facetted bar plot with lots of reordering (ends at around 38:40)\nfct_reorder\nNA\n\n\n\n\n0:40:30\nAlternative visualization of original media/revenue category using heat map\ngeom_tile\nNA\n\n\n\n\n0:41:20\nUsing scale_fill_gradient2 function to specify custom colour scale\nscale_fill_gradient2\nNA\n\n\n\n\n0:42:05\nGetting rid of gridlines in graph using theme function’s panel.grid argument\nNA\nNA\n\n\n\n\n0:44:05\nUsing fct_rev function to reverse levels of factors\nfct_rev\nNA\n\n\n\n\n0:44:35\nFixing overlapping axis text with tweaks to theme function’s axis.text argument\nNA\nNA\n\n\n\n\n0:46:05\nReviewing visualization that inspired this dataset\nNA\nNA\n\n\n\n\n0:47:25\nAdding text of total revenue to the end of each bar in a previous graph\nNA\nNA\n\n\n\n\n0:50:20\nUsing paste0 function at add a “B” (for “billions”) to the end of text labels on graph\npaste0\nNA\n\n\n\n\n0:51:35\nUsing expand_limits functions to give more space for text labels not to get cut off\nexpand_limits\nNA\n\n\n\n\n0:53:45\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Medium Articles.html",
    "href": "content_pages/Medium Articles.html",
    "title": "Medium Articles",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package)\nRecorded on: 2018-12-04\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Medium Articles.html#full-screencast",
    "href": "content_pages/Medium Articles.html#full-screencast",
    "title": "Medium Articles",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Medium Articles.html#timestamps",
    "href": "content_pages/Medium Articles.html#timestamps",
    "title": "Medium Articles",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:40\nUsing summarise_at and starts_with functions to quickly sum up all variables starting with “tag_”\nsummarise_at | starts_with\nNA\n\n\n\n\n0:6:55\nUsing gather function (now pivot_longer) to convert topic tag variables from wide to tall (tidy) format\ngather\nNA\n\n\n\n\n0:8:10\nExplanation of how gathering step above will let us find the most/least common tags\nNA\nNA\n\n\n\n\n0:9:00\nExplanation of using median (instead of mean) as measure of central tendency for number of claps an article got\nmedian\nNA\n\n\n\n\n0:9:50\nVisualizing log-normal (ish) distribution of number of claps an article gets\nNA\nNA\n\n\n\n\n0:12:05\nUsing pmin function to bin reading times of 10 minutes or more to cap out at 10 minutes\npmin\nNA\n\n\n\n\n0:12:35\nChanging scale_x_continuous function’s breaks argument to get custom labels and tick marks on a histogram\nscale_x_continuous\nNA\n\n\n\n\n0:14:35\nDiscussion of using mean vs. median as measure of central tendency for reading time (he decides on mean)\nNA\nNA\n\n\n\n\n0:16:00\nStarting text mining analysis\nNA\nNA\n\n\n\n\n0:16:40\nUsing unnest_tokens function from tidytext package to split character string into individual words\nunnest_tokens\ntidytext\n\n\n\n\n0:17:50\nExplanation of stop words and using anti_join function from tidytext package to get rid of them\nanti_join\ntidytext\n\n\n\n\n0:20:20\nUsing str_detect function to filter out “words” that are just numbers (e.g., “2”, “35”)\nstr_detect\nNA\n\n\n\n\n0:22:35\nQuick analysis of which individual words are associated with more/fewer claps (“What are the hype words?”)\nNA\nNA\n\n\n\n\n0:25:15\nUsing geometric mean as alternative to median to get more distinction between words (note 27:33 where he makes a quick fix)\nNA\nNA\n\n\n\n\n0:28:10\nStarting analysis of clusters of related words (e.g., “neural” is linked to “network”)\nNA\nNA\n\n\n\n\n0:30:30\nFinding correlations pairs of words using pairwise_cor function from widyr package\npairwise_cor\nwidyr\n\n\n\n\n0:34:00\nUsing ggraph and igraph packages to make network plot of correlated pairs of words\nNA\nggraph | igraph\n\n\n\n\n0:35:00\nUsing geom_node_text to add labels for points (vertices) in the network plot\ngeom_node_text\nNA\n\n\n\n\n0:38:40\nFiltering original data to only include words appear in the network plot (150 word pairs with most correlation)\nNA\nNA\n\n\n\n\n0:40:10\nAdding colour as a dimension to the network plot, representing geometric mean of claps\nNA\nNA\n\n\n\n\n0:40:50\nChanging default colour scale to one with Blue = Low and High = Red with scale_colour_gradient2 function\nscale_colour_gradient2\nNA\n\n\n\n\n0:43:15\nAdding dark outlines to points on network plot with a hack\nNA\nNA\n\n\n\n\n0:44:45\nStarting to predict number of claps based on title tag (Lasso regression)\nNA\nNA\n\n\n\n\n0:45:50\nExplanation of data format needed to conduct Lasso regression (and using cast_sparse function to get sparse matrix)\ncast_sparse\nNA\n\n\n\n\n0:47:45\nBringing in number of claps to the sparse matrix (un-tidy methods)\nNA\nNA\n\n\n\n\n0:49:00\nUsing cv.glmnet function (cv = cross validated) from glmnet package to run Lasso regression\ncv.glmnet\nglmnet\n\n\n\n\n0:49:55\nFinding and fixing mistake in defining Lasso model\nNA\nNA\n\n\n\n\n0:51:05\nExplanation of Lasso model\nNA\nNA\n\n\n\n\n0:52:35\nUsing tidy function from the broom package to tidy up the Lasso model\ntidy\nbroom\n\n\n\n\n0:54:35\nVisualizing how specific words affect the prediction of claps as lambda (Lasso’s penalty parameter) changes\nNA\nNA\n\n\n\n\n1:00:20\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/NCAA Women's Basketball.html",
    "href": "content_pages/NCAA Women's Basketball.html",
    "title": "NCAA Women’s Basketball",
    "section": "",
    "text": "Notable topics: Heatmap, Correlation analysis\nRecorded on: 2020-10-06\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/NCAA Women's Basketball.html#full-screencast",
    "href": "content_pages/NCAA Women's Basketball.html#full-screencast",
    "title": "NCAA Women’s Basketball",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/NCAA Women's Basketball.html#timestamps",
    "href": "content_pages/NCAA Women's Basketball.html#timestamps",
    "title": "NCAA Women’s Basketball",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:15:00\nUse fct_relevel from the forcats package to order the factor levels for the tourney_finish variable.\nfct_relevel\nforcats\n\n\n\n\n0:16:35\nUse geom_tile from the ggplot2 package to create a heatmap to show how far a particular seed ends up going in the tournament.\ngeom_tile | scale_fill_gradient2\nggplot2\n\n\n\n\n0:20:35\nUse scale_y_continuous from the ggplot2 package with breaks = seq(1, 16) in order to include all 16 seeds.\nscale_y_continuous\nggplot2\n\n\n\n\n0:20:55\nUse geom_text from the ggplot2 package with label = percent(pct) to apply the percentage to each tile in the heatmap.\ngeom_text | scales\nggplot2\n\n\n\n\n0:21:40\nUse scale_x_discrete and scale_y_continuous both with expand = c(0, 0) to remove the space between the x and y axis and the heatmap tiles. David calls this flattening.\nscale_x_discrete | scale_y_continuous\nggplot2\n\n\n\n\n0:32:15\nUse scale_y_reverse to flip the order of the y-axis from 1-16 to 16-1.\nscale_y_reverse\nggplot2\n\n\n\n\n0:34:45\nUse cor from the stats package to calculate the correlation between seed and tourney_finish. Then plotted to determine if there is a correlation over time.\ncor | geom_line\nstats | ggplot2\n\n\n\n\n0:39:50\nUse geom_smooth with method = \"loess\" to add a smoothing line with confidence bound to aid in seeing the trend between seed and reg_percent.\ngeom_smooth\nggplot2\n\n\n\n\n0:42:10\nUse fct_lump from the forcats package to lump together all the conference except for the n most frequent.\nfct_lump\nforcats\n\n\n\n\n0:42:55\nUse geom_jitter from the ggplot2 package instead of geom_boxplot to avoid overplotting which makes it easier to visualize the points that make up the distribution of the seed variable.\ngeom_jitter\nggplot2\n\n\n\n\n0:47:05\nUse geom_smooth with method = \"lm\" to aid in seeing the trend between reg_percent and tourney_w.\ngeom_smooth\nggplot2\n\n\n\n\n0:54:20\nCreate a dot pipe function using . and %>% to avoid duplicating summary statistics with summarize.\n. | %>%\nNA\n\n\n\n\n0:56:35\nUse glue from the glue package to concatenate together school and n_entries on the geo_col y-axis.\nglue\nglue\n\n\n\n\n0:59:50\nSummary of screencast.\nNA\nNA"
  },
  {
    "objectID": "content_pages/Ninja Warrior.html",
    "href": "content_pages/Ninja Warrior.html",
    "title": "Ninja Warrior",
    "section": "",
    "text": "Notable topics: Log-odds with tidylo package, Graphing with ggplot2\nRecorded on: 2020-12-15\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Ninja Warrior.html#full-screencast",
    "href": "content_pages/Ninja Warrior.html#full-screencast",
    "title": "Ninja Warrior",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Ninja Warrior.html#timestamps",
    "href": "content_pages/Ninja Warrior.html#timestamps",
    "title": "Ninja Warrior",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:35\nInspecting the dataset\nNA\nNA\n\n\n\n\n0:6:40\nUsing geom_histogram to look at distribution of obstacles in a stage\ngeom_histogram\nNA\n\n\n\n\n0:9:05\nUsing str_remove function to clean stage names (remove “(Regional/City)”)\nstr_remove\nNA\n\n\n\n\n0:10:40\nAsking, “Are there obstacles that are more common in the Finals than Qualifying rounds?”\nNA\nNA\n\n\n\n\n0:10:50\nUsing bind_log_odds function from tidylo package to calculate log-odds of obstacles within a stage type\nbind_log_odds\ntidylo\n\n\n\n\n0:16:05\nUsing unite function to combine two columns\nunite\nNA\n\n\n\n\n0:18:20\nGraphing the average position of different obstacles with many, many tweaks to make it look nice\nNA\nNA\n\n\n\n\n0:23:10\nCreating a stacked bar plot of which obstacles appear in which order\nNA\nNA\n\n\n\n\n0:30:30\nTurning stacked bar plot visualization into a custom function\nNA\nNA\n\n\n\n\n0:37:40\nAsking, “Is there data on how difficult an obstacle is?”\nNA\nNA\n\n\n\n\n0:45:30\nVisualizing which obstacles appear in different seasons with geom_tile and a lot of tweaking\ngeom_tile\nNA\n\n\n\n\n0:50:22\nReviewing the result of the previous step (obstacles in different seasons)\nNA\nNA\n\n\n\n\n0:59:25\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Nobel Prize Winners.html",
    "href": "content_pages/Nobel Prize Winners.html",
    "title": "Nobel Prize Winners",
    "section": "",
    "text": "Notable topics: Data manipulation, Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2019-05-24\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Nobel Prize Winners.html#full-screencast",
    "href": "content_pages/Nobel Prize Winners.html#full-screencast",
    "title": "Nobel Prize Winners",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Nobel Prize Winners.html#timestamps",
    "href": "content_pages/Nobel Prize Winners.html#timestamps",
    "title": "Nobel Prize Winners",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:00\nCreating a stacked bar plot using geom_col and the aes function’s fill argument (also bins years into decades with truncated division operator %/%)\ngeom_col | %/%\nNA\n\n\n\n\n0:3:30\nUsing n_distinct function to quickly count unique years in a group\nn_distinct\nNA\n\n\n\n\n0:9:00\nUsing distinct function and its .keep_all argument to de-duplicate data\ndistinct\nNA\n\n\n\n\n0:10:50\nUsing coalesce function to replace NAs in a variable (similar to SQL COALESCE verb)\ncoalesce\nNA\n\n\n\n\n0:16:10\nUsing year function from lubridate package to calculate (approx.) age of laureates at time of award\nyear\nlubridate\n\n\n\n\n0:16:50\nUsing fct_reorder function to arrange boxplot graph by the median age of winners\nfct_reorder\nNA\n\n\n\n\n0:22:50\nDefining a new variable within the count function (like doing a mutate in the count function)\ncount\nNA\n\n\n\n\n0:23:40\nCreating a small multiples bar plot using geom_col and facet_wrap functions\ngeom_col | facet_wrap\nNA\n\n\n\n\n0:26:15\nImporting income data from WDI package to explore relationship between high/low income countries and winners\nWDIsearch\nWDI\n\n\n\n\n0:33:45\nUsing fct_relevel to change the levels of a categorical income variable (e.g., “Upper middle income”) so that the ordering makes sense\nfct_relevel\nNA\n\n\n\n\n0:36:25\nStarting to explore new dataset of nobel laureate publications\nNA\nNA\n\n\n\n\n0:44:25\nTaking the mean of a subset of data without needing to fully filter the data beforehand\nmean\nNA\n\n\n\n\n0:49:15\nUsing rank function and its ties.method argument to add the ordinal number of a laureate’s publication (e.g., 1st paper, 2nd paper)\nrank\nNA\n\n\n\n\n1:05:10\nLots of playing around with exploratory histograms (geom_histogram)\ngeom_histogram\nNA\n\n\n\n\n1:06:45\nDiscussion of right-censoring as an issue (people winning the Nobel prize but still having active careers)\nNA\nNA\n\n\n\n\n1:10:20\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/NYC Restaurant Inspections.html",
    "href": "content_pages/NYC Restaurant Inspections.html",
    "title": "NYC Restaurant Inspections",
    "section": "",
    "text": "Notable topics: Multiple t-test models (broom package), Principal Component Analysis (PCA)\nRecorded on: 2018-12-11\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/NYC Restaurant Inspections.html#full-screencast",
    "href": "content_pages/NYC Restaurant Inspections.html#full-screencast",
    "title": "NYC Restaurant Inspections",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/NYC Restaurant Inspections.html#timestamps",
    "href": "content_pages/NYC Restaurant Inspections.html#timestamps",
    "title": "NYC Restaurant Inspections",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:18:45\nSeparating column using separate function\nseparate\nNA\n\n\n\n\n0:21:15\nTaking distinct observation, but keeping the remaining variables using distinct function with .keep_all argument\ndistinct\nNA\n\n\n\n\n0:25:00\nUsing broom package and nest function to perform multiple t-tests at the same time\nnest | t.test\nbroom\n\n\n\n\n0:26:20\nTidying nested t-test models using broom package\nNA\nbroom\n\n\n\n\n0:27:00\nCreating TIE fighter plot of estimates of means and their confidence intervals\nNA\nNA\n\n\n\n\n0:28:45\nRecode long description using regex to remove everything after a parenthesis\nNA\nNA\n\n\n\n\n0:33:45\nUsing cut function to manually bin data along user-specified intervals\ncut\nNA\n\n\n\n\n0:42:00\nAsking, “What type of violations tend to occur more in some cuisines than others?”\nNA\nNA\n\n\n\n\n0:42:45\nUsing semi_join function to get the most recent inspection of all the restaurants\nsemi_join\nNA\n\n\n\n\n0:52:00\nAsking, “What violations tend to occur together?”\nNA\nNA\n\n\n\n\n0:53:00\nUsing widyr package function pairwise_cor (pairwise correlation) to find co-occurrence of violation types\npairwise_cor\nwidyr\n\n\n\n\n0:55:30\nBeginning of PCA (Principal Component Analysis) using widely_svd function\nwidely_svd\nNA\n\n\n\n\n0:58:00\nActually typing in the widely_svd function\nwidely_svd\nNA\n\n\n\n\n0:58:15\nReviewing and explaining output of widely_svd function\nwidely_svd\nNA\n\n\n\n\n1:01:30\nCreating graph of opposing elements of a PCA dimension\nNA\nNA\n\n\n\n\n1:02:00\nShortening string using str_sub function\nstr_sub\nNA\n\n\n\n\n1:04:00\nReference to Julia Silge’s PCA walkthrough using StackOverflow data: https://juliasilge.com/blog/stack-overflow-pca/\nNA\nNA"
  },
  {
    "objectID": "content_pages/NYC Squirrel Census.html",
    "href": "content_pages/NYC Squirrel Census.html",
    "title": "NYC Squirrel Census",
    "section": "",
    "text": "Notable topics: Map visualization (ggmap package)\nRecorded on: 2019-11-01\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/NYC Squirrel Census.html#full-screencast",
    "href": "content_pages/NYC Squirrel Census.html#full-screencast",
    "title": "NYC Squirrel Census",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/NYC Squirrel Census.html#timestamps",
    "href": "content_pages/NYC Squirrel Census.html#timestamps",
    "title": "NYC Squirrel Census",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:45\nStarter EDA of latitude and longitude using geom_point\ngeom_point\nNA\n\n\n\n\n0:6:45\nAggregating squirrel counts by hectare to get a “binned” map\nNA\nNA\n\n\n\n\n0:9:00\nInvestigating colour notes\nNA\nNA\n\n\n\n\n0:10:30\nAsking question, “Are there areas of the parks where we see certain-coloured squirrels\nNA\nNA\n\n\n\n\n0:12:45\nPlotting latitude and percentage of gray squirrels to answer, “Do we get a lower proportion of gray squirrels as we go farther north?”\nNA\nNA\n\n\n\n\n0:13:30\nUsing logistic regression to test gray squirrel (proportion as we go farther north)\nNA\nNA\n\n\n\n\n0:16:30\nNoting that he could have used original data sets as input for logistic regression function\nNA\nNA\n\n\n\n\n0:19:30\n“Does a squirrel run away?” based on location in the park (latitude), using logistic regression\nNA\nNA\n\n\n\n\n0:20:45\nUsing summarise_at function to apply same function to multiple variables\nsummarise_at\nNA\n\n\n\n\n0:25:25\nLoading ggmap package\nNA\nggmap\n\n\n\n\n0:27:00\nStart using ggmap, with the get_map function\nget_map\nggmap\n\n\n\n\n0:28:20\nDecision to not set up Google API key to use ggmap properly\nNA\nNA\n\n\n\n\n0:30:15\nUsing the sf package to read in a shapefile of Central Park\nNA\nsf\n\n\n\n\n0:30:40\nUsing read_sf function from sf package to import a shapefile into R\nread_sf\nsf\n\n\n\n\n0:31:30\nUsing geom_sf function from sf package to visualise the imported shapefile\ngeom_sf\nsf\n\n\n\n\n0:32:45\nCombining shapefile “background” with relevant squirrel data in one plot\nNA\nNA\n\n\n\n\n0:34:40\nVisualising pathways (footpaths, bicycle paths) in the shapefile\nNA\nNA\n\n\n\n\n0:37:55\nFinishing visualisation and moving on to analysing activity types\nNA\nNA\n\n\n\n\n0:38:45\nSelecting fields based on whether they end with “ing”, then gathering those fields into tidy format\nends_with | gather\nNA\n\n\n\n\n0:39:50\nDecision to create a Shiny visualisation\nNA\nshiny\n\n\n\n\n0:41:30\nSetting Shiny app settings (e.g., slider for minimum number of squirrels)\nNA\nshiny\n\n\n\n\n0:42:15\nSetting up Shiny app options / variables\nNA\nshiny\n\n\n\n\n0:43:50\nExplanation of why setting up options in Shiny app the way he did\nNA\nshiny\n\n\n\n\n0:46:00\nSolving error “Discrete value supplied to continuous scale”\nNA\nshiny\n\n\n\n\n0:46:50\nFirst draft of Shiny app\nNA\nshiny\n\n\n\n\n0:48:35\nCreating a dynamic midpoint for the two-gradient scale in the app\nNA\nshiny\n\n\n\n\n0:51:30\nAdding additional variables of more behaviours to Shiny app (kuks, moans, runs from, etc.)\nNA\nshiny\n\n\n\n\n0:53:10\n“What are the distributions of some of these [behaviours]?”\nNA\nNA\n\n\n\n\n0:56:50\nAdding ground location (above ground, ground plane) to Shiny app\nNA\nshiny\n\n\n\n\n0:58:20\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Palmer Penguins.html",
    "href": "content_pages/Palmer Penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Notable topics: Modeling (logistic regression, k-nearest neighbors, decision tree, multiclass logistic regression) with cross validated accuracy\nRecorded on: 2020-07-28\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Palmer Penguins.html#full-screencast",
    "href": "content_pages/Palmer Penguins.html#full-screencast",
    "title": "Palmer Penguins",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Palmer Penguins.html#timestamps",
    "href": "content_pages/Palmer Penguins.html#timestamps",
    "title": "Palmer Penguins",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:11:17\nCreate a pivoted histogram plot to visualize the distribution of penguin metrics using pivot_longer, geom_histogram, and facet_wrap\npivot_longer | geom_histogram | facet_wrap\ntidyr | ggplot2\n\n\n\n\n0:14:40\nCreate a pivoted density plot to visualize the distribution of penguin metrics using geom_density and facet_wrap\ngeom_density | facet_wrap\nggplot2\n\n\n\n\n0:15:21\nCreate a pivoted boxplot plot to visualize the distribution of penguin metrics using geom_boxplot and facet_wrap\ngeom_boxplot | facet_wrap\nggplot2\n\n\n\n\n0:17:50\nCreate a bar plot to show penguin species changed over time\ngeom_bar\nggplot2\n\n\n\n\n0:18:25\nCreate a bar plot to show specie counts per island\ngeom_bar\nggplot2\n\n\n\n\n0:20:00\nCreate a logistic regression model to predict if a penguin is Adelie or not using bill length with cross validaiton of metrics\ninitital_split | training | logistic_reg | set_engine | fit | fct_lump | predict | metrics | vfold_cv | fit_resamples | collect_metrics |\ntidymodels | rsample | parsnip yardstick |\n\n\n\n\n0:39:35\nCreate second logistic regression model using 4 predictive metrics (bill length, bill depth, flipper length, body mass) and then compare the accuracy of both models\ninitital_split | training | logistic_reg | set_engine | fit | fct_lump | predict | metrics | vfold_cv | fit_resamples | collect_metrics |\ntidymodels | rsample | parsnip yardstick |\n\n\n\n\n0:43:25\nCreate a k-nearest neighbor model and then compare accuracy against logistic regression models to see which has the highest cross validated accuracy\nnearest_neighbor | initital_split | training | logistic_reg | set_engine | fit | fct_lump | predict | metrics | vfold_cv | fit_resamples | collect_metrics\ntidymodels | rsample | parsnip yardstick |\n\n\n\n\n0:53:05\nWhat is the accuracy of the testing holdout data on the k-nearest neighbor model?\ntesting | predict | metrics\nrsample | stats | yardstick\n\n\n\n\n1:05:40\nCreate a decision tree and then compare accuracy against the previous models to see which has the highest cross validated accuracy + how to extract a decision tree\ndecision_tree | set_engine\nparsnip\n\n\n\n\n1:10:45\nPerform multi class regression using multinom_reg\nmultinom_reg | set_engine | fit_resamples\nparsnip | tune\n\n\n\n\n1:19:40\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Pizza Ratings.html",
    "href": "content_pages/Pizza Ratings.html",
    "title": "Pizza Ratings",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-10-01\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Pizza Ratings.html#full-screencast",
    "href": "content_pages/Pizza Ratings.html#full-screencast",
    "title": "Pizza Ratings",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Pizza Ratings.html#timestamps",
    "href": "content_pages/Pizza Ratings.html#timestamps",
    "title": "Pizza Ratings",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:45\nTransforming time into something more readable (from time value of seconds since Unix epoch [1970-01-01] ), then converting it into a date\nNA\nNA\n\n\n\n\n0:9:05\nFormatting x-axis text so that it is rotated and readable, then re-ordering using fct_relevel function so that it is in its proper ordinal order\nfct_relevel\nNA\n\n\n\n\n0:11:00\nConverting string answers to integer counterparts to get an overall numeric value for how good each place is\nNA\nNA\n\n\n\n\n0:12:30\nCommentary on speed of mutate calculation within or without a group (non-grouped is slightly faster)\nNA\nNA\n\n\n\n\n0:15:30\nRe-ordering groups by total votes using fct_reorder function, while still maintaining the groups themselves\nfct_reorder\nNA\n\n\n\n\n0:19:15\nUsing glue package to combine place name and total respondents\nglue\nglue\n\n\n\n\n0:20:30\nUsing statistical test to give confidence intervals on average score\nNA\nNA\n\n\n\n\n0:22:15\nActually using the t.test function with toy example\nt.test\nNA\n\n\n\n\n0:23:15\nUsing weighted linear model instead (which doesn’t end up working)\nNA\nNA\n\n\n\n\n0:26:00\nUsing custom function with rep function to get vector of repeated scores (sneaky way of weighting) so that we can perform a proper t-test\nrep\nNA\n\n\n\n\n0:27:30\nSummarizing t-test function into a list (alternative to nesting)\nNA\nNA\n\n\n\n\n0:31:20\nAdding error bars using geom_errorbarh to make a TIE fighter plot that shows confidence intervals\ngeom_errorbarh\nNA\n\n\n\n\n0:36:30\nBringing in additional data from Barstool ratings (to supplement survey of Open R meetup NY)\nNA\nNA\n\n\n\n\n0:39:45\nGetting survey data to the place level so that we can add an additional dataset\nNA\nNA\n\n\n\n\n0:41:15\nChecking for duplicates in the joined data\nNA\nNA\n\n\n\n\n0:42:15\nCalling off the planned analysis due to low sample sizes (too much noise, not enough overlap between datasets)\nNA\nNA\n\n\n\n\n0:45:15\nLooking at Barstool data on its own\nNA\nNA\n\n\n\n\n0:55:15\nRenaming all variables with a certain string pattern in them\nNA\nNA\n\n\n\n\n0:58:00\nComparing Dave’s reviews with all other critics\nNA\nNA\n\n\n\n\n0:59:15\nAdding geom_abline showing x = y as comparison for geom_smooth linear model line\ngeom_abline\nNA\n\n\n\n\n1:02:30\nChanging the location of the aes() to change what the legend icons look like for size aesthetic\nNA\nNA"
  },
  {
    "objectID": "content_pages/Plants in Danger.html",
    "href": "content_pages/Plants in Danger.html",
    "title": "Plants in Danger",
    "section": "",
    "text": "Notable topics: Data manipulation, Web scraping (rvest package) and SelectorGadget\nRecorded on: 2020-08-18\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Plants in Danger.html#full-screencast",
    "href": "content_pages/Plants in Danger.html#full-screencast",
    "title": "Plants in Danger",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Plants in Danger.html#timestamps",
    "href": "content_pages/Plants in Danger.html#timestamps",
    "title": "Plants in Danger",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:00\nUsing count, fct_lump, and fct_reorder to get an overview of categorical data\ncount | fct_lump | fct_reorder\ndplyr | forcats\n\n\n\n\n0:5:00\nUsing fct_relevel to reorder the “Before 1900” level to the first location leaving the other levels in their existing order\nfct_relevel\nforcats\n\n\n\n\n0:8:05\nUsing n and sum in fct_reorder to reorder factor levels when there are multiple categories in count\nfct_reorder\nforcats\n\n\n\n\n0:12:00\nUsing reorder_within and scale_y_reordered such that the values are ordered within each facet\nreorder_within | scale_y_reordered\ntidytext\n\n\n\n\n0:14:55\nUsing `axis.text.x” to rotate overlapping labels\naxis.text.x\nggplot2\n\n\n\n\n0:19:05\nUsing filter and fct_lump to lump all levels except for the 8 most frequest facet panels\nfilter | fct_lump\ndplyr | forcats\n\n\n\n\n0:26:55\nUsing separate to separate the character column binomial_name into multiple columns (genus and species)\nseparate\ntidyr\n\n\n\n\n0:28:20\nUsing fct_lump within count to lump all levels except for the 8 most frequent genus\nfct_lump\nforcats\n\n\n\n\n0:45:30\nUsing rvest and SelectorGadget to web scrape list of species\nread_html | html_nodes | html_text\nrvest\n\n\n\n\n0:49:35\nUsing str_trim to remove whitespace from character string\nstr_trim\nstringr\n\n\n\n\n0:50:00\nUsing separate to separate character string into genus, species, and rest/citation columns and using extra = \"merge\" to merge extra pieces into the rest/citation column\nseparate\ntidyr\n\n\n\n\n0:51:00\nUsing rvest and SelectorGadget to web scrape image links\nread_html | html_nodes | html_text html_attr | inner_join | paste0 | map\nrvest | dplyr purrr\n\n\n\n\n0:57:50\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Plastic Waste.html",
    "href": "content_pages/Plastic Waste.html",
    "title": "Plastic Waste",
    "section": "",
    "text": "Notable topics: Map visualization (especially choropleth)\nRecorded on: 2019-05-27\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Plastic Waste.html#full-screencast",
    "href": "content_pages/Plastic Waste.html#full-screencast",
    "title": "Plastic Waste",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Plastic Waste.html#timestamps",
    "href": "content_pages/Plastic Waste.html#timestamps",
    "title": "Plastic Waste",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:45\nUsing summarise_all to get proportion of NA values across many variables\nsummarise_all\nNA\n\n\n\n\n0:16:50\nAdding text labels to scatter plot for some points using check_overlap argument\ngeom_text\nNA\n\n\n\n\n0:21:45\nUsing pmin function to get the lower of two possible numbers for a percentage variable that was showing > 100%\npmin\nNA\n\n\n\n\n0:29:00\nStarting to make a choropleth map\nNA\nNA\n\n\n\n\n0:29:30\nConnecting ISO country names (used in mapping code) to country names given in the dataset\nNA\nNA\n\n\n\n\n0:32:00\nActual code to create the map using given longitude and latitude\nNA\nNA\n\n\n\n\n0:33:45\nUsing fuzzyjoin package to link variables that use regular expression instead of character (using regex_right_join / regex_left_join function)\nregex_left_join\nfuzzyjoin\n\n\n\n\n0:36:15\nUsing coord_fixed function as a hack to get proper ratios for maps\ncoord_fixed\nNA\n\n\n\n\n0:39:30\nBringing in additional data using WDI package\nNA\nNA\n\n\n\n\n0:47:30\nUsing patchwork package to show multiple graphs in the same plot\nNA\npatchwork\n\n\n\n\n0:53:00\nImporting and rename multiple indicators from the WDI package at the same time\nNA\nNA"
  },
  {
    "objectID": "content_pages/R Downloads.html",
    "href": "content_pages/R Downloads.html",
    "title": "R Downloads",
    "section": "",
    "text": "Notable topics: Data manipulation (especially time series)\nRecorded on: 2018-10-30\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/R Downloads.html#full-screencast",
    "href": "content_pages/R Downloads.html#full-screencast",
    "title": "R Downloads",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/R Downloads.html#timestamps",
    "href": "content_pages/R Downloads.html#timestamps",
    "title": "R Downloads",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:20\nUsing geom_line function to visualize changes over time\ngeom_line\nNA\n\n\n\n\n0:7:35\nStarting to decompose time series data into day-of-week trend and overall trend (lots of lubridate package functions)\nNA\nlubridate\n\n\n\n\n0:9:50\nUsing floor_date function from lubridate package to round dates down to the week level\nNA\nNA\n\n\n\n\n0:10:05\nUsing min function to drop incomplete/partial week at the start of the dataset\nNA\nNA\n\n\n\n\n0:12:20\nUsing countrycode function from countrycode package to replace two-letter country codes with full names (e.g., “CA” becomes “Canada”)\ncountrycode\ncountrycode\n\n\n\n\n0:17:20\nUsing fct_lump function to get top N categories within a categorical variable and classify the rest as “Other”\nfct_lump\nNA\n\n\n\n\n0:20:30\nUsing hour function from lubridate package to pull out integer hour value from a datetime variable\nhour\nlubridate\n\n\n\n\n0:22:20\nUsing facet_wrap function to graph small multiples of downloads by country, then changing scales argument to allow different scales on y-axis\nfacet_wrap\nNA\n\n\n\n\n0:31:00\nStarting analysis of downloads by IP address\nNA\nNA\n\n\n\n\n0:35:20\nUsing as.POSIXlt to combine separate date and time variables to get a single datetime variable\nas.POSIXlt\nNA\n\n\n\n\n0:36:35\nUsing lag function to calculate time between downloads (time between events) per IP address (comparable to SQL window function)\nlag\nNA\n\n\n\n\n0:38:05\nUsing as.numeric function to convert variable from a time interval object to a numeric variable (number in seconds)\nas.numeric\nNA\n\n\n\n\n0:38:40\nExplanation of a bimodal log-normal distribution\nNA\nNA\n\n\n\n\n0:39:05\nHandy trick for setting easy-to-interpret intervals for time data on scale_x_log10 function’s breaks argument\nscale_x_log10\nNA\n\n\n\n\n0:47:40\nStarting to explore package downloads\nNA\nNA\n\n\n\n\n0:52:15\nAdding 1 to the numerator and denominator when calculating a ratio to get around dividing by zero\nNA\nNA\n\n\n\n\n0:57:55\nShowing how to look at package download data over time using cran_downloads function from the cranlogs package\ncran_downloads\ncranlogs"
  },
  {
    "objectID": "content_pages/Ramen Reviews.html",
    "href": "content_pages/Ramen Reviews.html",
    "title": "Ramen Reviews",
    "section": "",
    "text": "Notable topics: Web scraping (rvest package)\nRecorded on: 2019-06-04\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Ramen Reviews.html#full-screencast",
    "href": "content_pages/Ramen Reviews.html#full-screencast",
    "title": "Ramen Reviews",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Ramen Reviews.html#timestamps",
    "href": "content_pages/Ramen Reviews.html#timestamps",
    "title": "Ramen Reviews",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:45\nLooking at the website the data came from\nNA\nNA\n\n\n\n\n0:2:55\nUsing gather function (now pivot_longer) to convert wide data to long (tidy) format\ngather\nNA\n\n\n\n\n0:4:15\nGraphing counts of all categorical variables at once, then exploring them\nNA\nNA\n\n\n\n\n0:5:35\nUsing fct_lump function to lump three categorical variables to the top N categories and “Other”\nfct_lump\nNA\n\n\n\n\n0:7:45\nUsing reorder_within function to re-order factors that have the same name across multiple facets\nreorder_within\nNA\n\n\n\n\n0:9:10\nUsing lm function (linear model) to predict star rating\nlm\nNA\n\n\n\n\n0:9:50\nVisualising effects (and 95% CI) of indendent variables in linear model with a coefficient plot (TIE fighter plot)\nNA\nNA\n\n\n\n\n0:11:30\nUsing fct_relevel function to get “Other” as the base reference level for categorical independent variables in a linear model\nfct_relevel\nNA\n\n\n\n\n0:13:05\nUsing extract function and regex to split a camelCase variable into two separate variables\nextract\nNA\n\n\n\n\n0:14:45\nUsing facet_wrap function to split coefficient / TIE fighter plot into three separate plots, based on type of coefficient\nfacet_wrap\nNA\n\n\n\n\n0:15:40\nUsing geom_vline function to add reference line to graph\ngeom_vline\nNA\n\n\n\n\n0:17:20\nUsing unnest_tokens function from tidytext package to explore the relationship between variety (a sparse categorical variable) and star rating\nunnest_tokens\ntidytext\n\n\n\n\n0:18:55\nExplanation of how he would approach variety variable with Lasso regression\nNA\nNA\n\n\n\n\n0:19:35\nWeb scraping the using rvest package and SelectorGadget (Chrome Extension CSS selector)\nNA\nrvest\n\n\n\n\n0:21:20\nActually writing code for web scraping, using read_html, html_node, and html_table functions\nread_html | html_node | html_table\nrvest\n\n\n\n\n0:22:25\nUsing clean_names function from janitor package to clean up names of variables\nclean_names\njanitor\n\n\n\n\n0:23:05\nExplanation of web scraping task: get full review text using the links from the review summary table scraped above\nNA\nNA\n\n\n\n\n0:25:40\nUsing parse_number function as alternative to as.integer function to cleverly drop extra weird text in review number\nparse_number\nNA\n\n\n\n\n0:26:45\nUsing SelectorGadget (Chrome Extension CSS selector) to identify part of page that contains review text\nNA\nNA\n\n\n\n\n0:27:35\nUsing html_nodes, html_text, and str_subset functions to write custom function to scrape review text identified in step above\nhtml_nodes | html_text | str_subset\nrvest\n\n\n\n\n0:29:15\nAdding message function to custom scraping function to display URLs as they are being scraped\nmessage\nNA\n\n\n\n\n0:30:15\nUsing unnest_tokens and anti_join functions to split review text into individual words and remove stop words (e.g., “the”, “or”, “and”)\nunnest_tokens | anti_join\nNA\n\n\n\n\n0:31:05\nCatching a mistake in the custom function causing it to read the same URL every time\nNA\nNA\n\n\n\n\n0:31:55\nUsing str_detect function to filter out review paragraphs without a keyword in it\nstr_detect\nNA\n\n\n\n\n0:32:40\nUsing str_remove function and regex to get rid of string that follows a specific pattern\nstr_remove\nNA\n\n\n\n\n0:34:10\nExplanation of possibly and safely functions in purrr package\npossibly | safely\npurrr\n\n\n\n\n0:37:45\nReviewing output of the URL that failed to scrape, including using character(0) as a default null value\nNA\nNA\n\n\n\n\n0:48:00\nUsing pairwise_cor function from widyr package to see which words tend to appear in reviews together\npairwise_cor\nwidyr\n\n\n\n\n0:51:05\nUsing igraph and ggraph packages to make network plot of word correlations\nNA\nigraph | ggraph\n\n\n\n\n0:51:55\nUsing geom_node_text function to add labels to network plot\ngeom_node_text\nigraph | ggraph\n\n\n\n\n0:52:35\nIncluding all words (not just those connected to others) as vertices in the network plot\nNA\nigraph | ggraph\n\n\n\n\n0:54:40\nTweaking and refining network plot aesthetics (vertex size and colour)\nNA\nNA\n\n\n\n\n0:56:00\nWeird hack for getting a dark outline on hard-to-see vertex points\nNA\nNA\n\n\n\n\n0:59:15\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Seattle Bike Counts.html",
    "href": "content_pages/Seattle Bike Counts.html",
    "title": "Seattle Bike Counts",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-04-05\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Seattle Bike Counts.html#full-screencast",
    "href": "content_pages/Seattle Bike Counts.html#full-screencast",
    "title": "Seattle Bike Counts",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Seattle Bike Counts.html#timestamps",
    "href": "content_pages/Seattle Bike Counts.html#timestamps",
    "title": "Seattle Bike Counts",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:6:15\nUsing summarise_all / summarise_at function to aggregate multiple variables at the same time\nsummarise_all | summarise_at\nNA\n\n\n\n\n0:8:15\nUsing magnitude instead of absolute numbers to see trends in time of day\nNA\nNA\n\n\n\n\n0:12:00\nDividing time into categories (four categories for times of day, e.g., morning commute, night) using between function\nbetween\nNA\n\n\n\n\n0:15:00\nLooking for systematically missing data (which would bias the results of the analysis)\nNA\nNA\n\n\n\n\n0:19:45\nSummarising using a filter in the arguments based on whether the time window is during a commute time\nNA\nNA\n\n\n\n\n0:22:45\nCombining day of week and hour using functions in the lubridate package and as.difftime function (but then he uses facetting as an easier method)\nas.difftime\nlubridate\n\n\n\n\n0:26:30\nNormalizing day of week data to percent of weekly traffic\nNA\nNA\n\n\n\n\n0:42:00\nStarting analysis of directions of travel by time of day (commute vs. reverse-commute)\nNA\nNA\n\n\n\n\n0:43:45\nFiltering out weekend days using wday function from lubridate package\nwday\nlubridate\n\n\n\n\n0:45:30\nUsing spread function to create new variable of ratio of bike counts at different commute times\nspread\nNA\n\n\n\n\n0:47:30\nVisualizing ratio of bike counts by time of day\nNA\nNA\n\n\n\n\n0:50:15\nVisualizing ratio by hour instead of time of day\nNA\nNA\n\n\n\n\n0:52:50\nOrdering crossing in graph by when the average trip happened using mean of hour weighted by bike count\nNA\nNA\n\n\n\n\n0:54:50\nQuick and dirty filter when creating a new variable within a mutate function\nmutate\nNA"
  },
  {
    "objectID": "content_pages/Seattle Pet Names.html",
    "href": "content_pages/Seattle Pet Names.html",
    "title": "Seattle Pet Names",
    "section": "",
    "text": "Notable topics: Hypergeometric hypothesis testing, Adjusting for multiple hypothesis testing\nRecorded on: 2019-03-16\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Seattle Pet Names.html#full-screencast",
    "href": "content_pages/Seattle Pet Names.html#full-screencast",
    "title": "Seattle Pet Names",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Seattle Pet Names.html#timestamps",
    "href": "content_pages/Seattle Pet Names.html#timestamps",
    "title": "Seattle Pet Names",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:40\nUsing mdy function from lubridate package to convert character-formatted date to date-class\nmdy\nlubridate\n\n\n\n\n0:4:20\nExploratory bar graph showing top species of cats, using geom_col function\ngeom_col\nNA\n\n\n\n\n0:6:30\nSpecifying facet_wrap function’s ncol argument to get graphs stacked vertically (instead of side-by-side)\nfacet_wrap\nNA\n\n\n\n\n0:9:55\nAsking, “Are some animal names associated with particular dog breeds?”\nNA\nNA\n\n\n\n\n0:11:15\nExplanation of add_count function\nadd_count\nNA\n\n\n\n\n0:12:35\nAdding up various metrics (e.g., number of names overall, number of breeds overall), but note a mistake that gets fixed at 17:05\nNA\nNA\n\n\n\n\n0:16:10\nCalculating a ratio for names that appear over-represented within a breed, then explaining how small samples can be misleading\nNA\nNA\n\n\n\n\n0:17:05\nSpotting and fixing an aggregation mistake\nNA\nNA\n\n\n\n\n0:17:55\nExplanation of how to investigate which names might be over-represented within a breed\nNA\nNA\n\n\n\n\n0:18:55\nExplanation of how to use hypergeometric distribution to test for name over-representation\nNA\nNA\n\n\n\n\n0:20:40\nUsing phyper function to calculate p-values for a one-sided hypergeometric test\nphyper\nNA\n\n\n\n\n0:23:30\nAdditional explanation of hypergeometric distribution\nNA\nNA\n\n\n\n\n0:24:00\nFirst investigation of why and how to interpret a p-value histogram (second at 29:45, third at 37:45, and answer at 39:30)\nNA\nNA\n\n\n\n\n0:25:15\nNoticing that we are missing zeros (i.e., having a breed/name combination with 0 dogs), which is important for the hypergeometric test\nNA\nNA\n\n\n\n\n0:27:10\nUsing complete function to turn implicit zeros (for breed/name combination) into explicit zeros\ncomplete\nNA\n\n\n\n\n0:29:45\nSecond investigation of p-value histogram (after adding in implicit zeros)\nNA\nNA\n\n\n\n\n0:31:55\nExplanation of multiple hypothesis testing and correction methods (e.g., Bonferroni, Holm), and applying using p.adjust function\np.adjust\nNA\n\n\n\n\n0:34:25\nExplanation of False Discovery Rate (FDR) control as a method for correcting for multiple hypothesis testing, and applying using p.adjust function\np.adjust\nNA\n\n\n\n\n0:37:45\nThird investigation of p-value histogram, to hunt for under-represented names\nNA\nNA\n\n\n\n\n0:39:30\nAnswer to why the p-value distribution is not well-behaved\nNA\nNA\n\n\n\n\n0:42:40\nUsing crossing function to created a simulated dataset to explore how different values affect the p-value\ncrossing\nNA\n\n\n\n\n0:44:55\nExplanation of how total number of names and total number of breeds affects p-value\nNA\nNA\n\n\n\n\n0:46:00\nMore general explanation of what different shapes of p-value histogram might indicate\nNA\nNA\n\n\n\n\n0:47:30\nRenaming variables within a transmute function, using backticks to get names with spaces in them\ntransmute\nNA\n\n\n\n\n0:49:20\nUsing kable function from the knitr package to create a nice-looking table\nkable\nknitr\n\n\n\n\n0:50:00\nExplanation of one-side p-value (as opposed to two-sided p-value)\nNA\nNA\n\n\n\n\n0:53:55\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Simpsons Guest Stars.html",
    "href": "content_pages/Simpsons Guest Stars.html",
    "title": "Simpsons Guest Stars",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package)\nRecorded on: 2019-08-30\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Simpsons Guest Stars.html#full-screencast",
    "href": "content_pages/Simpsons Guest Stars.html#full-screencast",
    "title": "Simpsons Guest Stars",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Simpsons Guest Stars.html#timestamps",
    "href": "content_pages/Simpsons Guest Stars.html#timestamps",
    "title": "Simpsons Guest Stars",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:15\nUsing str_detect function to find guests that played themselves\nstr_detect\nNA\n\n\n\n\n0:7:55\nUsing separate_rows function and regex to get delimited values onto different rows (e.g., “Edna Krabappel; Ms. Melon” gets split into two rows)\nseparate_rows\nNA\n\n\n\n\n0:9:55\nUsing parse_number function to convert a numeric variable coded as character to a proper numeric variable\nparse_number\nNA\n\n\n\n\n0:14:45\nDownloading and importing supplementary dataset of dialogue\nNA\nNA\n\n\n\n\n0:16:10\nUsing semi_join function to filter dataframe based on values that appear in another dataframe\nsemi_join\nNA\n\n\n\n\n0:18:05\nUsing anti_join function to check which values in a dataframe do not appear in another dataframe\nanti_join\nNA\n\n\n\n\n0:20:50\nUsing ifelse function to recode a single value with another (i.e., “Edna Krapabbel” becomes “Edna Krabappel-Flanders”)\nifelse\nNA\n\n\n\n\n0:26:20\nExplaining the goal of all the data cleaning steps\nNA\nNA\n\n\n\n\n0:31:25\nUsing sample function to get an example line for each character\nsample\nNA\n\n\n\n\n0:33:20\nSetting geom_histogram function’s binwidth and center arguments to get specific bin sizes\ngeom_histogram\nNA\n\n\n\n\n0:37:25\nUsing unnest_tokens and anti_join functions from tidytext package to split dialogue into individual words and remove stop words (e.g., “the”, “or”, “and”)\nunnest_tokens | anti_join\ntidytext\n\n\n\n\n0:38:55\nUsing bind_tf_idf function from tidytext package to get the TF-IDF (term frequency-inverse document frequency) of individual words\nbind_tf_idf\ntidytext\n\n\n\n\n0:42:50\nUsing top_n function to get the top 1 TF-IDF value for each role\ntop_n\nNA\n\n\n\n\n0:44:05\nUsing paste0 function to combine two character variables (e.g., “Groundskeeper Willie” and “ach” (separate variables) become “Groundskeeper Willie: ach”)\npaste0\nNA\n\n\n\n\n0:48:10\nExplanation of what TF-IDF (text frequency-inverse document frequency) tells us and how it is a “catchphrase detector”\nNA\nNA\n\n\n\n\n0:56:40\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Space Launches.html",
    "href": "content_pages/Space Launches.html",
    "title": "Space Launches",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2019-01-15\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Space Launches.html#full-screencast",
    "href": "content_pages/Space Launches.html#full-screencast",
    "title": "Space Launches",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Space Launches.html#timestamps",
    "href": "content_pages/Space Launches.html#timestamps",
    "title": "Space Launches",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:40\nUsing str_detect function to find missions with “Apollo” in their name\nstr_detect\nNA\n\n\n\n\n0:6:20\nStarting EDA (exploratory data analysis)\nNA\nNA\n\n\n\n\n0:15:10\nUsing fct_collapse function to recode factors (similar to case_when function)\nfct_collapse\nNA\n\n\n\n\n0:16:45\nUsing countrycode function from countrycode package to get full country names from country codes (e.g. “RU” becomes “Russia”)\ncountrycode\ncountrycode\n\n\n\n\n0:18:15\nUsing replace_na function to convert NA (missing) observations to “Other”\nNA\nNA\n\n\n\n\n0:19:10\nCreating a line graph using geom_line function with different colours for different categories\ngeom_line\nNA\n\n\n\n\n0:21:05\nUsing fct_reorder function to reorder factors in line graph above, in order to make legend more readable\nfct_reorder\nNA\n\n\n\n\n0:32:00\nCreating a bar graph, using geom_col function, of most active (by number of launches) private or startup agencies\ngeom_col\nNA\n\n\n\n\n0:35:05\nUsing truncated division operator %/% to bin data into decades\n%/%\nNA\n\n\n\n\n0:35:35\nUsing complete function to turn implicit zeros into explicit zeros (makes for a cleaner line graph)\ncomplete\nNA\n\n\n\n\n0:37:15\nUsing facet_wrap function to create small multiples of a line graph, then proceeding to tweak the graph\nfacet_wrap\nNA\n\n\n\n\n0:42:50\nUsing semi_join function as a filtering step\nsemi_join\nNA\n\n\n\n\n0:43:15\nUsing geom_point to create a timeline of launches by vehicle type\ngeom_point\nNA\n\n\n\n\n0:47:20\nExplanation of why boxplots over time might not be a good visualization choice\nNA\nNA\n\n\n\n\n0:48:00\nUsing geom_jitter function to tweak the timeline graph to be more readable\ngeom_jitter\nNA\n\n\n\n\n0:51:30\nCreating a second timeline graph for US vehicles and launches\nNA\nNA\n\n\n\n\n0:56:35\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Student-Teacher Ratios.html",
    "href": "content_pages/Student-Teacher Ratios.html",
    "title": "Student-Teacher Ratios",
    "section": "",
    "text": "Notable topics: WDI package (World Development Indicators)\nRecorded on: 2019-05-10\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Student-Teacher Ratios.html#full-screencast",
    "href": "content_pages/Student-Teacher Ratios.html#full-screencast",
    "title": "Student-Teacher Ratios",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Student-Teacher Ratios.html#timestamps",
    "href": "content_pages/Student-Teacher Ratios.html#timestamps",
    "title": "Student-Teacher Ratios",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:7:30\nUsing slice function to select 10 highest and 10 lowest student-teacher ratios (like a filter using row numbers)\nslice\nNA\n\n\n\n\n0:12:35\nAdding GDP per capita to a dataset using WDI package\nNA\nWDI\n\n\n\n\n0:17:40\nUsing geom_text to add labels to points on a scatterplot\ngeom_text\nNA\n\n\n\n\n0:19:00\nUsing WDIsearch function from WDI package to search for country population data\nWDIsearch\nWDI\n\n\n\n\n0:23:20\nExplanation of trick with geom_text function’s check_overlap argument to get label for US to appear by rearranging row order\nNA\nNA\n\n\n\n\n0:25:45\nUsing comma_format function from scales format to get more readable numeric legend (e.g., “500,000,000” instead of “5e+08”)\ncomma_format\nscales\n\n\n\n\n0:27:55\nExploring different education-related indicators in the WDI package\nNA\nWDI\n\n\n\n\n0:31:55\nUsing spread function (now pivot_wider) to turn data from tidy to wide format\nspread | pivot_wider\nNA\n\n\n\n\n0:32:15\nUsing to_snake_case function from snakecase package to conver field names to snake_case\nto_snake_case\nsnakecase\n\n\n\n\n0:48:30\nExploring female/male school secondary school enrollment\nNA\nNA\n\n\n\n\n0:51:50\nNote of caution on keeping confounders in mind when interpreting scatterplots\nNA\nNA\n\n\n\n\n0:52:30\nCreating a linear regression of secondary school enrollment to explore confounders\nlm\nNA\n\n\n\n\n0:54:30\nDiscussing the actual confounder (GDP per capita) in the linear regression above\nNA\nNA\n\n\n\n\n0:57:20\nAdding world region as another potential confounder\nNA\nNA\n\n\n\n\n0:58:00\nUsing aov function (ANOVA) to explore confounders further\naov\nNA\n\n\n\n\n1:06:50\nReviewing and interpreting the final linear regression model\nNA\nNA\n\n\n\n\n1:08:00\nUsing cor function (correlation) to get correlation matrix for three variables (and brief explanation of multi-collinearity)\ncor\nNA\n\n\n\n\n1:10:10\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Tennis Tournaments.html",
    "href": "content_pages/Tennis Tournaments.html",
    "title": "Tennis Tournaments",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-04-09\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Tennis Tournaments.html#full-screencast",
    "href": "content_pages/Tennis Tournaments.html#full-screencast",
    "title": "Tennis Tournaments",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Tennis Tournaments.html#timestamps",
    "href": "content_pages/Tennis Tournaments.html#timestamps",
    "title": "Tennis Tournaments",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:00\nIdentifying duplicated rows ands fixing them\nNA\nNA\n\n\n\n\n0:11:15\nUsing add_count and fct_reorder functions to order categories that are broken down into sub-categories for graphing\nadd_count | fct_reorder\nNA\n\n\n\n\n0:13:00\nTidying graph titles (e.g., replacing underscores with spaces) using str_to_title and str_replace functions\nstr_to_title | str_replace\nNA\n\n\n\n\n0:15:00\nUsing inner_join function to merge datasets\ninner_join\nNA\n\n\n\n\n0:15:30\nCalculating age from date of birth using difftime and as.numeric functions\ndifftime | as.numeric\nNA\n\n\n\n\n0:16:35\nAdding simple calculations like mean and median into the text portion of markdown document\nNA\nNA\n\n\n\n\n0:17:45\nLooking at distribution of wins by sex using overlapping histograms\nNA\nNA\n\n\n\n\n0:18:55\nBinning years into decades using truncated division %/%\n%/%\nNA\n\n\n\n\n0:20:15\nSplitting up boxplots so that they are separated into pairs (M/F) across a different group (decade) using interaction function\ninteraction\nNA\n\n\n\n\n0:20:30\nAnalyzing distribution of ages across decades, looking specifically at the effect of Serena Williams (one individual having a disproportionate affect on the data, making it look like there’s a trend)\nNA\nNA\n\n\n\n\n0:24:30\nAvoiding double-counting of individuals by counting their average age instead of their age at each win\nNA\nNA\n\n\n\n\n0:30:20\nStarting analysis to predict winner of Grand Slam tournaments\nNA\nNA\n\n\n\n\n0:35:00\nCreating rolling count using row_number function to make a count of previous tournament experience\nrow_number\nNA\n\n\n\n\n0:39:45\nCreating rolling win count using cumsum function\ncumsum\nNA\n\n\n\n\n0:41:00\nLagging rolling win count using lag function (otherwise we get information about a win before a player has actually won, for prediction purposes)\nlag\nNA\n\n\n\n\n0:43:30\nAsking, “When someone is a finalist, what is their probability of winning as a function of previous tournaments won?”\nNA\nNA\n\n\n\n\n0:48:00\nAsking, “How does the number of wins a finalist has affect their chance of winning?”\nNA\nNA\n\n\n\n\n0:49:00\nBacktesting simple classifier where person with more tournament wins is predicted to win the given tournament\nNA\nNA\n\n\n\n\n0:51:45\nCreating classifier that gives points based on how far a player got in previous tournaments\nNA\nNA\n\n\n\n\n0:52:55\nUsing match function to turn name of round reached (1st round, 2nd round, …) into a number score (1, 2, …)\nmatch\nNA\n\n\n\n\n0:54:20\nUsing cummean function to get score of average past performance (instead of cumsum function)\ncummean\nNA\n\n\n\n\n1:04:10\nPulling names of rounds (1st round, 2nd round, … ) based on the rounded numeric score of previous performance\nNA\nNA"
  },
  {
    "objectID": "content_pages/Thanksgiving Dinner.html",
    "href": "content_pages/Thanksgiving Dinner.html",
    "title": "Thanksgiving Dinner",
    "section": "",
    "text": "Notable topics: Survey data, Network graphing\nRecorded on: 2018-11-21\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Thanksgiving Dinner.html#full-screencast",
    "href": "content_pages/Thanksgiving Dinner.html#full-screencast",
    "title": "Thanksgiving Dinner",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Thanksgiving Dinner.html#timestamps",
    "href": "content_pages/Thanksgiving Dinner.html#timestamps",
    "title": "Thanksgiving Dinner",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:10\nExploratory bar chart of age distribution (and gender) of survey respondents\nNA\nNA\n\n\n\n\n0:7:40\nUsing count function on multiple columns to get detailed counts\nNA\nNA\n\n\n\n\n0:11:25\nParsing numbers from text using parse_number function, then using those numbers to re-level an ordinal factor (income bands)\nNA\nNA\n\n\n\n\n0:13:05\nExploring relationship between income and using homemade (vs. canned) cranberry sauce\nNA\nNA\n\n\n\n\n0:14:00\nAdding group = 1 argument to the aes function to properly display a line chart\nNA\nNA\n\n\n\n\n0:14:30\nRotating text for axis labels that overlap\nNA\nNA\n\n\n\n\n0:16:50\nGetting confidence intervals for proportions using Jeffreys interval (using beta distribution with an uniformative prior)\nqbeta\nNA\n\n\n\n\n0:17:55\nExplanation of Clopper-Pearson approach as alternative to Jeffreys interval\nNA\nNA\n\n\n\n\n0:18:30\nUsing geom_ribbon function add shaded region to line chart that shows confidence intervals\ngeom_ribbon\nNA\n\n\n\n\n0:21:55\nUsing starts_with function to select fields with names that start with a certain string (e.g., using “pie” selects “pie1” and “pie2”)\nstarts_with\nNA\n\n\n\n\n0:22:55\nUsing gather function to get wide-format data to tidy (tall) format\ngather\nNA\n\n\n\n\n0:23:45\nUsing str_remove and regex to remove digits from field values (e.g., “dessert1” and “dessert2” get turned into “dessert”)\nstr_remove\nNA\n\n\n\n\n0:27:00\n“What are people eating?” Graphing pies, sides, and desserts\nNA\nNA\n\n\n\n\n0:28:00\nUsing fct_reorder function to reorder foods based on how popular they are\nfct_reorder\nNA\n\n\n\n\n0:28:45\nUsing n_distinct function count the number of unique respondents\nn_distinct\nNA\n\n\n\n\n0:30:25\nUsing facet_wrap function to facet food types into their own graphs\nNA\nNA\n\n\n\n\n0:32:50\nUsing parse_number function to convert age ranges as character string into a numeric field\nparse_number\nNA\n\n\n\n\n0:35:35\nExploring relationship between US region and food types\nNA\nNA\n\n\n\n\n0:36:15\nUsing group_by, then mutate, then count to calculate a complicated summary\nNA\nNA\n\n\n\n\n0:40:35\nExploring relationship between praying at Thanksgiving (yes/no) and food types\nNA\nNA\n\n\n\n\n0:42:30\nEmpirical Bayes binomial estimation for calculating binomial confidence intervals (see Dave’s book on Empirical Bayes)\nadd_ebb_estimate\nebbr\n\n\n\n\n0:45:30\nAsking, “What sides/desserts/pies are eaten together?”\nNA\nNA\n\n\n\n\n0:46:20\nCalculating pairwise correlation of food types\npairwise_cor\nwidyr\n\n\n\n\n0:49:05\nNetwork graph of pairwise correlation\nNA\nggraph | igraph\n\n\n\n\n0:51:40\nAdding text labels to nodes using geom_node_text function\ngeom_node_text\nggraph | igraph\n\n\n\n\n0:53:00\nGetting rid of unnecessary graph elements (e.g., axes, gridlines) with theme_void function\ntheme_void\nNA\n\n\n\n\n0:53:25\nExplanation of network graph relationships\nNA\nNA\n\n\n\n\n0:55:05\nAdding dimension to network graph (node colour) to represent the type of food\nNA\nNA\n\n\n\n\n0:57:45\nFixing overlapping text labels using the geom_node_text function’s repel argument\ngeom_node_text\nggraph | igraph\n\n\n\n\n0:58:55\nTweaking display of percentage legend to be in more readable format (e.g., “40%” instead of “0.4”)\nNA\nscales\n\n\n\n\n1:00:05\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/The Office.html",
    "href": "content_pages/The Office.html",
    "title": "The Office",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package), LASSO regression (glmnet package)\nRecorded on: 2020-03-16\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/The Office.html#full-screencast",
    "href": "content_pages/The Office.html#full-screencast",
    "title": "The Office",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/The Office.html#timestamps",
    "href": "content_pages/The Office.html#timestamps",
    "title": "The Office",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:45\nOverview of transcripts data\nNA\nNA\n\n\n\n\n0:2:25\nOverview of ratintgs data\nNA\nNA\n\n\n\n\n0:4:10\nUsing fct_inorder function to create a factor with levels based on when they appear in the dataframe\nfct_inorder\nNA\n\n\n\n\n0:4:50\nUsing theme and element_text to turn axis labels 90 degrees\ntheme | element_text\nNA\n\n\n\n\n0:5:55\nCreating a line graph with points at each observation (using geom_line and geom_point)\ngeom_line | geom_point\nNA\n\n\n\n\n0:7:10\nAdding text labels to very high and very low-rated episodes\nNA\nNA\n\n\n\n\n0:8:50\nUsing theme function’s panel.grid.major argument to get rid of some extraneous gridlines, using element_blank function\ntheme | element_blank\nNA\n\n\n\n\n0:10:15\nUsing geom_text_repel from ggrepel package to experiment with different labelling (before abandoning this approach)\ngeom_text_repel\nggrepel\n\n\n\n\n0:12:45\nUsing row_number function to add episode_number field to make graphing easier\nrow_number\nNA\n\n\n\n\n0:14:05\nExplanation of why number of ratings (votes) is relevant to interpreting the graph\nNA\nNA\n\n\n\n\n0:19:10\nUsing unnest_tokens function from tidytext package to split full-sentence text field to individual words\nunnest_tokens\ntidytext\n\n\n\n\n0:20:10\nUsing anti_join function to filter out stop words (e.g., and, or, the)\nanti_join\nNA\n\n\n\n\n0:22:25\nUsing str_remove_all function to get rid of quotation marks from character names (quirks that might pop up when parsing)\nstr_remove_all\nNA\n\n\n\n\n0:25:40\nAsking, “Are there words that are specific to certain characters?” (using bind_tf_idf function)\nbind_tf_idf\ntidytext\n\n\n\n\n0:32:25\nUsing reorder_within function to re-order factors within a grouping (when a term appears in multiple groups) and scale_x_reordered function to graph\nreorder_within | scale_x_reordered\nNA\n\n\n\n\n0:37:05\nAsking, “What effects the popularity of an episode?”\nNA\nNA\n\n\n\n\n0:37:55\nDealing with inconsistent episode names between datasets\nNA\nNA\n\n\n\n\n0:41:25\nUsing str_remove function and some regex to remove “(Parts 1&2)” from some episode names\nstr_remove\nNA\n\n\n\n\n0:42:45\nUsing str_to_lower function to further align episode names (addresses inconsistent capitalization)\nstr_to_lower\nNA\n\n\n\n\n0:52:20\nSetting up dataframe of features for a LASSO regression, with director and writer each being a feature with its own line\nNA\nNA\n\n\n\n\n0:52:55\nUsing separate_rows function to separate episodes with multiple writers so that each has their own row\nseparate_rows\nNA\n\n\n\n\n0:58:25\nUsing log2 function to transform number of lines fields to something more useable (since it is log-normally distributed)\nlog2\nNA\n\n\n\n\n1:00:20\nUsing cast_sparse function from tidytext package to create a sparse matrix of features by episode\ncast_sparse\ntidytext\n\n\n\n\n1:01:55\nUsing semi_join function as a “filtering join”\nsemi_join\nNA\n\n\n\n\n1:02:30\nSetting up dataframes (after we have our features) to run LASSO regression\nNA\nNA\n\n\n\n\n1:03:50\nUsing cv.glmnet function from glmnet package to run a cross-validated LASSO regression\ncv.glmnet\nglmnet\n\n\n\n\n1:05:35\nExplanation of how to pick a lambda penalty parameter\nNA\nNA\n\n\n\n\n1:05:55\nExplanation of output of LASSO model\nNA\nNA\n\n\n\n\n1:09:25\nOutline of why David likes regularized linear models (which is what LASSO is)\nNA\nNA\n\n\n\n\n1:10:55\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/TidyTuesday Tweets.html",
    "href": "content_pages/TidyTuesday Tweets.html",
    "title": "TidyTuesday Tweets",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package)\nRecorded on: 2019-01-07\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/TidyTuesday Tweets.html#full-screencast",
    "href": "content_pages/TidyTuesday Tweets.html#full-screencast",
    "title": "TidyTuesday Tweets",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/TidyTuesday Tweets.html#timestamps",
    "href": "content_pages/TidyTuesday Tweets.html#timestamps",
    "title": "TidyTuesday Tweets",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:1:20\nImporting an rds file using read_rds function\nNA\nNA\n\n\n\n\n0:2:55\nUsing floor_date function from lubridate package to round dates down (that’s what the floor part does) to the month level\nfloor_date\nlubridate\n\n\n\n\n0:5:25\nAsking, “Which tweets get the most re-tweets?”\nNA\nNA\n\n\n\n\n0:5:50\nUsing contains function to select only columns that contain a certain string (“retweet” in this case)\ncontains\nNA\n\n\n\n\n0:8:05\nExploring likes/re-tweets ratio, including dealing with one or the other being 0 (which would cause divide by zero error)\nNA\nNA\n\n\n\n\n0:11:00\nStarting exploration of actual text of tweets\nNA\nNA\n\n\n\n\n0:11:35\nUsing unnest_tokens function from tidytext package to break tweets into individual words (using token argument specifically for tweet-style text)\nunnest_tokens\ntidytext\n\n\n\n\n0:12:55\nUsing anti_join function to filter out stop words (e.g., “and”, “or”, “the”) from tokenized data frame\nanti_join\nNA\n\n\n\n\n0:14:45\nCalculating summary statistics per word (average retweets and likes), then looking at distributions\nNA\nNA\n\n\n\n\n0:16:00\nExplanation of Poisson log normal distribution (number of retweets fits this distribution)\nNA\nNA\n\n\n\n\n0:17:45\nAdditional example of Poisson log normal distribution (number of likes)\nNA\nNA\n\n\n\n\n0:18:20\nExplanation of geometric mean as better summary statistic than median or arithmetic mean\nNA\nNA\n\n\n\n\n0:25:20\nUsing floor_date function from lubridate package to floor dates to the week level and tweaking so that a week starts on Monday (default is Sunday)\nfloor_date\nlubridate\n\n\n\n\n0:30:20\nAsking, “What topic is each week about?” using just the tweet text\nNA\nNA\n\n\n\n\n0:31:30\nCalculating TF-IDF of tweets, with week as the “document”\nbind_tf_idf\ntidytext\n\n\n\n\n0:33:45\nUsing top_n and group_by functions to select the top tf-idf score for each week\ntop_n\nNA\n\n\n\n\n0:37:55\nUsing str_detect function to filter out “words” that are just numbers (e.g., 16, 36)\nstr_detect\nNA\n\n\n\n\n0:41:00\nUsing distinct function with .keep_all argument to ensure only top 1 result, as alternative to top_n function (which includes ties)\ndistinct\nNA\n\n\n\n\n0:42:30\nMaking Jenny Bryan disappointed\nNA\nNA\n\n\n\n\n0:42:55\nUsing geom_text function to add text labels to graph to show to word associated with each week\ngeom_text\nNA\n\n\n\n\n0:44:10\nUsing geom_text_repel function from ggrepel package as an alternative to geom_text function for adding text labels to graph\ngeom_text_repel\nggrepel\n\n\n\n\n0:46:30\nUsing rvest package to scrape web data from a table in Tidy Tuesday README\nNA\nrvest\n\n\n\n\n0:51:00\nStarting to look at #rstats tweets\nNA\nNA\n\n\n\n\n0:56:35\nSpotting signs of fake accounts with purchased followers (lots of hashtags)\nNA\nNA\n\n\n\n\n0:59:15\nExplanation of spotting fake accounts\nNA\nNA\n\n\n\n\n1:00:45\nUsing str_detect to filter out web URLs\nstr_detect\nNA\n\n\n\n\n1:03:55\nUsing str_count function and some regex to count how many hashtags a tweet has\nstr_count\nNA\n\n\n\n\n1:07:25\nCreating a Bland-Altman plot (total on x-axis, variable of interest on y-axis)\nNA\nNA\n\n\n\n\n1:08:45\nUsing geom_text function with check_overlap argument to add labels to scatterplot\ngeom_text\nNA\n\n\n\n\n1:12:20\nAsking, “Who are the most active #rstats tweeters?”\nNA\nNA\n\n\n\n\n1:15:00\nSummary of screncast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Tour de France.html",
    "href": "content_pages/Tour de France.html",
    "title": "Tour de France",
    "section": "",
    "text": "Notable topics: Survival analysis, Animated bar graph (gganimate package)\nRecorded on: 2020-04-07\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Tour de France.html#full-screencast",
    "href": "content_pages/Tour de France.html#full-screencast",
    "title": "Tour de France",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Tour de France.html#timestamps",
    "href": "content_pages/Tour de France.html#timestamps",
    "title": "Tour de France",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:3:55\nGetting an overview of the data\nNA\nNA\n\n\n\n\n0:8:55\nAggregating data into decades using the truncated division operator %/%\n%/%\nNA\n\n\n\n\n0:21:50\nNoting that death data is right-censored (i.e., some winners are still alive)\nNA\nNA\n\n\n\n\n0:24:05\nUsing transmute function, which combines functionality of mutate (to create new variables) and select (to choose variables to keep)\ntransmute\nNA\n\n\n\n\n0:25:30\nUsing survfit function from survival package to conduct survival analysis\nsurvfit\nsurvival\n\n\n\n\n0:27:30\nUsing glance function from broom package to get a one-row model summary of the survival model\nglance\nbroom\n\n\n\n\n0:31:00\nUsing extract function to pull out a string matching a regular expression from a variable (stage number in this case)\nextract\nNA\n\n\n\n\n0:34:30\nTheorizing that there is a parsing issue with the original data’s time field\nNA\nNA\n\n\n\n\n0:41:15\nUsing group_by function’s built-in “peeling” feature, where a summarise call will “peel away” one group but left other groupings intact\ngroup_by\nNA\n\n\n\n\n0:42:05\nUsing rank function, then upgrading to percent_rank function to give percentile rankings (between 0 and 1)\nrank | percent_rank\nNA\n\n\n\n\n0:47:50\nUsing geom_smooth function with method argument as “lm” to plot a linear regression\ngeom_smooth\nNA\n\n\n\n\n0:48:10\nUsing cut function to bin numbers (percentiles in this case) into categories\ncut\nNA\n\n\n\n\n0:50:25\nReviewing boxplots exploring relationship between first-stage performance and overall Tour performance\nNA\nNA\n\n\n\n\n0:51:30\nStarting to create an animation using gganimate package\nNA\ngganimate\n\n\n\n\n0:56:00\nActually writing the code to create the animation\nNA\nNA\n\n\n\n\n0:58:20\nUsing reorder_within function from tidytext package to re-order factors that have the same name across multiple groups\nreorder_within\ntidytext\n\n\n\n\n1:02:40\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Transit Costs.html",
    "href": "content_pages/Transit Costs.html",
    "title": "Transit Costs",
    "section": "",
    "text": "Notable topics: EDA (Exploratory Data Analysis) with boxplots, interactive Shiny dashboard\nRecorded on: 2021-01-05\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Transit Costs.html#full-screencast",
    "href": "content_pages/Transit Costs.html#full-screencast",
    "title": "Transit Costs",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Transit Costs.html#timestamps",
    "href": "content_pages/Transit Costs.html#timestamps",
    "title": "Transit Costs",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:45\nUsing countrycode function from countrycode package to convert two-letter country codes to country names\ncountrycode\ncountrycode\n\n\n\n\n0:6:20\nUsing geom_errorbarh function to visualize start and end times of transit projects\ngeom_errorbarh\nNA\n\n\n\n\n0:7:15\nUsing fct_reorder function to reorder lines by project midpoint year\nfct_reorder\nNA\n\n\n\n\n0:9:10\nUsing as.numeric to convert character field (real_cost) to proper numeric field\nas.numeric\nNA\n\n\n\n\n0:10:20\nUsing mutate_at function to apply the same function (as.numeric) to multiple fields in one line of code\nmutate_at\nNA\n\n\n\n\n0:13:40\nUsing geom_boxplot and fct_lump to visualize cost per kilometre by country as boxplots\ngeom_boxplot | fct_lump\nNA\n\n\n\n\n0:15:35\nUsing glue function from glue package to combine fields to make easy-to-read labels on a graph\nglue\nglue\n\n\n\n\n0:19:15\nSplitting boxplots into whether they are railroads (rr) or not, using factor function and fill argument\nfactor\nNA\n\n\n\n\n0:24:15\nInvestigating sources of missing data for Shanghai\nNA\nNA\n\n\n\n\n0:31:35\nUsing geom_jitter with geom_boxplot to show distribution of items within each group\ngeom_jitter\nNA\n\n\n\n\n0:33:00\nSetting geom_boxplot argument outlier.size = -1 as a hack to get rid of boxplot-generated outlier points\ngeom_boxplot\nNA\n\n\n\n\n0:40:40\nStarting to build a shiny app\nNA\nshiny\n\n\n\n\n0:48:55\nReview of preliminary shiny app\nNA\nshiny\n\n\n\n\n0:58:00\nScreencast summary\nNA\nNA\n\n\n\n\n1:00:25\nShowing how to upload code to GitHub in RStudio\nNA\nNA"
  },
  {
    "objectID": "content_pages/TV Golden Age.html",
    "href": "content_pages/TV Golden Age.html",
    "title": "TV Golden Age",
    "section": "",
    "text": "Notable topics: Data manipulation, Logistic regression\nRecorded on: 2019-01-09\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/TV Golden Age.html#full-screencast",
    "href": "content_pages/TV Golden Age.html#full-screencast",
    "title": "TV Golden Age",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/TV Golden Age.html#timestamps",
    "href": "content_pages/TV Golden Age.html#timestamps",
    "title": "TV Golden Age",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:25\nQuick tip on how to start exploring a new dataset\nNA\nNA\n\n\n\n\n0:7:30\nInvestigating inconsistency of shows having a count of seasons that is different from the number of seasons given in the data\nNA\nNA\n\n\n\n\n0:10:10\nUsing %in% operator and all function to only get shows that have a first season and don’t have skipped seasons in the data\n%in% | all\nNA\n\n\n\n\n0:15:30\nAsking, “Which seasons have the most variation in ratings?”\nNA\nNA\n\n\n\n\n0:20:25\nUsing facet_wrap function to separate different shows on a line graph into multiple small graphs\nfacet_wrap\nNA\n\n\n\n\n0:20:50\nWriting custom embedded function to get width of breaks on the x-axis to always be even (e.g., season 2, 4, 6, etc.)\nNA\nNA\n\n\n\n\n0:23:50\nCommitting, finding, and explaining a common error of using the same variable name when summarizing multiple things\nNA\nNA\n\n\n\n\n0:28:20\nUsing truncated division operator %/% to bin data into two-year bins instead of annual (e.g., 1990 and 1991 get binned to 1990)\n%/%\nNA\n\n\n\n\n0:31:30\nUsing subsetting (with square brackets) within the mutate function to calculate mean on only a subset of data (without needing to filter)\nmutate\nNA\n\n\n\n\n0:33:50\nUsing gather function (now pivot_longer) to get metrics as columns into tidy format, in order to graph them all at once with a facet_wrap\ngather\nNA\n\n\n\n\n0:36:30\nUsing pmin function to lump all seasons after 4 into one row (it still shows “4”, but it represents “4+”)\npmin\nNA\n\n\n\n\n0:39:00\nAsking, “If season 1 is good, do you get a second season?” (show survival)\nNA\nNA\n\n\n\n\n0:40:35\nUsing paste0 and spread functions to get season 1-3 ratings into three columns, one for each season\npaste0 | spread\nNA\n\n\n\n\n0:42:05\nUsing distinct function with .keep_all argument remove duplicates by only keeping the first one that appears\ndistinct\nNA\n\n\n\n\n0:45:50\nUsing logistic regression to answer, “Does season 1 rating affect the probability of getting a second season?” (note he forgets to specify the family argument, fixed at 57:25)\nglm\nNA\n\n\n\n\n0:48:35\nUsing ntile function to divide data into N bins (5 in this case), then eventually using cut function instead\nntile | cut\nNA\n\n\n\n\n0:57:00\nAdding year as an independent variable to the logistic regression model\nNA\nNA\n\n\n\n\n0:58:50\nAdding an interaction term (season 1 interacting with year) to the logistic regression model\nNA\nNA\n\n\n\n\n0:59:55\nUsing augment function as a method of visualizing and interpreting coefficients of regression model\naugment\nNA\n\n\n\n\n1:00:30\nUsing crossing function to create new data to test the logistic regression model on and interpret model coefficients\ncrossing\nNA\n\n\n\n\n1:03:40\nFitting natural splines using the splines package, which would capture a non-linear relationship\nNA\nsplines\n\n\n\n\n1:06:15\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/US Dairy Consumption.html",
    "href": "content_pages/US Dairy Consumption.html",
    "title": "US Dairy Consumption",
    "section": "",
    "text": "Notable topics: Time series analysis, Forecasting (sweep package)\nRecorded on: 2019-01-29\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/US Dairy Consumption.html#full-screencast",
    "href": "content_pages/US Dairy Consumption.html#full-screencast",
    "title": "US Dairy Consumption",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/US Dairy Consumption.html#timestamps",
    "href": "content_pages/US Dairy Consumption.html#timestamps",
    "title": "US Dairy Consumption",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:50\nIdentifying the need for a gather step\nNA\nNA\n\n\n\n\n0:4:40\nChanging snake case to title case using str_to_title and str_replace_all functions\nstr_to_title | str_replace_all\nNA\n\n\n\n\n0:6:20\nIdentifying need for separating categories into major and minor categories (e.g., “Cheese Other” can be divided into “Cheese” and “Other”)\nNA\nNA\n\n\n\n\n0:7:10\nUsing separate function to split categories into major and minor categories (good explanation of “extra” argument, which merges additional separations into one field)\nseparate\nNA\n\n\n\n\n0:8:20\nUsing coalesce function to deal with NAs resulting from above step\ncoalesce\nNA\n\n\n\n\n0:10:30\nDealing with graph of minor category that is linked to multiple major categories (“Other” linked to “Cheese” and “Frozen”)\nNA\nNA\n\n\n\n\n0:13:10\nIntroducing fct_lump function as an approach to work with many categories\nfct_lump\nNA\n\n\n\n\n0:14:50\nIntroducing facetting (facet_wrap function) as second alternative to working with many categories\nfacet_wrap\nNA\n\n\n\n\n0:15:50\nDealing with “Other” category having two parts to it by using ifelse function in the cleaning step (e.g., go from “Other” to “Other Cheese”)\nNA\nNA\n\n\n\n\n0:19:45\nLooking at page for the sweep package\nNA\nsweep\n\n\n\n\n0:21:20\nUsing tk_ts function to coerce a tibble to a timeseries\ntk_ts\nsweep\n\n\n\n\n0:22:10\nTurning year column (numeric) into a date by adding number of years to Jan 1, 0001\nNA\nNA\n\n\n\n\n0:26:00\nNesting time series object into each combination of category and product\nNA\nNA\n\n\n\n\n0:27:50\nApplying ETS (Error, Trend, Seasonal) model to each time series\nNA\nNA\n\n\n\n\n0:28:10\nUsing sw_glance function (sweep package’s version of glance function) to pull out model parameters from model field created in above step\nsw_glance\nsweep\n\n\n\n\n0:29:45\nUsing sw_augment function to append fitted values and residuals from the model to the original data\nsw_augment\nNA\n\n\n\n\n0:30:50\nVisualising actual and fitted values on the same graph to get a look at the ETS model\nNA\nNA\n\n\n\n\n0:32:10\nUsing Arima function (note the capital A) as alternative to ETS (not sure what difference is between arima and Arima)\nArima\nNA\n\n\n\n\n0:35:00\nForecasting into the future using an ETS model using various functions: unnest, sw_sweep, forecast\nsw_sweep | forecast\nsweep\n\n\n\n\n0:37:45\nUsing geom_ribbon function to add confidence bounds to forecast\ngeom_ribbon\nNA\n\n\n\n\n0:40:20\nForecasting using auto-ARIMA (instead of ETS)\nNA\nNA\n\n\n\n\n0:40:55\nApplying two forecasting methods at the same time (auto-ARIMA and ETS) using the crossing function\ncrossing\nNA\n\n\n\n\n0:41:55\nQuick test of how invoke function works (used to call a function easily, e.g., when it is a character string instead of called directly)\ninvoke\nNA\n\n\n\n\n0:47:35\nRemoving only one part of legend (line type of solid or dashed) using scale_linetype_discrete function\nscale_linetype_discrete\nNA\n\n\n\n\n0:51:25\nUsing gather function to clean up new dataset\ngather\nNA\n\n\n\n\n0:52:05\nUsing fct_recode to fix a typo in a categorical variable\nfct_recode\nNA\n\n\n\n\n0:56:00\nCopy-pasting previous forecasting code to cheese and reviewing any changes needed\nNA\nNA\n\n\n\n\n0:57:20\nDiscussing alternative approach: creating interactive visualisation using shiny package to do direct comparisons\nNA\nshiny"
  },
  {
    "objectID": "content_pages/US Incarceration.html",
    "href": "content_pages/US Incarceration.html",
    "title": "US Incarceration",
    "section": "",
    "text": "Notable topics: Animated map (gganimate package), Dealing with missing data\nRecorded on: 2019-01-25\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/US Incarceration.html#full-screencast",
    "href": "content_pages/US Incarceration.html#full-screencast",
    "title": "US Incarceration",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/US Incarceration.html#timestamps",
    "href": "content_pages/US Incarceration.html#timestamps",
    "title": "US Incarceration",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:4:30\nCreating a facetted (small multiples) line graph of incarceration rate by urbanicity and race over time\nfacet_wrap\nNA\n\n\n\n\n0:7:45\nDiscussion of statistical testing of incarceration rates by urbanicity (e.g., rural, suburban)\nNA\nNA\n\n\n\n\n0:11:25\nExploring the extent of missing data on prison population\nNA\nNA\n\n\n\n\n0:14:15\nUsing any function to filter down to states that have at least one (hence the any function) row of non-missing data\nany\nNA\n\n\n\n\n0:18:40\nUsing cut function to manually bin data along user-specified intervals\ncut\nNA\n\n\n\n\n0:24:15\nStarting to create a choropleth map of incarceration rate by state\nNA\nNA\n\n\n\n\n0:26:20\nUsing match function to match two-letter state abbreviation to full state name, in order to get data needed to create a map\nmatch\nNA\n\n\n\n\n0:28:00\nActually typing the code (now that we have the necessary data) to create a choropleth map\nNA\nNA\n\n\n\n\n0:33:05\nUsing str_remove function and regex to chop off the end of county names (e.g., “Allen Parish” becomes “Allen”)\nstr_remove\nNA\n\n\n\n\n0:33:30\nMaking choropleth more specific by drilling down to county-level data\nNA\nNA\n\n\n\n\n0:41:10\nStarting to make an animated choropleth map using gganimate\nNA\ngganimate\n\n\n\n\n0:42:20\nUsing modulo operator %% to choose every 5th year\n%%\nNA\n\n\n\n\n0:43:45\nUsing scale_fill_gradient2 function’s limits argument to exclude unusally high values that were blowing out the scale\nscale_fill_gradient2\nNA\n\n\n\n\n0:48:15\nUsing summarise_at function to apply the same function to multiple fields at the same time\nsummarise_at\nNA\n\n\n\n\n0:50:10\nStarting to investigate missing data (how much is missing, where is it missing, etc.)\nNA\nNA\n\n\n\n\n0:54:50\nCreating a line graph that excludes counties with missing data\nNA\nNA\n\n\n\n\n0:57:05\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/US PhDs.html",
    "href": "content_pages/US PhDs.html",
    "title": "US PhDs",
    "section": "",
    "text": "Notable topics: Data cleaning (getting messy data into tidy format)\nRecorded on: 2019-02-22\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/US PhDs.html#full-screencast",
    "href": "content_pages/US PhDs.html#full-screencast",
    "title": "US PhDs",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/US PhDs.html#timestamps",
    "href": "content_pages/US PhDs.html#timestamps",
    "title": "US PhDs",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:3:15\nUsing read_xlsx function to read in Excel spreadsheet, including skipping first few rows that don’t have data\nread_xlsx\nNA\n\n\n\n\n0:7:25\nOverview of starting very messy data\nNA\nNA\n\n\n\n\n0:8:20\nUsing gather function to clean up wide dataset\ngather\nNA\n\n\n\n\n0:9:20\nUsing fill function to fill in NA values with a entries in a previous observation\nfill\nNA\n\n\n\n\n0:10:10\nCleaning variable that has number and percent in it, on top of one another using a combination of ifelse and fill functions\nfill | ifelse\nNA\n\n\n\n\n0:12:00\nUsing spread function on cleaned data to separate number and percent by year\nspread\nNA\n\n\n\n\n0:13:50\nSpotted a mistake where he had the wrong string on str_detect function\nstr_detect\nNA\n\n\n\n\n0:16:50\nUsing sample function to get 6 random fields of study to graph\nsample\nNA\n\n\n\n\n0:18:50\nCleaning another dataset, which is much easier to clean\nNA\nNA\n\n\n\n\n0:19:05\nRenaming the first field, even without knowing the exact name\nNA\nNA\n\n\n\n\n0:21:55\nCleaning another dataset\nNA\nNA\n\n\n\n\n0:23:10\nDiscussing challenge of when indentation is used in original dataset (for group / sub-group distinction)\nNA\nNA\n\n\n\n\n0:25:20\nStarting to separate out data that is appended to one another in the original dataset (all, male, female)\nNA\nNA\n\n\n\n\n0:27:30\nRemoving field with long name using contains function\ncontains\nNA\n\n\n\n\n0:28:10\nUsing fct_recode function to rename an oddly-named category in a categorical variable (ifelse function is probably a better alternative)\nfct_recode\nNA\n\n\n\n\n0:35:30\nDiscussing solution to broad major field description and fine major field description (meaningfully indented in original data)\nNA\nNA\n\n\n\n\n0:39:40\nUsing setdiff function to separate broad and fine major fields\nsetdiff\nNA"
  },
  {
    "objectID": "content_pages/US Wind Turbines.html",
    "href": "content_pages/US Wind Turbines.html",
    "title": "US Wind Turbines",
    "section": "",
    "text": "Notable topics: Animated map (gganimate package)\nRecorded on: 2018-11-06\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/US Wind Turbines.html#full-screencast",
    "href": "content_pages/US Wind Turbines.html#full-screencast",
    "title": "US Wind Turbines",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/US Wind Turbines.html#timestamps",
    "href": "content_pages/US Wind Turbines.html#timestamps",
    "title": "US Wind Turbines",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:3:50\nUsing count function to explore categorical variables\ncount\nNA\n\n\n\n\n0:5:00\nCreating a quick-and-dirty map using geom_point function and latitude and longitude data\ngeom_point\nNA\n\n\n\n\n0:6:10\nExplaining need for mapproj package when plotting maps in ggplot2\ncoord_map\nmapproj\n\n\n\n\n0:7:35\nUsing borders function to add US state borders to map\nborders\nNA\n\n\n\n\n0:10:45\nUsing fct_lump to get the top 6 project categories and put the rest in a lumped “Other” category\nfct_lump\nNA\n\n\n\n\n0:11:30\nChanging data so that certain categories’ points appear in front of other categories’ points on the map\nNA\nNA\n\n\n\n\n0:14:15\nTaking the centroid (average longitude and latitude) of points across a geographic area as a way to aggregate categories to one point\nNA\nNA\n\n\n\n\n0:19:40\nUsing ifelse function to clean missing data that is coded as “-9999”\nifelse\nNA\n\n\n\n\n0:26:00\nAsking, “How has turbine capacity changed over time?”\nNA\nNA\n\n\n\n\n0:33:15\nExploring different models of wind turbines\nNA\nNA\n\n\n\n\n0:38:00\nUsing mutate_if function to find NA values (coded as -9999) in multiple columns and replace them with an actual NA\nmutate_if\nNA\n\n\n\n\n0:45:40\nReviewing documentation for gganimate package\nNA\ngganimate\n\n\n\n\n0:47:00\nAttempting to set up gganimate map\nNA\ngganimate\n\n\n\n\n0:48:55\nUnderstanding gganimate package using a “Hello World” / toy example, then trying to debug turbine animation\nNA\ngganimate\n\n\n\n\n0:56:45\nUsing is.infinite function to get rid of troublesome Inf values\nis.infinite\nNA\n\n\n\n\n0:57:55\nQuick hack for getting cumulative data from a table using crossing function (though it does end up with some duplication)\ncrossing\nNA\n\n\n\n\n1:01:45\nDiagnosis of gganimate issue (points between integer years are being interpolated)\nNA\ngganimate\n\n\n\n\n1:04:35\nPseudo-successful gganimate map (cumulative points show up, but some points are missing)\nNA\ngganimate\n\n\n\n\n1:05:40\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Volcano Eruptions.html",
    "href": "content_pages/Volcano Eruptions.html",
    "title": "Volcano Eruptions",
    "section": "",
    "text": "Notable topics: Static map with ggplot2, Interactive map with leaflet, Animated map with gganimate\nRecorded on: 2020-05-12\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Volcano Eruptions.html#full-screencast",
    "href": "content_pages/Volcano Eruptions.html#full-screencast",
    "title": "Volcano Eruptions",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Volcano Eruptions.html#timestamps",
    "href": "content_pages/Volcano Eruptions.html#timestamps",
    "title": "Volcano Eruptions",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:7:00\nChange the last_eruption_year into years_ago by using mutate from the dplyr package with years_ago = 2020 - as.numeric(last_eruption_year)). In the plot David includes +1 to account for 0 values in the years_ago variable.\nmutate\ndplyr\n\n\n\n\n0:9:50\nUse str_detect from the stringr package to search the volcano_name variable for Vesuvius when not sure if spelling is correct.\nstr_detect\nstringr\n\n\n\n\n0:12:50\nUse the longitude and latitude to create a world map showing where the volcanoes are located.\ngrom_point | theme_map | borders\nggplot2 | ggthemes\n\n\n\n\n0:15:30\nUse fct_lump from theforcats package to lump together all primary_volcano_type factor levels except for the n most frequent.\nfct_lump\nforcats\n\n\n\n\n0:16:25\nUse str_remove from the stringr package with the regular expression \"\\\\(.\\\\)\" to remove the parentheses.\nstr_remove\nstringr\n\n\n\n\n0:18:30\nUse the leaflet package to create an interactive map with popup information about each volcano.\nleaflet | addTiles | addCircleMarkers\nleaflet\n\n\n\n\n0:24:10\nUse glue from the glue package to create an HTML string by concatenating volcano_name and primary_volcano_type between HTML <p></p> tags.\nglue\nglue\n\n\n\n\n0:27:15\nUse the DT package to turn the leaflet popup information into a datatable.\ngather | nest | map\nDT\n\n\n\n\n0:31:40\nUse str_replace_all fromt he stringr package to replace all the underscores _ in volcano_name with space. Then use str_to_title from the stringr package to convert the volcano_name variable to title case.\nstr_replace | str_to_title\nstringr\n\n\n\n\n0:32:05\nUse kable with format = HTML from the knitr package instead of DT to make turning the data into HTML much easier.\nkable\nknitr\n\n\n\n\n0:34:05\nUse paste0 from base R to bold the Volcano Name, Primary Volcano Type, and Last Eruption Year in the leaflet popup.\npaste0\nbase\n\n\n\n\n0:34:50\nUse replace_na from the tidyr package to replace unknown with NA.\nreplace_na\ntidyr\n\n\n\n\n0:37:15\nUse addMeasure from the leaflet package to add a tool to the map that allows for the measuring of distance between points.\naddMeasure\nleaflet\n\n\n\n\n0:39:30\nUse colorNumeric from the leaflet package to color the points based on their population within 5km. To accomplish this, David creates 2 new variables: 1) transformed_pop to get the population on a log2 scale & 2) pop_color which uses the colorNumeric function to generate the color hex values based on transformed_pop.\ncolorNumeric\nleaflet\n\n\n\n\n0:46:30\nUse the gganimate package to create an animated map.\ntransition_time | frame_time\ngganimate\n\n\n\n\n0:48:45\nUse geom_point from the ggplot2 package with size = .00001 * 10 ^ vei so the size of the points are then proportional to the volume metrics provided in the Volcano Eruption Index. The metrics are in Km^3.\ngeom_point\nggplot2\n\n\n\n\n0:50:20\nUse scale_size_continuous from the ggplot2 package with range = c(.1, 6) to make the smaller points smaller and larger points larger.\nscale_size_continuous\nggplot2\n\n\n\n\n0:50:55\nUse scale_color_gradient2 from the ggplot2 package to apply color gradient to each point based on the volcano size and whether its low or high.\nscale_color_gradient2\nggplot2\n\n\n\n\n0:59:40\nSummary of screencast while waiting for gganimate map to render.\nAlso, brief discussion on using transition_reveal instead of transition_time to keep the point on the map instead of replacing them in each frame.\ntransition_reveal\ngganimate"
  },
  {
    "objectID": "content_pages/Wine Ratings.html",
    "href": "content_pages/Wine Ratings.html",
    "title": "Wine Ratings",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package), LASSO regression (glmnet package)\nRecorded on: 2019-05-31\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Wine Ratings.html#full-screencast",
    "href": "content_pages/Wine Ratings.html#full-screencast",
    "title": "Wine Ratings",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Wine Ratings.html#timestamps",
    "href": "content_pages/Wine Ratings.html#timestamps",
    "title": "Wine Ratings",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:3:15\nUsing extract function from tidyr package to pull out year from text field\nextract\ntidyr\n\n\n\n\n0:9:15\nChanging extract function to pull out year column more accurately\nextract\ntidyr\n\n\n\n\n0:13:00\nStarting to explore prediction of points\nNA\nNA\n\n\n\n\n0:17:00\nUsing fct_lump on country variable to collapse countries into an “Other” category, then fct_relevel to set the baseline category for a linear model\nfct_lump | fct_relevel\nNA\n\n\n\n\n0:21:30\nInvestigating year as a potential confounding variable\nNA\nNA\n\n\n\n\n0:24:45\nInvestigating “taster_name” as a potential confounding variable\nNA\nNA\n\n\n\n\n0:27:45\nCoefficient (TIE fighter) plot to see effect size of terms in a linear model, using tidy function from broom package\ntidy\nbroom\n\n\n\n\n0:30:45\nPolishing category names for presentation in graph using str_replace function\nstr_replace\nNA\n\n\n\n\n0:32:15\nUsing augment function to add predictions of linear model to original data\naugment\nNA\n\n\n\n\n0:33:30\nPlotting predicted points vs. actual points\nNA\nNA\n\n\n\n\n0:34:45\nUsing ANOVA to determine the amount of variation that explained by different terms\nNA\nNA\n\n\n\n\n0:36:45\nUsing tidytext package to set up wine review text for Lasso regression\nNA\ntidytext\n\n\n\n\n0:40:00\nSetting up and using pairwise_cor function to look at words that appear in reviews together\npairwise_cor\nwidyr\n\n\n\n\n0:45:00\nCreating sparse matrix using cast_sparse function from tidytext package; used to perform a regression on positive/negative words\ncast_sparse\ntidytext\n\n\n\n\n0:46:45\nChecking if rownames of sparse matrix correspond to the wine_id values they represent\nNA\nNA\n\n\n\n\n0:47:00\nSetting up sparse matrix for using glmnet package to do sparse regression using Lasso method\nNA\nglmnet\n\n\n\n\n0:48:15\nActually writing code for doing Lasso regression\nNA\nglmnet\n\n\n\n\n0:49:45\nBasic explanation of Lasso regression\nNA\nNA\n\n\n\n\n0:51:00\nPutting Lasso model into tidy format\ntidy\nNA\n\n\n\n\n0:53:15\nExplaining how the number of terms increases as lambda (penalty parameter) decreases\nNA\nNA\n\n\n\n\n0:54:00\nAnswering how we choose a lambda value (penalty parameter) for Lasso regression\nNA\nNA\n\n\n\n\n0:56:45\nUsing parallelization for intensive computations\nNA\nNA\n\n\n\n\n0:58:30\nAdding price (from original linear model) to Lasso regression\nNA\nNA\n\n\n\n\n1:02:15\nShows glmnet.fit piece of a Lasso (glmnet) model\nNA\nglmnet\n\n\n\n\n1:03:30\nPicking a lambda value (penalty parameter) and explaining which one to pick\nNA\nNA\n\n\n\n\n1:08:15\nTaking most extreme coefficients (positive and negative) by grouping theme by direction\nNA\nNA\n\n\n\n\n1:10:30\nDemonstrating tidytext package’s sentiment lexicon, then looking at individual reviews to demonstrate the model\nNA\ntidytext\n\n\n\n\n1:17:30\nVisualizing each coefficient’s effect on a single review\nNA\nNA\n\n\n\n\n1:20:30\nUsing str_trunc to truncate character strings\nstr_trunc\nNA"
  },
  {
    "objectID": "content_pages/Women in the Workplace.html",
    "href": "content_pages/Women in the Workplace.html",
    "title": "Women in the Workplace",
    "section": "",
    "text": "Notable topics: Interactive scatterplot (plotly and shiny packages)\nRecorded on: 2019-03-05\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Women in the Workplace.html#full-screencast",
    "href": "content_pages/Women in the Workplace.html#full-screencast",
    "title": "Women in the Workplace",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Women in the Workplace.html#timestamps",
    "href": "content_pages/Women in the Workplace.html#timestamps",
    "title": "Women in the Workplace",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:5:50\nWriting a custom function that summarizes variables based on their names (then abandoning the idea)\nNA\nNA\n\n\n\n\n0:9:15\nUsing complete.cases function to find observations that have an NA value in any variable\ncomplete.cases\nNA\n\n\n\n\n0:9:50\nUsing subsetting within a summarise function to calculate a weighted mean when dealing with 0 or NA values in some observations\nNA\nNA\n\n\n\n\n0:12:20\nDebugging what is causing NA values to appear in the summarise output (finds the error at 13:25)\nNA\nNA\n\n\n\n\n0:17:50\nHypothesizing about one sector illustrating a variation of Simpson’s Paradox\nNA\nNA\n\n\n\n\n0:25:25\nCreating a scatterplot with a logarithmic scale and using scale_colour_gradient2 function to encode data to point colour\nscale_colour_gradient2\nNA\n\n\n\n\n0:30:00\nCreating an interactive plot (tooltips show up on hover) using ggplotly function from plotly package\nggplotly\nplotly\n\n\n\n\n0:33:20\nFiddling with scale_size_continuous function’s range argument to specify point size on a scatterplot (which are encoded to total workers)\nNA\nNA\n\n\n\n\n0:34:50\nExplanation of why healthcare sector is a good example of Simpson’s Paradox\nNA\nNA\n\n\n\n\n0:43:15\nStarting to create a shiny app with “occupation” as only input (many tweaks in subsequent minutes to make it work)\nNA\nshiny\n\n\n\n\n0:47:55\nTweaking size (height) of graph in shiny app\nNA\nshiny\n\n\n\n\n0:54:05\nSummary of screencast\nNA\nNA"
  },
  {
    "objectID": "content_pages/Women's World Cup.html",
    "href": "content_pages/Women's World Cup.html",
    "title": "Women’s World Cup",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-07-22\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Women's World Cup.html#full-screencast",
    "href": "content_pages/Women's World Cup.html#full-screencast",
    "title": "Women’s World Cup",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/Women's World Cup.html#timestamps",
    "href": "content_pages/Women's World Cup.html#timestamps",
    "title": "Women’s World Cup",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:2:15\nAdding country names using countrycode package\nNA\ncountrycode\n\n\n\n\n0:3:45\nWeb scraping country codes from Wikipedia\nNA\nNA\n\n\n\n\n0:6:00\nCombining tables that are separate lists into one dataframe\nNA\nNA\n\n\n\n\n0:14:00\nUsing rev function (reverse) to turn multiple rows of soccer match scores into one row (base team and opposing team)\nrev\nNA\n\n\n\n\n0:26:30\nApplying a geom_smooth linear model line to a scatter plot, then facetting it\ngeom_smooth\nNA\n\n\n\n\n0:28:30\nAdding a line with a slope of 1 (x = y) using geom_abline\ngeom_abline\nNA\n\n\n\n\n0:40:00\nPulling out elements of a list that is embedded in a dataframe\nNA\nNA\n\n\n\n\n1:09:45\nUsing glue function to add context to facet titles\nglue\nglue"
  },
  {
    "objectID": "content_pages/X-Men Comics.html",
    "href": "content_pages/X-Men Comics.html",
    "title": "X-Men Comics",
    "section": "",
    "text": "Notable topics: Data manipulation, Lollipop graph, Grouping using floor division\nRecorded on: 2020-06-30\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/X-Men Comics.html#full-screencast",
    "href": "content_pages/X-Men Comics.html#full-screencast",
    "title": "X-Men Comics",
    "section": "Full screencast",
    "text": "Full screencast"
  },
  {
    "objectID": "content_pages/X-Men Comics.html#timestamps",
    "href": "content_pages/X-Men Comics.html#timestamps",
    "title": "X-Men Comics",
    "section": "Timestamps",
    "text": "Timestamps\n\n0:07:25\nUsing separate to separate the name from secrete identity in the character column\nseparate\ntidyr\n\n\n\n\n0:09:55\nUsing summarize and across to find the frequency of the action variables and find out how many issues each action was used for each character\nsummarize | across\ndplyr\n\n\n\n\n0:13:25\nCreate a geom_col chart to visualize which character speaks in the most issues\ngeom_col | fct_reorder\nggplot2 | forcats\n\n\n\n\n0:18:35\nCreate a geom_point chart to visualize each character’s average lines per issue in which the character is depicted\ngeom_point | geom_text | geom_text_repel | summarize\nggplot2 | ggrepel | dplyr\n\n\n\n\n0:22:05\nCreate a geom_point chart to visualize each character’s average thoughts per issue in which the character is depicted\ngeom_point | geom_text | geom_text_repel | summarize\nggplot2 | ggrepel | dplyr\n\n\n\n\n0:23:10\nCreate a geom_point chart to visualize character’s speech versus thought ratio per issue in which the character is depicted\ngeom_point | geom_text | geom_text_repel | summarize\nggplot2 | ggrepel | dplyr\n\n\n\n\n0:30:05\nCreate a geom_point to visualize character’s number of lines while in costume versus not in costume\ngeom_point | pivot_longer | fct_reorder\nggplot2 | tidyr | forcats\n\n\n\n\n0:34:30\nCreate a geom_point chart to visualize the lines in costume versus lines out of costume ratio\ngeom_point | geom_text | geom_text_repel | summarize\nggplot2 | ggrepel | dplyr\n\n\n\n\n0:39:20\nCreate a lollipop graph using geom_point and geom_errorbarh to visualize the lines in costume versus lines out of costume ratio and their distance from 1.0 (1 to 1)\ngeom_point | fct_reorder | geom_errorbarh\nggplot2 | forcats\n\n\n\n\n0:45:00\nUse summarize to find the frequency of each location and the total number of unique issues where the location is used\nsummarize | group_by | arrange\ndplyr\n\n\n\n\n0:46:00\nUse summarize and fct_lump to count how many issues each author has written while lumping together all authors except the most frequent\nsummarize | fct_lump\ndplyr | forcats\n\n\n\n\n0:47:25\nUse summarize and fct_lump to see if the authors rates of passing the Bechdel test differ from one another\nsummarize | fct_lump\ndplyr | forcats\n\n\n\n\n0:52:45\nCreate a geom_line chart to visualize if the rates of passing the Bechdel test changed over time and floor division %/% to generate 20 observations per group\ngeom_line | summarize\ndplyr\n\n\n\n\n0:54:35\nCreate a geom_col to visualize the amount of lines each character has per issue over time giving context to Bechdel test passing rates\ngeom_col | summarize | fct_lump | facet_wrap\nggplot | dplyr | forcats\n\n\n\n\n1:00:00\nSummary of screencast\nNA\nNA"
  }
]